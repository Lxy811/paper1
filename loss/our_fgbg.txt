INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.01', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'Predictor001', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/Predictor001-fgbg'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  

INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.01
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: Predictor001
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/Predictor001-fgbg
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/Predictor001-fgbg/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Loading data statistics from: ./checkpoints/Predictor001-fgbg/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                              loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                              loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fuse_pos_union.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fuse_pos_union.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion1.dense_x.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion1.dense_x.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion1.dense_y.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion1.dense_y.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion2.dense_x.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion2.dense_x.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion2.dense_y.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.fusion2.dense_y.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual1.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual1.weight of shape (1024, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_bbox.0.bias of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_bbox.0.weight of shape (64, 21)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_bbox.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.rel_bbox.3.weight of shape (128, 64)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                            loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                          loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                            loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                          loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/Predictor001-fgbg/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.rel_bbox.0.weight: nan, (torch.Size([64, 21]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.rel_bbox.0.bias: nan, (torch.Size([64]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.rel_bbox.3.weight: nan, (torch.Size([128, 64]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.rel_bbox.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion1.dense_x.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion1.dense_x.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion1.dense_y.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion1.dense_y.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion2.dense_x.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion2.dense_x.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion2.dense_y.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fusion2.dense_y.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fuse_pos_union.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.fuse_pos_union.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual1.weight: nan, (torch.Size([1024, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual1.bias: nan, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 4:27:13  iter: 1  loss: 142.0969 (142.0969)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.3715 (4.3715)  loss_bg: 137.7255 (137.7255)  time: 1.0022 (1.0022)  data: 0.1129 (0.1129)  lr: 0.001600  max mem: 7132
INFO:maskrcnn_benchmark:eta: 4:44:31  iter: 100  loss: 5.9915 (20.5645)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.8369 (5.3681)  loss_bg: 0.9321 (15.1964)  time: 1.0249 (1.0737)  data: 0.1036 (0.1612)  lr: 0.004451  max mem: 10061
INFO:maskrcnn_benchmark:eta: 4:41:26  iter: 200  loss: 5.1238 (13.1511)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1059 (4.9873)  loss_bg: 0.9561 (8.1638)  time: 0.9913 (1.0688)  data: 0.0908 (0.1584)  lr: 0.007331  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:40:16  iter: 300  loss: 5.1152 (10.6072)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1231 (4.7499)  loss_bg: 0.7892 (5.8573)  time: 0.9970 (1.0711)  data: 0.0899 (0.1630)  lr: 0.010211  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:37:27  iter: 400  loss: 5.2530 (9.2939)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0321 (4.5761)  loss_bg: 0.8598 (4.7177)  time: 1.0044 (1.0671)  data: 0.0947 (0.1612)  lr: 0.013091  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:35:29  iter: 500  loss: 4.9666 (8.4730)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8882 (4.4468)  loss_bg: 0.9868 (4.0262)  time: 1.0034 (1.0664)  data: 0.0857 (0.1597)  lr: 0.015971  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:32:07  iter: 600  loss: 4.8001 (7.9508)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.7942 (4.3808)  loss_bg: 0.9516 (3.5700)  time: 0.9763 (1.0602)  data: 0.0854 (0.1546)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:30:00  iter: 700  loss: 5.0835 (7.5315)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.6870 (4.3079)  loss_bg: 0.8887 (3.2236)  time: 0.9949 (1.0589)  data: 0.0854 (0.1540)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:28:33  iter: 800  loss: 4.3416 (7.1714)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3207 (4.2287)  loss_bg: 0.8195 (2.9427)  time: 0.9930 (1.0601)  data: 0.0842 (0.1561)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:26:42  iter: 900  loss: 4.5372 (6.9101)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4532 (4.1688)  loss_bg: 0.7860 (2.7412)  time: 1.0005 (1.0598)  data: 0.0841 (0.1557)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:24:37  iter: 1000  loss: 4.4770 (6.6835)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.5132 (4.1161)  loss_bg: 0.7576 (2.5673)  time: 0.9945 (1.0585)  data: 0.0850 (0.1542)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:22:50  iter: 1100  loss: 4.2945 (6.4848)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4297 (4.0644)  loss_bg: 0.8813 (2.4205)  time: 0.9933 (1.0584)  data: 0.0842 (0.1541)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:20:31  iter: 1200  loss: 4.2448 (6.3155)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4014 (4.0182)  loss_bg: 0.8228 (2.2973)  time: 0.9879 (1.0562)  data: 0.0849 (0.1520)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:19:16  iter: 1300  loss: 4.2354 (6.1712)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3412 (3.9783)  loss_bg: 0.8678 (2.1929)  time: 1.0132 (1.0583)  data: 0.0872 (0.1540)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:17:27  iter: 1400  loss: 4.2387 (6.0446)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.5320 (3.9423)  loss_bg: 0.8788 (2.1023)  time: 0.9810 (1.0580)  data: 0.0850 (0.1538)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:15:34  iter: 1500  loss: 4.2807 (5.9311)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4661 (3.9094)  loss_bg: 0.8041 (2.0217)  time: 1.0007 (1.0575)  data: 0.0891 (0.1533)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:13:49  iter: 1600  loss: 4.3020 (5.8359)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4772 (3.8836)  loss_bg: 0.7720 (1.9523)  time: 1.0206 (1.0576)  data: 0.0865 (0.1531)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:11:53  iter: 1700  loss: 4.2498 (5.7484)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4346 (3.8561)  loss_bg: 0.8845 (1.8923)  time: 1.0074 (1.0569)  data: 0.0845 (0.1524)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:09:59  iter: 1800  loss: 4.1697 (5.6643)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2160 (3.8270)  loss_bg: 0.7411 (1.8372)  time: 0.9933 (1.0563)  data: 0.0864 (0.1517)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:08:28  iter: 1900  loss: 4.1991 (5.5904)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2532 (3.8023)  loss_bg: 0.8145 (1.7881)  time: 1.0000 (1.0574)  data: 0.0838 (0.1525)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:06:25  iter: 2000  loss: 4.1773 (5.5218)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2268 (3.7790)  loss_bg: 0.8365 (1.7427)  time: 0.9990 (1.0561)  data: 0.0832 (0.1512)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:04:27  iter: 2100  loss: 4.1150 (5.4593)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2902 (3.7573)  loss_bg: 0.9322 (1.7020)  time: 0.9973 (1.0552)  data: 0.0863 (0.1505)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:02:32  iter: 2200  loss: 3.9420 (5.4031)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2390 (3.7364)  loss_bg: 0.7514 (1.6667)  time: 1.0064 (1.0545)  data: 0.0851 (0.1502)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 4:00:30  iter: 2300  loss: 4.2328 (5.3509)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3154 (3.7189)  loss_bg: 0.8221 (1.6319)  time: 0.9930 (1.0533)  data: 0.0877 (0.1492)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:58:30  iter: 2400  loss: 4.2488 (5.3048)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2500 (3.7037)  loss_bg: 0.8373 (1.6011)  time: 1.0117 (1.0523)  data: 0.0846 (0.1480)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:56:54  iter: 2500  loss: 4.3484 (5.2641)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3343 (3.6908)  loss_bg: 0.8576 (1.5733)  time: 0.9925 (1.0529)  data: 0.0846 (0.1485)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:54:59  iter: 2600  loss: 4.0692 (5.2209)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1940 (3.6746)  loss_bg: 0.8233 (1.5463)  time: 0.9966 (1.0522)  data: 0.0852 (0.1477)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:53:02  iter: 2700  loss: 4.1164 (5.1818)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1285 (3.6602)  loss_bg: 0.7703 (1.5216)  time: 1.0023 (1.0513)  data: 0.0856 (0.1468)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:51:14  iter: 2800  loss: 4.2167 (5.1458)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3116 (3.6473)  loss_bg: 0.8413 (1.4984)  time: 1.0262 (1.0511)  data: 0.0883 (0.1468)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:49:18  iter: 2900  loss: 4.1170 (5.1115)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2171 (3.6343)  loss_bg: 0.8858 (1.4772)  time: 1.0084 (1.0503)  data: 0.0838 (0.1460)  lr: 0.016000  max mem: 10812
INFO:maskrcnn_benchmark:eta: 3:46:12  iter: 3000  loss: 3.9825 (nan)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1617 (nan)  loss_bg: 0.8653 (nan)  time: 1.0359 (1.0521)  data: 0.1557 (0.1480)  lr: 0.016000  max mem: 10812

INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.01', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'False', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'PredictorLoss100', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/Loss/PredictorLoss100-sgcls'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.01
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: PredictorLoss100
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/Loss/PredictorLoss100-sgcls
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/Loss/PredictorLoss100-sgcls/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Loading data statistics from: ./checkpoints/Loss/PredictorLoss100-sgcls/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from ./checkpoints/Loss/PredictorLoss100-sgcls/model_final.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Loading optimizer from ./checkpoints/Loss/PredictorLoss100-sgcls/model_final.pth
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.001', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'LxyPredictor1', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/LAMBDA/Lxy1-predcls0.001'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.001
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: LxyPredictor1
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/LAMBDA/Lxy1-predcls0.001
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/LAMBDA/Lxy1-predcls0.001/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/LAMBDA/Lxy1-predcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/LAMBDA/Lxy1-predcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.weight of shape (5, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.weight of shape (11, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.weight of shape (20, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.weight of shape (39, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.weight of shape (5, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.weight of shape (11, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.weight of shape (20, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.weight of shape (39, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/LAMBDA/Lxy1-predcls0.001/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: inf, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: inf, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: inf, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: inf, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 3660179.50000, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 261768.87500, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 165614.50000, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 63001.29688, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 62602.37109, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 57362.33984, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 55915.27344, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 119.16109, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 2.13941, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2584293.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2560207.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2553874.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2550023.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1831590.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2584293.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2547595.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2540925.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1824926.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1745375.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1729979.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1790790.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1742902.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : inf, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : inf, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: inf, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: inf, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: inf, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: inf, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2568075.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2564506.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2555852.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2555852.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2555852.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2555852.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2547595.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2536603.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2522597.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1815543.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1812865.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1802599.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1801259.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1734541.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1728638.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1660061.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1646980.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1557466.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1541778.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 239462.35938, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 199187.82812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 180580.35938, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 170862.48438, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 151572.65625, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 69289.83594, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 69289.83594, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 63367.38281, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 63367.38281, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 51396.03906, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 51396.03906, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 50560.41406, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 45297.63281, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: nan, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: nan, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: nan, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 50560.41406, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 50.11031, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 46.54889, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 46.13401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 43.80591, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: nan, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 6:00:23  iter: 1  loss: 303.9484 (303.9484)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0013 (0.0013)  rel_ce_loss: 59.3001 (59.3001)  1_CE_loss: 17.1706 (17.1706)  2_CE_loss: 44.0865 (44.0865)  2_DKS_loss: 1.5581 (1.5581)  3_CE_loss: 23.3058 (23.3058)  3_DKS_loss: 3.1182 (3.1182)  4_CE_loss: 83.3092 (83.3092)  4_DKS_loss: 4.7956 (4.7956)  5_CE_loss: 59.7422 (59.7422)  5_DKS_loss: 7.5609 (7.5609)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.3515 (1.3515)  data: 0.1719 (0.1719)  lr: 0.001600  max mem: 7921
INFO:maskrcnn_benchmark:eta: 4:48:42  iter: 100  loss: 19.7603 (60.7943)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0027 (0.0029)  rel_ce_loss: 8.2519 (16.5720)  1_CE_loss: 0.0556 (1.9652)  2_CE_loss: 0.5906 (5.9258)  2_DKS_loss: 0.0000 (0.3483)  3_CE_loss: 0.7283 (3.6875)  3_DKS_loss: 0.0066 (0.7888)  4_CE_loss: 3.9761 (14.4054)  4_DKS_loss: 0.0261 (1.3522)  5_CE_loss: 4.2612 (12.2551)  5_DKS_loss: 1.5721 (3.4912)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0828 (1.0895)  data: 0.1159 (0.1128)  lr: 0.004451  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:45:34  iter: 200  loss: 14.9264 (39.7458)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0030)  rel_ce_loss: 5.9836 (11.9495)  1_CE_loss: 0.0176 (1.0393)  2_CE_loss: 0.4355 (3.3056)  2_DKS_loss: 0.0000 (0.1744)  3_CE_loss: 0.4221 (2.1595)  3_DKS_loss: 0.0089 (0.4092)  4_CE_loss: 2.7344 (9.1955)  4_DKS_loss: 0.0417 (0.7135)  5_CE_loss: 3.4649 (8.2913)  5_DKS_loss: 0.9709 (2.5050)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0847 (1.0844)  data: 0.0995 (0.1117)  lr: 0.007331  max mem: 9668
INFO:maskrcnn_benchmark:eta: 4:43:45  iter: 300  loss: 10.5683 (30.7711)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0029 (0.0031)  rel_ce_loss: 3.7302 (9.6016)  1_CE_loss: 0.0720 (0.7283)  2_CE_loss: 0.2667 (2.3758)  2_DKS_loss: 0.0002 (0.1169)  3_CE_loss: 0.3671 (1.5895)  3_DKS_loss: 0.0087 (0.2797)  4_CE_loss: 2.2098 (7.0994)  4_DKS_loss: 0.0319 (0.4971)  5_CE_loss: 2.0634 (6.5069)  5_DKS_loss: 0.5907 (1.9728)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0862 (1.0844)  data: 0.1020 (0.1111)  lr: 0.010211  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:41:55  iter: 400  loss: 9.4768 (25.5261)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0028 (0.0031)  rel_ce_loss: 3.7412 (8.1452)  1_CE_loss: 0.0655 (0.5682)  2_CE_loss: 0.3653 (1.8900)  2_DKS_loss: 0.0000 (0.0881)  3_CE_loss: 0.4565 (1.3068)  3_DKS_loss: 0.0024 (0.2144)  4_CE_loss: 1.6175 (5.8410)  4_DKS_loss: 0.0402 (0.3892)  5_CE_loss: 1.9135 (5.4370)  5_DKS_loss: 0.5892 (1.6431)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0811 (1.0843)  data: 0.0998 (0.1111)  lr: 0.013091  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:40:12  iter: 500  loss: 6.7577 (21.9597)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0027 (0.0031)  rel_ce_loss: 3.0805 (7.1472)  1_CE_loss: 0.0873 (0.4700)  2_CE_loss: 0.3204 (1.5714)  2_DKS_loss: 0.0001 (0.0709)  3_CE_loss: 0.3578 (1.1053)  3_DKS_loss: 0.0093 (0.1744)  4_CE_loss: 1.0862 (4.9471)  4_DKS_loss: 0.0379 (0.3215)  5_CE_loss: 1.3706 (4.7439)  5_DKS_loss: 0.4034 (1.4048)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0722 (1.0847)  data: 0.1037 (0.1113)  lr: 0.015971  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:38:15  iter: 600  loss: 5.3490 (19.3264)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0029 (0.0032)  rel_ce_loss: 2.5579 (6.3748)  1_CE_loss: 0.0187 (0.4030)  2_CE_loss: 0.1906 (1.3510)  2_DKS_loss: 0.0000 (0.0593)  3_CE_loss: 0.1213 (0.9609)  3_DKS_loss: 0.0048 (0.1477)  4_CE_loss: 0.8809 (4.3212)  4_DKS_loss: 0.0147 (0.2743)  5_CE_loss: 1.2932 (4.2021)  5_DKS_loss: 0.2405 (1.2288)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0838 (1.0841)  data: 0.1035 (0.1110)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:36:17  iter: 700  loss: 4.9660 (17.3363)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0032)  rel_ce_loss: 1.9942 (5.7861)  1_CE_loss: 0.0075 (0.3521)  2_CE_loss: 0.1829 (1.1889)  2_DKS_loss: 0.0001 (0.0511)  3_CE_loss: 0.1792 (0.8537)  3_DKS_loss: 0.0088 (0.1283)  4_CE_loss: 0.9235 (3.8377)  4_DKS_loss: 0.0317 (0.2399)  5_CE_loss: 1.2841 (3.8006)  5_DKS_loss: 0.2215 (1.0948)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0646 (1.0835)  data: 0.1001 (0.1107)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:34:24  iter: 800  loss: 4.6378 (15.7511)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0032)  rel_ce_loss: 2.0077 (5.3071)  1_CE_loss: 0.0592 (0.3130)  2_CE_loss: 0.1600 (1.0606)  2_DKS_loss: 0.0000 (0.0448)  3_CE_loss: 0.1506 (0.7696)  3_DKS_loss: 0.0024 (0.1134)  4_CE_loss: 0.7712 (3.4632)  4_DKS_loss: 0.0152 (0.2132)  5_CE_loss: 1.0870 (3.4737)  5_DKS_loss: 0.2026 (0.9895)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0610 (1.0832)  data: 0.1023 (0.1107)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:32:35  iter: 900  loss: 4.0398 (14.4843)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0032)  rel_ce_loss: 1.6437 (4.9214)  1_CE_loss: 0.0108 (0.2834)  2_CE_loss: 0.0965 (0.9604)  2_DKS_loss: 0.0000 (0.0399)  3_CE_loss: 0.1447 (0.7023)  3_DKS_loss: 0.0039 (0.1017)  4_CE_loss: 0.7775 (3.1671)  4_DKS_loss: 0.0193 (0.1924)  5_CE_loss: 0.9245 (3.2057)  5_DKS_loss: 0.2140 (0.9069)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0748 (1.0832)  data: 0.1021 (0.1108)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:30:51  iter: 1000  loss: 3.6870 (13.4349)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0029 (0.0032)  rel_ce_loss: 1.5232 (4.5972)  1_CE_loss: 0.0217 (0.2576)  2_CE_loss: 0.1055 (0.8767)  2_DKS_loss: 0.0000 (0.0359)  3_CE_loss: 0.1087 (0.6454)  3_DKS_loss: 0.0056 (0.0923)  4_CE_loss: 0.7724 (2.9264)  4_DKS_loss: 0.0146 (0.1754)  5_CE_loss: 0.8411 (2.9874)  5_DKS_loss: 0.1908 (0.8373)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.0834)  data: 0.1002 (0.1107)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:29:02  iter: 1100  loss: 3.9034 (12.5731)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0032)  rel_ce_loss: 1.5463 (4.3372)  1_CE_loss: 0.0217 (0.2371)  2_CE_loss: 0.0854 (0.8072)  2_DKS_loss: 0.0000 (0.0327)  3_CE_loss: 0.0980 (0.5979)  3_DKS_loss: 0.0026 (0.0845)  4_CE_loss: 0.6450 (2.7215)  4_DKS_loss: 0.0149 (0.1615)  5_CE_loss: 0.9256 (2.8101)  5_DKS_loss: 0.2220 (0.7803)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1039 (1.0834)  data: 0.1275 (0.1108)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:27:15  iter: 1200  loss: 3.8781 (11.8312)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0032)  rel_ce_loss: 1.7457 (4.1110)  1_CE_loss: 0.0096 (0.2194)  2_CE_loss: 0.0919 (0.7489)  2_DKS_loss: 0.0001 (0.0300)  3_CE_loss: 0.0903 (0.5580)  3_DKS_loss: 0.0040 (0.0779)  4_CE_loss: 0.6086 (2.5465)  4_DKS_loss: 0.0171 (0.1496)  5_CE_loss: 0.9342 (2.6562)  5_DKS_loss: 0.1975 (0.7303)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0461 (1.0835)  data: 0.1009 (0.1108)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:25:27  iter: 1300  loss: 3.4729 (11.1917)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0027 (0.0032)  rel_ce_loss: 1.4192 (3.9143)  1_CE_loss: 0.0054 (0.2044)  2_CE_loss: 0.0680 (0.6981)  2_DKS_loss: 0.0000 (0.0278)  3_CE_loss: 0.0869 (0.5237)  3_DKS_loss: 0.0032 (0.0723)  4_CE_loss: 0.5403 (2.3981)  4_DKS_loss: 0.0150 (0.1396)  5_CE_loss: 0.8646 (2.5221)  5_DKS_loss: 0.1330 (0.6880)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0953 (1.0835)  data: 0.1018 (0.1108)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:23:37  iter: 1400  loss: 2.9739 (10.6245)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 1.2658 (3.7369)  1_CE_loss: 0.0066 (0.1913)  2_CE_loss: 0.0758 (0.6550)  2_DKS_loss: 0.0000 (0.0258)  3_CE_loss: 0.0785 (0.4926)  3_DKS_loss: 0.0025 (0.0674)  4_CE_loss: 0.5221 (2.2689)  4_DKS_loss: 0.0103 (0.1307)  5_CE_loss: 0.7369 (2.4036)  5_DKS_loss: 0.1233 (0.6490)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0920 (1.0834)  data: 0.1025 (0.1108)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:21:46  iter: 1500  loss: 3.2225 (10.1351)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.3551 (3.5830)  1_CE_loss: 0.0114 (0.1801)  2_CE_loss: 0.0866 (0.6170)  2_DKS_loss: 0.0001 (0.0241)  3_CE_loss: 0.0736 (0.4663)  3_DKS_loss: 0.0046 (0.0633)  4_CE_loss: 0.5667 (2.1579)  4_DKS_loss: 0.0184 (0.1232)  5_CE_loss: 0.8184 (2.3017)  5_DKS_loss: 0.1369 (0.6151)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0698 (1.0832)  data: 0.1024 (0.1106)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:19:50  iter: 1600  loss: 2.8495 (9.6926)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.2945 (3.4443)  1_CE_loss: 0.0076 (0.1699)  2_CE_loss: 0.0501 (0.5839)  2_DKS_loss: 0.0000 (0.0226)  3_CE_loss: 0.0558 (0.4430)  3_DKS_loss: 0.0019 (0.0596)  4_CE_loss: 0.5345 (2.0564)  4_DKS_loss: 0.0105 (0.1164)  5_CE_loss: 0.7451 (2.2081)  5_DKS_loss: 0.1217 (0.5852)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0573 (1.0827)  data: 0.1004 (0.1106)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:18:06  iter: 1700  loss: 2.8687 (9.2998)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.3460 (3.3218)  1_CE_loss: 0.0060 (0.1610)  2_CE_loss: 0.0607 (0.5544)  2_DKS_loss: 0.0000 (0.0213)  3_CE_loss: 0.0554 (0.4221)  3_DKS_loss: 0.0018 (0.0563)  4_CE_loss: 0.4887 (1.9652)  4_DKS_loss: 0.0082 (0.1104)  5_CE_loss: 0.6554 (2.1254)  5_DKS_loss: 0.1466 (0.5585)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0923 (1.0830)  data: 0.1015 (0.1104)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:16:16  iter: 1800  loss: 2.9416 (8.9533)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 1.3481 (3.2125)  1_CE_loss: 0.0129 (0.1534)  2_CE_loss: 0.0769 (0.5284)  2_DKS_loss: 0.0002 (0.0201)  3_CE_loss: 0.0688 (0.4034)  3_DKS_loss: 0.0030 (0.0534)  4_CE_loss: 0.4543 (1.8869)  4_DKS_loss: 0.0070 (0.1051)  5_CE_loss: 0.7086 (2.0521)  5_DKS_loss: 0.0955 (0.5347)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0930 (1.0828)  data: 0.1093 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:14:31  iter: 1900  loss: 2.3264 (8.6233)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.1508 (3.1081)  1_CE_loss: 0.0063 (0.1462)  2_CE_loss: 0.0484 (0.5039)  2_DKS_loss: 0.0000 (0.0191)  3_CE_loss: 0.0603 (0.3861)  3_DKS_loss: 0.0021 (0.0507)  4_CE_loss: 0.4149 (1.8127)  4_DKS_loss: 0.0081 (0.1001)  5_CE_loss: 0.5639 (1.9814)  5_DKS_loss: 0.0782 (0.5117)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1047 (1.0831)  data: 0.1049 (0.1104)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:12:40  iter: 2000  loss: 2.7323 (8.3298)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 1.1556 (3.0140)  1_CE_loss: 0.0112 (0.1399)  2_CE_loss: 0.0623 (0.4826)  2_DKS_loss: 0.0000 (0.0181)  3_CE_loss: 0.0705 (0.3709)  3_DKS_loss: 0.0015 (0.0483)  4_CE_loss: 0.4681 (1.7459)  4_DKS_loss: 0.0067 (0.0957)  5_CE_loss: 0.6265 (1.9201)  5_DKS_loss: 0.0910 (0.4910)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0873 (1.0829)  data: 0.1038 (0.1104)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:10:52  iter: 2100  loss: 2.3826 (8.0565)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0033)  rel_ce_loss: 1.0873 (2.9258)  1_CE_loss: 0.0166 (0.1341)  2_CE_loss: 0.0477 (0.4630)  2_DKS_loss: 0.0000 (0.0173)  3_CE_loss: 0.0609 (0.3565)  3_DKS_loss: 0.0020 (0.0461)  4_CE_loss: 0.4271 (1.6848)  4_DKS_loss: 0.0092 (0.0916)  5_CE_loss: 0.6009 (1.8619)  5_DKS_loss: 0.0756 (0.4720)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1027 (1.0829)  data: 0.1052 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:09:02  iter: 2200  loss: 2.4287 (7.8022)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.0884 (2.8433)  1_CE_loss: 0.0115 (0.1290)  2_CE_loss: 0.0506 (0.4451)  2_DKS_loss: 0.0000 (0.0165)  3_CE_loss: 0.0560 (0.3435)  3_DKS_loss: 0.0018 (0.0441)  4_CE_loss: 0.4531 (1.6285)  4_DKS_loss: 0.0108 (0.0879)  5_CE_loss: 0.5897 (1.8064)  5_DKS_loss: 0.0753 (0.4544)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0775 (1.0828)  data: 0.0988 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:07:14  iter: 2300  loss: 2.2642 (7.5654)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 0.9944 (2.7659)  1_CE_loss: 0.0023 (0.1240)  2_CE_loss: 0.0577 (0.4287)  2_DKS_loss: 0.0001 (0.0158)  3_CE_loss: 0.0479 (0.3313)  3_DKS_loss: 0.0024 (0.0423)  4_CE_loss: 0.3877 (1.5762)  4_DKS_loss: 0.0092 (0.0846)  5_CE_loss: 0.5480 (1.7548)  5_DKS_loss: 0.0744 (0.4384)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0890 (1.0828)  data: 0.1223 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:05:28  iter: 2400  loss: 2.0338 (7.3435)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 0.8978 (2.6923)  1_CE_loss: 0.0092 (0.1194)  2_CE_loss: 0.0447 (0.4136)  2_DKS_loss: 0.0000 (0.0152)  3_CE_loss: 0.0529 (0.3200)  3_DKS_loss: 0.0017 (0.0407)  4_CE_loss: 0.3690 (1.5283)  4_DKS_loss: 0.0075 (0.0815)  5_CE_loss: 0.5126 (1.7065)  5_DKS_loss: 0.0611 (0.4228)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0721 (1.0830)  data: 0.1003 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:03:43  iter: 2500  loss: 2.1330 (7.1386)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.8729 (2.6250)  1_CE_loss: 0.0097 (0.1153)  2_CE_loss: 0.0361 (0.3993)  2_DKS_loss: 0.0001 (0.0146)  3_CE_loss: 0.0395 (0.3096)  3_DKS_loss: 0.0022 (0.0392)  4_CE_loss: 0.4132 (1.4831)  4_DKS_loss: 0.0094 (0.0787)  5_CE_loss: 0.5875 (1.6621)  5_DKS_loss: 0.0561 (0.4085)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0767 (1.0832)  data: 0.1085 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:01:50  iter: 2600  loss: 2.0580 (6.9475)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.9070 (2.5620)  1_CE_loss: 0.0085 (0.1115)  2_CE_loss: 0.0396 (0.3860)  2_DKS_loss: 0.0001 (0.0140)  3_CE_loss: 0.0414 (0.2997)  3_DKS_loss: 0.0018 (0.0377)  4_CE_loss: 0.3589 (1.4417)  4_DKS_loss: 0.0075 (0.0760)  5_CE_loss: 0.5016 (1.6205)  5_DKS_loss: 0.0489 (0.3950)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0717 (1.0829)  data: 0.1009 (0.1102)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 4:00:02  iter: 2700  loss: 1.9370 (6.7666)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 0.8739 (2.5018)  1_CE_loss: 0.0102 (0.1079)  2_CE_loss: 0.0388 (0.3738)  2_DKS_loss: 0.0001 (0.0135)  3_CE_loss: 0.0386 (0.2909)  3_DKS_loss: 0.0019 (0.0364)  4_CE_loss: 0.3432 (1.4022)  4_DKS_loss: 0.0077 (0.0735)  5_CE_loss: 0.5180 (1.5809)  5_DKS_loss: 0.0380 (0.3824)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0993 (1.0829)  data: 0.1011 (0.1102)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:58:18  iter: 2800  loss: 1.9335 (6.5947)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 0.9104 (2.4442)  1_CE_loss: 0.0073 (0.1046)  2_CE_loss: 0.0495 (0.3623)  2_DKS_loss: 0.0002 (0.0130)  3_CE_loss: 0.0390 (0.2823)  3_DKS_loss: 0.0029 (0.0352)  4_CE_loss: 0.3416 (1.3654)  4_DKS_loss: 0.0100 (0.0712)  5_CE_loss: 0.4630 (1.5427)  5_DKS_loss: 0.0430 (0.3705)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1002 (1.0832)  data: 0.1061 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:56:33  iter: 2900  loss: 2.4736 (6.4432)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.1427 (2.3941)  1_CE_loss: 0.0124 (0.1015)  2_CE_loss: 0.0465 (0.3516)  2_DKS_loss: 0.0000 (0.0126)  3_CE_loss: 0.0422 (0.2745)  3_DKS_loss: 0.0049 (0.0342)  4_CE_loss: 0.3527 (1.3326)  4_DKS_loss: 0.0087 (0.0691)  5_CE_loss: 0.6174 (1.5097)  5_DKS_loss: 0.0949 (0.3601)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0894 (1.0835)  data: 0.1051 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:54:44  iter: 3000  loss: 2.0692 (6.3033)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.8795 (2.3486)  1_CE_loss: 0.0095 (0.0987)  2_CE_loss: 0.0604 (0.3420)  2_DKS_loss: 0.0001 (0.0122)  3_CE_loss: 0.0572 (0.2671)  3_DKS_loss: 0.0037 (0.0333)  4_CE_loss: 0.3805 (1.3007)  4_DKS_loss: 0.0106 (0.0672)  5_CE_loss: 0.4821 (1.4804)  5_DKS_loss: 0.0416 (0.3499)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0813 (1.0835)  data: 0.1027 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:52:55  iter: 3100  loss: 1.7578 (6.1620)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.7361 (2.3010)  1_CE_loss: 0.0068 (0.0960)  2_CE_loss: 0.0375 (0.3325)  2_DKS_loss: 0.0001 (0.0118)  3_CE_loss: 0.0388 (0.2600)  3_DKS_loss: 0.0023 (0.0323)  4_CE_loss: 0.3260 (1.2706)  4_DKS_loss: 0.0076 (0.0653)  5_CE_loss: 0.4361 (1.4486)  5_DKS_loss: 0.0323 (0.3406)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0846 (1.0834)  data: 0.1072 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:51:05  iter: 3200  loss: 1.8966 (6.0279)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.8283 (2.2552)  1_CE_loss: 0.0090 (0.0934)  2_CE_loss: 0.0458 (0.3238)  2_DKS_loss: 0.0003 (0.0114)  3_CE_loss: 0.0481 (0.2536)  3_DKS_loss: 0.0028 (0.0314)  4_CE_loss: 0.3908 (1.2426)  4_DKS_loss: 0.0075 (0.0635)  5_CE_loss: 0.4483 (1.4183)  5_DKS_loss: 0.0494 (0.3314)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0597 (1.0832)  data: 0.1033 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:49:17  iter: 3300  loss: 1.8006 (5.9004)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 0.8114 (2.2123)  1_CE_loss: 0.0063 (0.0910)  2_CE_loss: 0.0436 (0.3155)  2_DKS_loss: 0.0003 (0.0111)  3_CE_loss: 0.0436 (0.2474)  3_DKS_loss: 0.0072 (0.0306)  4_CE_loss: 0.3536 (1.2154)  4_DKS_loss: 0.0112 (0.0619)  5_CE_loss: 0.4725 (1.3892)  5_DKS_loss: 0.0351 (0.3227)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0604 (1.0833)  data: 0.0972 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:47:27  iter: 3400  loss: 1.9128 (5.7831)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 0.8517 (2.1722)  1_CE_loss: 0.0084 (0.0887)  2_CE_loss: 0.0376 (0.3078)  2_DKS_loss: 0.0004 (0.0108)  3_CE_loss: 0.0360 (0.2415)  3_DKS_loss: 0.0020 (0.0299)  4_CE_loss: 0.3740 (1.1902)  4_DKS_loss: 0.0055 (0.0603)  5_CE_loss: 0.4941 (1.3633)  5_DKS_loss: 0.0508 (0.3150)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0792 (1.0831)  data: 0.1082 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:45:37  iter: 3500  loss: 2.0004 (5.6715)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.8781 (2.1345)  1_CE_loss: 0.0152 (0.0866)  2_CE_loss: 0.0529 (0.3003)  2_DKS_loss: 0.0002 (0.0105)  3_CE_loss: 0.0413 (0.2358)  3_DKS_loss: 0.0037 (0.0292)  4_CE_loss: 0.3440 (1.1663)  4_DKS_loss: 0.0069 (0.0589)  5_CE_loss: 0.5074 (1.3385)  5_DKS_loss: 0.0487 (0.3076)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0764 (1.0830)  data: 0.1084 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:43:47  iter: 3600  loss: 1.8831 (5.5657)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 0.8381 (2.0984)  1_CE_loss: 0.0061 (0.0846)  2_CE_loss: 0.0510 (0.2935)  2_DKS_loss: 0.0002 (0.0102)  3_CE_loss: 0.0472 (0.2307)  3_DKS_loss: 0.0038 (0.0286)  4_CE_loss: 0.3375 (1.1438)  4_DKS_loss: 0.0087 (0.0575)  5_CE_loss: 0.4219 (1.3146)  5_DKS_loss: 0.0477 (0.3004)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0997 (1.0829)  data: 0.1145 (0.1103)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:41:50  iter: 3700  loss: 1.5805 (5.4620)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7716 (2.0630)  1_CE_loss: 0.0095 (0.0828)  2_CE_loss: 0.0437 (0.2869)  2_DKS_loss: 0.0002 (0.0099)  3_CE_loss: 0.0448 (0.2257)  3_DKS_loss: 0.0015 (0.0279)  4_CE_loss: 0.2941 (1.1219)  4_DKS_loss: 0.0057 (0.0562)  5_CE_loss: 0.4194 (1.2910)  5_DKS_loss: 0.0257 (0.2934)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0664 (1.0822)  data: 0.0858 (0.1097)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:39:55  iter: 3800  loss: 1.7433 (5.3639)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7864 (2.0293)  1_CE_loss: 0.0168 (0.0810)  2_CE_loss: 0.0450 (0.2807)  2_DKS_loss: 0.0002 (0.0097)  3_CE_loss: 0.0368 (0.2210)  3_DKS_loss: 0.0019 (0.0272)  4_CE_loss: 0.3015 (1.1010)  4_DKS_loss: 0.0048 (0.0549)  5_CE_loss: 0.4461 (1.2690)  5_DKS_loss: 0.0415 (0.2867)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0816)  data: 0.0857 (0.1091)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:37:58  iter: 3900  loss: 1.6466 (5.2697)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7817 (1.9971)  1_CE_loss: 0.0040 (0.0793)  2_CE_loss: 0.0447 (0.2747)  2_DKS_loss: 0.0002 (0.0094)  3_CE_loss: 0.0355 (0.2165)  3_DKS_loss: 0.0027 (0.0266)  4_CE_loss: 0.3108 (1.0812)  4_DKS_loss: 0.0073 (0.0537)  5_CE_loss: 0.4067 (1.2475)  5_DKS_loss: 0.0337 (0.2803)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0526 (1.0809)  data: 0.0867 (0.1085)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:---Total norm 6.30465 clip coef 0.79306-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 3.12101, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.16342, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.82873, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.69395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.53294, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.21086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.15547, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.03014, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.93072, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.91335, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.86811, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.84998, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.81487, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.77908, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.70960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.70960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.70960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.70960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.68766, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.66836, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.63888, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.61671, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.57542, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.53566, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.52073, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.49115, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.48842, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.45654, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.44465, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.39608, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.37229, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.36398, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.33602, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.30548, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.30382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29152, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.28974, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.28674, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28631, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28393, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.27680, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27159, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25432, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.25100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24439, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.24086, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23857, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22662, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22498, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.22352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22122, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.21743, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.21580, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.21389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20392, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20034, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19717, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19578, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18327, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17600, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.17049, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.15966, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.15516, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.15516, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.15022, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14965, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14897, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14863, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13793, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.13333, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13124, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.12659, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12067, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11617, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11592, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11444, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11319, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11208, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11082, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.10773, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10362, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10360, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10338, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10275, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09996, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.09286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09140, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08819, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.08782, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08695, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08634, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08477, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08477, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08414, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08403, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08232, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08227, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08197, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08184, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08124, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08095, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08029, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07988, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07917, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07853, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07816, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07806, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07643, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07386, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07211, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07185, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07054, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06994, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06850, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06840, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06802, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06716, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06705, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06466, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.06281, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06217, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06108, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05731, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05714, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05679, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05679, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05668, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05495, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05467, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05442, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05419, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05217, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05155, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04964, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04663, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04643, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.04526, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04479, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.04384, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04363, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04277, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04226, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04117, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04106, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04102, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04071, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04064, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04035, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04018, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03976, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03965, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03831, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03803, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03706, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03706, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03611, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03611, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.03586, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.03502, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03458, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03262, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03207, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03202, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03137, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03124, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03056, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03039, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03019, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02985, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02962, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02830, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02830, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.02745, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.02745, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02703, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.02646, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02515, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02495, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02482, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02290, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.02287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02282, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02201, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02194, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02182, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02181, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02138, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02109, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02092, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02087, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.02061, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.02061, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02036, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01989, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01989, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01910, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01881, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01852, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01817, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01797, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01758, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01718, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01702, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01600, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01596, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01594, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01572, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01566, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01515, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01473, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01463, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01420, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01347, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01285, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01269, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01266, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01214, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01214, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01191, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.01182, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01132, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01054, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01048, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00945, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.00898, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00829, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00822, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00821, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00819, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00803, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00797, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00780, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00761, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00717, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00714, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00709, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00705, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00703, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00699, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00668, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00636, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00615, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00569, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00569, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00554, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00553, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00553, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00552, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00551, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00547, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00545, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00542, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00540, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00539, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00537, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00537, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00531, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00531, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00528, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00528, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00528, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00525, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00524, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00523, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00517, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00517, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00515, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00510, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00502, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00502, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00502, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00422, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00382, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00367, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00276, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00276, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00276, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00274, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00264, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00259, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00230, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00213, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00213, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00212, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00210, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00207, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00206, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00204, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00204, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00202, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00201, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00201, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00200, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00198, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00196, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00196, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00195, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00195, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00193, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00192, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00192, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00191, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00189, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00189, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00189, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00175, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00174, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00171, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00153, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00127, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00112, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00093, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00091, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00091, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00059, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00057, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00053, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00049, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00039, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00036, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00032, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00015, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:36:01  iter: 4000  loss: 1.6523 (5.1811)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7572 (1.9666)  1_CE_loss: 0.0095 (0.0776)  2_CE_loss: 0.0423 (0.2691)  2_DKS_loss: 0.0004 (0.0092)  3_CE_loss: 0.0435 (0.2122)  3_DKS_loss: 0.0032 (0.0261)  4_CE_loss: 0.3325 (1.0628)  4_DKS_loss: 0.0068 (0.0526)  5_CE_loss: 0.4416 (1.2273)  5_DKS_loss: 0.0283 (0.2742)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0510 (1.0802)  data: 0.0854 (0.1080)  lr: 0.016000  max mem: 10865
INFO:maskrcnn_benchmark:eta: 3:34:09  iter: 4100  loss: 1.7328 (5.0949)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7625 (1.9373)  1_CE_loss: 0.0083 (0.0760)  2_CE_loss: 0.0471 (0.2637)  2_DKS_loss: 0.0002 (0.0090)  3_CE_loss: 0.0509 (0.2081)  3_DKS_loss: 0.0024 (0.0255)  4_CE_loss: 0.3482 (1.0446)  4_DKS_loss: 0.0078 (0.0515)  5_CE_loss: 0.4151 (1.2077)  5_DKS_loss: 0.0267 (0.2682)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0490 (1.0798)  data: 0.0885 (0.1075)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:32:14  iter: 4200  loss: 1.5651 (5.0121)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7229 (1.9089)  1_CE_loss: 0.0026 (0.0745)  2_CE_loss: 0.0358 (0.2584)  2_DKS_loss: 0.0002 (0.0088)  3_CE_loss: 0.0344 (0.2041)  3_DKS_loss: 0.0020 (0.0250)  4_CE_loss: 0.2894 (1.0273)  4_DKS_loss: 0.0058 (0.0505)  5_CE_loss: 0.4049 (1.1888)  5_DKS_loss: 0.0240 (0.2625)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0653 (1.0792)  data: 0.0899 (0.1071)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:30:22  iter: 4300  loss: 1.5718 (4.9333)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7362 (1.8817)  1_CE_loss: 0.0097 (0.0731)  2_CE_loss: 0.0491 (0.2533)  2_DKS_loss: 0.0002 (0.0086)  3_CE_loss: 0.0323 (0.2004)  3_DKS_loss: 0.0022 (0.0245)  4_CE_loss: 0.2962 (1.0109)  4_DKS_loss: 0.0107 (0.0495)  5_CE_loss: 0.3836 (1.1707)  5_DKS_loss: 0.0274 (0.2572)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0506 (1.0788)  data: 0.0871 (0.1066)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:28:28  iter: 4400  loss: 1.4683 (4.8595)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7251 (1.8564)  1_CE_loss: 0.0088 (0.0717)  2_CE_loss: 0.0214 (0.2486)  2_DKS_loss: 0.0002 (0.0084)  3_CE_loss: 0.0258 (0.1968)  3_DKS_loss: 0.0015 (0.0240)  4_CE_loss: 0.2828 (0.9954)  4_DKS_loss: 0.0052 (0.0486)  5_CE_loss: 0.3682 (1.1542)  5_DKS_loss: 0.0172 (0.2521)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0552 (1.0784)  data: 0.0851 (0.1062)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:26:35  iter: 4500  loss: 1.6548 (4.7885)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7020 (1.8319)  1_CE_loss: 0.0114 (0.0704)  2_CE_loss: 0.0335 (0.2440)  2_DKS_loss: 0.0002 (0.0082)  3_CE_loss: 0.0321 (0.1933)  3_DKS_loss: 0.0015 (0.0235)  4_CE_loss: 0.3313 (0.9806)  4_DKS_loss: 0.0066 (0.0477)  5_CE_loss: 0.4104 (1.1382)  5_DKS_loss: 0.0276 (0.2473)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0563 (1.0779)  data: 0.0872 (0.1058)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:24:43  iter: 4600  loss: 1.7619 (4.7224)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7816 (1.8095)  1_CE_loss: 0.0137 (0.0692)  2_CE_loss: 0.0452 (0.2398)  2_DKS_loss: 0.0003 (0.0081)  3_CE_loss: 0.0386 (0.1902)  3_DKS_loss: 0.0024 (0.0231)  4_CE_loss: 0.3179 (0.9666)  4_DKS_loss: 0.0078 (0.0469)  5_CE_loss: 0.4185 (1.1232)  5_DKS_loss: 0.0333 (0.2426)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0692 (1.0775)  data: 0.0850 (0.1054)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:22:51  iter: 4700  loss: 1.6450 (4.6582)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7628 (1.7874)  1_CE_loss: 0.0038 (0.0680)  2_CE_loss: 0.0389 (0.2357)  2_DKS_loss: 0.0003 (0.0079)  3_CE_loss: 0.0380 (0.1871)  3_DKS_loss: 0.0027 (0.0227)  4_CE_loss: 0.2943 (0.9528)  4_DKS_loss: 0.0063 (0.0460)  5_CE_loss: 0.4046 (1.1089)  5_DKS_loss: 0.0324 (0.2383)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0649 (1.0772)  data: 0.0863 (0.1050)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:20:59  iter: 4800  loss: 1.6156 (4.5953)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7327 (1.7657)  1_CE_loss: 0.0121 (0.0670)  2_CE_loss: 0.0359 (0.2318)  2_DKS_loss: 0.0003 (0.0077)  3_CE_loss: 0.0366 (0.1841)  3_DKS_loss: 0.0028 (0.0223)  4_CE_loss: 0.2872 (0.9397)  4_DKS_loss: 0.0076 (0.0452)  5_CE_loss: 0.4048 (1.0947)  5_DKS_loss: 0.0266 (0.2339)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0558 (1.0767)  data: 0.0849 (0.1046)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:19:07  iter: 4900  loss: 1.6601 (4.5355)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7336 (1.7450)  1_CE_loss: 0.0066 (0.0659)  2_CE_loss: 0.0490 (0.2281)  2_DKS_loss: 0.0003 (0.0076)  3_CE_loss: 0.0366 (0.1813)  3_DKS_loss: 0.0030 (0.0219)  4_CE_loss: 0.3257 (0.9270)  4_DKS_loss: 0.0078 (0.0445)  5_CE_loss: 0.4219 (1.0812)  5_DKS_loss: 0.0216 (0.2298)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0676 (1.0763)  data: 0.0867 (0.1043)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:17:15  iter: 5000  loss: 1.6181 (4.4769)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7013 (1.7248)  1_CE_loss: 0.0105 (0.0648)  2_CE_loss: 0.0402 (0.2244)  2_DKS_loss: 0.0001 (0.0074)  3_CE_loss: 0.0412 (0.1785)  3_DKS_loss: 0.0033 (0.0215)  4_CE_loss: 0.2865 (0.9146)  4_DKS_loss: 0.0079 (0.0438)  5_CE_loss: 0.3843 (1.0678)  5_DKS_loss: 0.0295 (0.2258)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0655 (1.0760)  data: 0.0862 (0.1039)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:15:24  iter: 5100  loss: 1.5300 (4.4219)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7112 (1.7060)  1_CE_loss: 0.0101 (0.0638)  2_CE_loss: 0.0279 (0.2210)  2_DKS_loss: 0.0002 (0.0073)  3_CE_loss: 0.0336 (0.1759)  3_DKS_loss: 0.0008 (0.0211)  4_CE_loss: 0.3104 (0.9031)  4_DKS_loss: 0.0051 (0.0431)  5_CE_loss: 0.3751 (1.0555)  5_DKS_loss: 0.0225 (0.2220)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0387 (1.0756)  data: 0.0869 (0.1036)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:13:32  iter: 5200  loss: 1.7225 (4.3683)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7106 (1.6873)  1_CE_loss: 0.0115 (0.0629)  2_CE_loss: 0.0394 (0.2177)  2_DKS_loss: 0.0004 (0.0072)  3_CE_loss: 0.0426 (0.1735)  3_DKS_loss: 0.0023 (0.0208)  4_CE_loss: 0.3724 (0.8920)  4_DKS_loss: 0.0072 (0.0424)  5_CE_loss: 0.4068 (1.0431)  5_DKS_loss: 0.0317 (0.2183)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0554 (1.0752)  data: 0.0863 (0.1034)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:11:42  iter: 5300  loss: 1.6056 (4.3147)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7331 (1.6689)  1_CE_loss: 0.0012 (0.0619)  2_CE_loss: 0.0361 (0.2144)  2_DKS_loss: 0.0001 (0.0070)  3_CE_loss: 0.0409 (0.1710)  3_DKS_loss: 0.0024 (0.0204)  4_CE_loss: 0.3007 (0.8808)  4_DKS_loss: 0.0058 (0.0417)  5_CE_loss: 0.3918 (1.0309)  5_DKS_loss: 0.0204 (0.2147)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0700 (1.0750)  data: 0.0873 (0.1031)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:09:52  iter: 5400  loss: 1.5686 (4.2637)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7276 (1.6514)  1_CE_loss: 0.0114 (0.0610)  2_CE_loss: 0.0423 (0.2113)  2_DKS_loss: 0.0003 (0.0069)  3_CE_loss: 0.0476 (0.1686)  3_DKS_loss: 0.0019 (0.0201)  4_CE_loss: 0.3123 (0.8701)  4_DKS_loss: 0.0044 (0.0410)  5_CE_loss: 0.3829 (1.0191)  5_DKS_loss: 0.0194 (0.2111)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0631 (1.0748)  data: 0.0888 (0.1028)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:08:02  iter: 5500  loss: 1.5489 (4.2151)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7094 (1.6346)  1_CE_loss: 0.0037 (0.0601)  2_CE_loss: 0.0420 (0.2083)  2_DKS_loss: 0.0002 (0.0068)  3_CE_loss: 0.0476 (0.1663)  3_DKS_loss: 0.0028 (0.0198)  4_CE_loss: 0.2926 (0.8601)  4_DKS_loss: 0.0053 (0.0404)  5_CE_loss: 0.3540 (1.0079)  5_DKS_loss: 0.0176 (0.2076)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0532 (1.0745)  data: 0.0860 (0.1026)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:06:13  iter: 5600  loss: 1.5664 (4.1673)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7214 (1.6182)  1_CE_loss: 0.0115 (0.0593)  2_CE_loss: 0.0370 (0.2053)  2_DKS_loss: 0.0002 (0.0067)  3_CE_loss: 0.0293 (0.1640)  3_DKS_loss: 0.0024 (0.0195)  4_CE_loss: 0.3091 (0.8501)  4_DKS_loss: 0.0051 (0.0398)  5_CE_loss: 0.3715 (0.9969)  5_DKS_loss: 0.0187 (0.2044)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0478 (1.0743)  data: 0.0888 (0.1023)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:04:21  iter: 5700  loss: 1.4995 (4.1217)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6949 (1.6023)  1_CE_loss: 0.0017 (0.0584)  2_CE_loss: 0.0352 (0.2025)  2_DKS_loss: 0.0002 (0.0066)  3_CE_loss: 0.0272 (0.1618)  3_DKS_loss: 0.0025 (0.0192)  4_CE_loss: 0.3036 (0.8408)  4_DKS_loss: 0.0084 (0.0392)  5_CE_loss: 0.3808 (0.9865)  5_DKS_loss: 0.0250 (0.2012)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0530 (1.0740)  data: 0.0868 (0.1021)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:02:31  iter: 5800  loss: 1.5297 (4.0776)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6780 (1.5870)  1_CE_loss: 0.0023 (0.0576)  2_CE_loss: 0.0211 (0.1997)  2_DKS_loss: 0.0003 (0.0064)  3_CE_loss: 0.0224 (0.1598)  3_DKS_loss: 0.0020 (0.0189)  4_CE_loss: 0.3113 (0.8316)  4_DKS_loss: 0.0063 (0.0387)  5_CE_loss: 0.4107 (0.9765)  5_DKS_loss: 0.0219 (0.1982)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0598 (1.0737)  data: 0.0879 (0.1019)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 3:00:41  iter: 5900  loss: 1.4700 (4.0338)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7119 (1.5718)  1_CE_loss: 0.0033 (0.0568)  2_CE_loss: 0.0330 (0.1970)  2_DKS_loss: 0.0002 (0.0063)  3_CE_loss: 0.0308 (0.1577)  3_DKS_loss: 0.0020 (0.0186)  4_CE_loss: 0.2777 (0.8225)  4_DKS_loss: 0.0049 (0.0381)  5_CE_loss: 0.3734 (0.9664)  5_DKS_loss: 0.0201 (0.1952)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0477 (1.0734)  data: 0.0910 (0.1017)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:58:51  iter: 6000  loss: 1.5646 (3.9935)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7167 (1.5578)  1_CE_loss: 0.0044 (0.0560)  2_CE_loss: 0.0323 (0.1944)  2_DKS_loss: 0.0001 (0.0062)  3_CE_loss: 0.0288 (0.1558)  3_DKS_loss: 0.0021 (0.0184)  4_CE_loss: 0.2913 (0.8143)  4_DKS_loss: 0.0045 (0.0376)  5_CE_loss: 0.3915 (0.9572)  5_DKS_loss: 0.0251 (0.1924)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0528 (1.0731)  data: 0.0872 (0.1014)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:57:01  iter: 6100  loss: 1.4901 (3.9542)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0034)  rel_ce_loss: 0.7018 (1.5442)  1_CE_loss: 0.0025 (0.0553)  2_CE_loss: 0.0373 (0.1920)  2_DKS_loss: 0.0001 (0.0061)  3_CE_loss: 0.0323 (0.1539)  3_DKS_loss: 0.0019 (0.0181)  4_CE_loss: 0.2670 (0.8062)  4_DKS_loss: 0.0050 (0.0371)  5_CE_loss: 0.3753 (0.9484)  5_DKS_loss: 0.0235 (0.1897)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0804 (1.0728)  data: 0.0876 (0.1012)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:55:11  iter: 6200  loss: 1.4522 (3.9154)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6326 (1.5307)  1_CE_loss: 0.0063 (0.0546)  2_CE_loss: 0.0482 (0.1896)  2_DKS_loss: 0.0002 (0.0060)  3_CE_loss: 0.0363 (0.1521)  3_DKS_loss: 0.0037 (0.0179)  4_CE_loss: 0.2642 (0.7982)  4_DKS_loss: 0.0047 (0.0366)  5_CE_loss: 0.3564 (0.9394)  5_DKS_loss: 0.0222 (0.1870)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0647 (1.0726)  data: 0.0872 (0.1010)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:53:22  iter: 6300  loss: 1.4821 (3.8782)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7158 (1.5179)  1_CE_loss: 0.0077 (0.0539)  2_CE_loss: 0.0326 (0.1873)  2_DKS_loss: 0.0001 (0.0060)  3_CE_loss: 0.0355 (0.1503)  3_DKS_loss: 0.0018 (0.0176)  4_CE_loss: 0.2881 (0.7904)  4_DKS_loss: 0.0051 (0.0361)  5_CE_loss: 0.3798 (0.9309)  5_DKS_loss: 0.0186 (0.1845)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0674 (1.0724)  data: 0.0899 (0.1009)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:51:31  iter: 6400  loss: 1.5237 (3.8419)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7423 (1.5052)  1_CE_loss: 0.0036 (0.0533)  2_CE_loss: 0.0339 (0.1851)  2_DKS_loss: 0.0001 (0.0059)  3_CE_loss: 0.0324 (0.1486)  3_DKS_loss: 0.0021 (0.0174)  4_CE_loss: 0.2833 (0.7828)  4_DKS_loss: 0.0048 (0.0356)  5_CE_loss: 0.3854 (0.9226)  5_DKS_loss: 0.0230 (0.1820)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0572 (1.0721)  data: 0.0869 (0.1007)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:49:42  iter: 6500  loss: 1.5211 (3.8065)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7207 (1.4930)  1_CE_loss: 0.0174 (0.0527)  2_CE_loss: 0.0421 (0.1829)  2_DKS_loss: 0.0002 (0.0058)  3_CE_loss: 0.0288 (0.1469)  3_DKS_loss: 0.0032 (0.0172)  4_CE_loss: 0.2942 (0.7754)  4_DKS_loss: 0.0059 (0.0352)  5_CE_loss: 0.3765 (0.9146)  5_DKS_loss: 0.0286 (0.1796)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0574 (1.0719)  data: 0.0889 (0.1005)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:47:53  iter: 6600  loss: 1.5531 (3.7725)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7131 (1.4811)  1_CE_loss: 0.0154 (0.0520)  2_CE_loss: 0.0382 (0.1808)  2_DKS_loss: 0.0002 (0.0057)  3_CE_loss: 0.0342 (0.1453)  3_DKS_loss: 0.0017 (0.0170)  4_CE_loss: 0.3447 (0.7684)  4_DKS_loss: 0.0045 (0.0348)  5_CE_loss: 0.3955 (0.9069)  5_DKS_loss: 0.0164 (0.1772)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0409 (1.0716)  data: 0.0871 (0.1003)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:46:03  iter: 6700  loss: 1.5618 (3.7405)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7318 (1.4700)  1_CE_loss: 0.0095 (0.0514)  2_CE_loss: 0.0335 (0.1787)  2_DKS_loss: 0.0003 (0.0056)  3_CE_loss: 0.0389 (0.1438)  3_DKS_loss: 0.0019 (0.0168)  4_CE_loss: 0.3092 (0.7618)  4_DKS_loss: 0.0058 (0.0343)  5_CE_loss: 0.3842 (0.8997)  5_DKS_loss: 0.0209 (0.1751)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0495 (1.0714)  data: 0.0869 (0.1002)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:44:14  iter: 6800  loss: 1.5284 (3.7087)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6752 (1.4590)  1_CE_loss: 0.0147 (0.0509)  2_CE_loss: 0.0423 (0.1768)  2_DKS_loss: 0.0002 (0.0055)  3_CE_loss: 0.0362 (0.1423)  3_DKS_loss: 0.0013 (0.0166)  4_CE_loss: 0.2972 (0.7553)  4_DKS_loss: 0.0048 (0.0339)  5_CE_loss: 0.3974 (0.8924)  5_DKS_loss: 0.0196 (0.1729)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0647 (1.0711)  data: 0.0884 (0.1000)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:42:25  iter: 6900  loss: 1.6081 (3.6775)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0034)  rel_ce_loss: 0.6929 (1.4481)  1_CE_loss: 0.0103 (0.0503)  2_CE_loss: 0.0334 (0.1749)  2_DKS_loss: 0.0001 (0.0055)  3_CE_loss: 0.0300 (0.1408)  3_DKS_loss: 0.0026 (0.0164)  4_CE_loss: 0.2922 (0.7487)  4_DKS_loss: 0.0081 (0.0335)  5_CE_loss: 0.3943 (0.8854)  5_DKS_loss: 0.0187 (0.1707)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0583 (1.0709)  data: 0.0893 (0.0998)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:40:37  iter: 7000  loss: 1.5019 (3.6469)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6193 (1.4375)  1_CE_loss: 0.0024 (0.0498)  2_CE_loss: 0.0342 (0.1730)  2_DKS_loss: 0.0001 (0.0054)  3_CE_loss: 0.0266 (0.1393)  3_DKS_loss: 0.0013 (0.0162)  4_CE_loss: 0.3011 (0.7423)  4_DKS_loss: 0.0033 (0.0331)  5_CE_loss: 0.3869 (0.8785)  5_DKS_loss: 0.0259 (0.1686)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0538 (1.0708)  data: 0.0869 (0.0997)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:38:48  iter: 7100  loss: 1.4750 (3.6168)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6873 (1.4271)  1_CE_loss: 0.0079 (0.0492)  2_CE_loss: 0.0270 (0.1711)  2_DKS_loss: 0.0002 (0.0053)  3_CE_loss: 0.0367 (0.1379)  3_DKS_loss: 0.0014 (0.0160)  4_CE_loss: 0.2774 (0.7360)  4_DKS_loss: 0.0036 (0.0327)  5_CE_loss: 0.3559 (0.8717)  5_DKS_loss: 0.0229 (0.1665)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0646 (1.0707)  data: 0.0875 (0.0995)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:37:00  iter: 7200  loss: 1.4002 (3.5876)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0034)  rel_ce_loss: 0.6502 (1.4170)  1_CE_loss: 0.0105 (0.0487)  2_CE_loss: 0.0365 (0.1693)  2_DKS_loss: 0.0002 (0.0052)  3_CE_loss: 0.0383 (0.1366)  3_DKS_loss: 0.0016 (0.0158)  4_CE_loss: 0.2768 (0.7300)  4_DKS_loss: 0.0052 (0.0323)  5_CE_loss: 0.3573 (0.8650)  5_DKS_loss: 0.0164 (0.1644)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0541 (1.0705)  data: 0.0872 (0.0994)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:35:10  iter: 7300  loss: 1.4016 (3.5594)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6568 (1.4070)  1_CE_loss: 0.0105 (0.0482)  2_CE_loss: 0.0351 (0.1676)  2_DKS_loss: 0.0003 (0.0052)  3_CE_loss: 0.0366 (0.1352)  3_DKS_loss: 0.0030 (0.0156)  4_CE_loss: 0.2741 (0.7241)  4_DKS_loss: 0.0047 (0.0320)  5_CE_loss: 0.3532 (0.8586)  5_DKS_loss: 0.0315 (0.1625)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0614 (1.0702)  data: 0.0868 (0.0992)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:eta: 2:33:21  iter: 7400  loss: 1.4849 (3.5315)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6586 (1.3972)  1_CE_loss: 0.0027 (0.0477)  2_CE_loss: 0.0367 (0.1660)  2_DKS_loss: 0.0002 (0.0051)  3_CE_loss: 0.0304 (0.1339)  3_DKS_loss: 0.0018 (0.0154)  4_CE_loss: 0.3269 (0.7184)  4_DKS_loss: 0.0038 (0.0316)  5_CE_loss: 0.3974 (0.8522)  5_DKS_loss: 0.0132 (0.1606)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0127 (1.0699)  data: 0.0872 (0.0991)  lr: 0.016000  max mem: 10958
INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.001', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'LxyPredictor1', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/LAMBDA/Lxy1-predcls0.001'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.001
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: LxyPredictor1
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/LAMBDA/Lxy1-predcls0.001
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/LAMBDA/Lxy1-predcls0.001/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Loading data statistics from: ./checkpoints/LAMBDA/Lxy1-predcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.weight of shape (5, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.weight of shape (11, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.weight of shape (20, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.weight of shape (39, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.weight of shape (5, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.weight of shape (11, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.weight of shape (20, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.weight of shape (39, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/LAMBDA/Lxy1-predcls0.001/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: inf, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: inf, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: inf, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: inf, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: inf, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: inf, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 343389.90625, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 200062.10938, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 175910.20312, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 109042.68750, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 91385.25000, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 73394.22656, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 72627.08594, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2462616.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2434535.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2386488.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2198430.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2182797.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1447830.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2198430.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2164259.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1630455.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1596018.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1585972.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1396184.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1374105.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 479499.78125, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 466578.78125, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 53552.71094, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 17.03175, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : inf, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : inf, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: inf, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: inf, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: inf, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: inf, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: inf, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2462616.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2384312.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2384312.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2384312.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2384312.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2348216.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2340828.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2339658.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2310003.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1743952.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1642779.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1609139.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1595582.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1571471.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1558413.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1466296.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1460401.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1425665.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1346192.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 233583.29688, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 182544.57812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 168911.64062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 145820.39062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 135491.39062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 75038.17188, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 75038.17188, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 68629.20312, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 65477.74219, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 65477.74219, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 47626.83203, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 23645.93164, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: nan, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 23645.93164, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: nan, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 68629.20312, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 50.78840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 39.30519, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 38.54179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 33.31704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: nan, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 5:01:21  iter: 1  loss: 367.0403 (367.0403)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0015 (0.0015)  rel_ce_loss: 65.2232 (65.2232)  1_CE_loss: 51.0932 (51.0932)  2_CE_loss: 9.7190 (9.7190)  2_DKS_loss: 1.0454 (1.0454)  3_CE_loss: 55.8801 (55.8801)  3_DKS_loss: 2.7285 (2.7285)  4_CE_loss: 107.6762 (107.6762)  4_DKS_loss: 4.7204 (4.7204)  5_CE_loss: 61.4632 (61.4632)  5_DKS_loss: 7.4897 (7.4897)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1302 (1.1302)  data: 0.1042 (0.1042)  lr: 0.001600  max mem: 7341
INFO:maskrcnn_benchmark:eta: 4:44:03  iter: 100  loss: 19.0941 (69.7284)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0027 (0.0030)  rel_ce_loss: 7.6008 (17.7263)  1_CE_loss: 0.2555 (6.0780)  2_CE_loss: 0.6948 (2.3141)  2_DKS_loss: 0.0007 (0.2609)  3_CE_loss: 0.4361 (7.2472)  3_DKS_loss: 0.0188 (0.7248)  4_CE_loss: 3.9046 (18.3421)  4_DKS_loss: 0.0568 (1.3147)  5_CE_loss: 4.4744 (12.3350)  5_DKS_loss: 1.6544 (3.3823)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0768 (1.0719)  data: 0.0876 (0.0892)  lr: 0.004451  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:41:34  iter: 200  loss: 14.1551 (43.2165)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0028 (0.0031)  rel_ce_loss: 5.8145 (12.2101)  1_CE_loss: 0.0907 (3.1261)  2_CE_loss: 0.6326 (1.5259)  2_DKS_loss: 0.0003 (0.1322)  3_CE_loss: 0.4821 (3.8917)  3_DKS_loss: 0.0045 (0.3744)  4_CE_loss: 2.4121 (10.6946)  4_DKS_loss: 0.0464 (0.6852)  5_CE_loss: 3.1417 (8.1159)  5_DKS_loss: 1.4727 (2.4573)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0647 (1.0693)  data: 0.0886 (0.0893)  lr: 0.007331  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:39:37  iter: 300  loss: 9.8551 (33.3149)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0031)  rel_ce_loss: 4.2054 (9.9033)  1_CE_loss: 0.0215 (2.1284)  2_CE_loss: 0.3927 (1.1925)  2_DKS_loss: 0.0002 (0.0892)  3_CE_loss: 0.3370 (2.7467)  3_DKS_loss: 0.0116 (0.2574)  4_CE_loss: 1.9031 (8.1290)  4_DKS_loss: 0.0373 (0.4790)  5_CE_loss: 2.0615 (6.4530)  5_DKS_loss: 0.6175 (1.9332)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0455 (1.0686)  data: 0.0873 (0.0895)  lr: 0.010211  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:37:27  iter: 400  loss: 8.1177 (27.3430)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0029 (0.0032)  rel_ce_loss: 3.2005 (8.3711)  1_CE_loss: 0.0100 (1.6272)  2_CE_loss: 0.2695 (0.9888)  2_DKS_loss: 0.0013 (0.0678)  3_CE_loss: 0.2426 (2.1516)  3_DKS_loss: 0.0114 (0.1978)  4_CE_loss: 1.7004 (6.5734)  4_DKS_loss: 0.0606 (0.3760)  5_CE_loss: 1.8678 (5.3779)  5_DKS_loss: 0.5445 (1.6083)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0607 (1.0672)  data: 0.0875 (0.0892)  lr: 0.013091  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:35:26  iter: 500  loss: 6.1288 (23.2764)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 2.4231 (7.2496)  1_CE_loss: 0.0667 (1.3231)  2_CE_loss: 0.2486 (0.8594)  2_DKS_loss: 0.0000 (0.0549)  3_CE_loss: 0.1998 (1.7822)  3_DKS_loss: 0.0024 (0.1622)  4_CE_loss: 1.1987 (5.5438)  4_DKS_loss: 0.0353 (0.3101)  5_CE_loss: 1.3727 (4.6246)  5_DKS_loss: 0.2848 (1.3633)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0647 (1.0662)  data: 0.0894 (0.0891)  lr: 0.015971  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:33:43  iter: 600  loss: 5.9885 (20.5393)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0032)  rel_ce_loss: 2.5444 (6.5090)  1_CE_loss: 0.0770 (1.1207)  2_CE_loss: 0.3173 (0.7709)  2_DKS_loss: 0.0004 (0.0462)  3_CE_loss: 0.1936 (1.5279)  3_DKS_loss: 0.0068 (0.1384)  4_CE_loss: 0.8091 (4.8092)  4_DKS_loss: 0.0451 (0.2666)  5_CE_loss: 1.0622 (4.1395)  5_DKS_loss: 0.3041 (1.2077)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0772 (1.0665)  data: 0.0909 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:32:03  iter: 700  loss: 5.5136 (18.4135)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 2.2224 (5.9161)  1_CE_loss: 0.0382 (0.9714)  2_CE_loss: 0.1323 (0.6914)  2_DKS_loss: 0.0002 (0.0401)  3_CE_loss: 0.1079 (1.3377)  3_DKS_loss: 0.0044 (0.1206)  4_CE_loss: 0.8327 (4.2596)  4_DKS_loss: 0.0181 (0.2340)  5_CE_loss: 1.3324 (3.7612)  5_DKS_loss: 0.2506 (1.0781)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0592 (1.0669)  data: 0.0893 (0.0894)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:30:24  iter: 800  loss: 4.5510 (16.7154)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0032)  rel_ce_loss: 1.8677 (5.4284)  1_CE_loss: 0.0157 (0.8600)  2_CE_loss: 0.1376 (0.6287)  2_DKS_loss: 0.0000 (0.0353)  3_CE_loss: 0.1284 (1.1961)  3_DKS_loss: 0.0053 (0.1069)  4_CE_loss: 0.7328 (3.8324)  4_DKS_loss: 0.0156 (0.2087)  5_CE_loss: 1.0656 (3.4393)  5_DKS_loss: 0.2040 (0.9764)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0750 (1.0674)  data: 0.0871 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:28:43  iter: 900  loss: 3.9205 (15.3252)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 1.7030 (5.0274)  1_CE_loss: 0.0351 (0.7717)  2_CE_loss: 0.1440 (0.5740)  2_DKS_loss: 0.0001 (0.0315)  3_CE_loss: 0.1181 (1.0787)  3_DKS_loss: 0.0047 (0.0959)  4_CE_loss: 0.6632 (3.4871)  4_DKS_loss: 0.0190 (0.1881)  5_CE_loss: 0.9683 (3.1758)  5_DKS_loss: 0.1950 (0.8917)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0701 (1.0678)  data: 0.0871 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:26:53  iter: 1000  loss: 4.0648 (14.2027)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 1.6754 (4.6977)  1_CE_loss: 0.0376 (0.6994)  2_CE_loss: 0.0712 (0.5293)  2_DKS_loss: 0.0002 (0.0285)  3_CE_loss: 0.1200 (0.9851)  3_DKS_loss: 0.0052 (0.0871)  4_CE_loss: 0.6355 (3.2131)  4_DKS_loss: 0.0171 (0.1717)  5_CE_loss: 0.9358 (2.9627)  5_DKS_loss: 0.1942 (0.8249)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0543 (1.0676)  data: 0.0886 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:25:07  iter: 1100  loss: 3.6279 (13.2555)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 1.5971 (4.4199)  1_CE_loss: 0.0006 (0.6399)  2_CE_loss: 0.1130 (0.4923)  2_DKS_loss: 0.0000 (0.0260)  3_CE_loss: 0.1120 (0.9069)  3_DKS_loss: 0.0049 (0.0798)  4_CE_loss: 0.6536 (2.9813)  4_DKS_loss: 0.0119 (0.1579)  5_CE_loss: 0.8766 (2.7800)  5_DKS_loss: 0.1839 (0.7683)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0801 (1.0676)  data: 0.0886 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:23:14  iter: 1200  loss: 3.3463 (12.4532)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.4424 (4.1861)  1_CE_loss: 0.0022 (0.5901)  2_CE_loss: 0.1062 (0.4594)  2_DKS_loss: 0.0001 (0.0239)  3_CE_loss: 0.0877 (0.8399)  3_DKS_loss: 0.0042 (0.0737)  4_CE_loss: 0.6288 (2.7834)  4_DKS_loss: 0.0123 (0.1464)  5_CE_loss: 0.7923 (2.6289)  5_DKS_loss: 0.1591 (0.7182)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0732 (1.0672)  data: 0.0874 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:21:24  iter: 1300  loss: 3.1708 (11.7666)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.4386 (3.9823)  1_CE_loss: 0.0210 (0.5477)  2_CE_loss: 0.0556 (0.4318)  2_DKS_loss: 0.0001 (0.0221)  3_CE_loss: 0.0574 (0.7839)  3_DKS_loss: 0.0040 (0.0685)  4_CE_loss: 0.5371 (2.6182)  4_DKS_loss: 0.0150 (0.1366)  5_CE_loss: 0.7346 (2.4970)  5_DKS_loss: 0.1471 (0.6750)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0788 (1.0670)  data: 0.0871 (0.0892)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:19:35  iter: 1400  loss: 3.3122 (11.1773)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 1.5106 (3.8074)  1_CE_loss: 0.0036 (0.5114)  2_CE_loss: 0.0727 (0.4084)  2_DKS_loss: 0.0001 (0.0206)  3_CE_loss: 0.0720 (0.7362)  3_DKS_loss: 0.0031 (0.0640)  4_CE_loss: 0.5454 (2.4751)  4_DKS_loss: 0.0140 (0.1282)  5_CE_loss: 0.8364 (2.3849)  5_DKS_loss: 0.1335 (0.6378)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0662 (1.0668)  data: 0.0875 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:17:45  iter: 1500  loss: 3.0671 (10.6488)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.4313 (3.6509)  1_CE_loss: 0.0213 (0.4795)  2_CE_loss: 0.0727 (0.3875)  2_DKS_loss: 0.0005 (0.0193)  3_CE_loss: 0.0707 (0.6940)  3_DKS_loss: 0.0043 (0.0601)  4_CE_loss: 0.4675 (2.3470)  4_DKS_loss: 0.0166 (0.1209)  5_CE_loss: 0.7963 (2.2820)  5_DKS_loss: 0.1320 (0.6043)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0678 (1.0666)  data: 0.0891 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:15:59  iter: 1600  loss: 3.0197 (10.1696)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.2801 (3.5053)  1_CE_loss: 0.0264 (0.4515)  2_CE_loss: 0.0578 (0.3689)  2_DKS_loss: 0.0004 (0.0181)  3_CE_loss: 0.0795 (0.6564)  3_DKS_loss: 0.0044 (0.0567)  4_CE_loss: 0.5313 (2.2338)  4_DKS_loss: 0.0160 (0.1144)  5_CE_loss: 0.7262 (2.1870)  5_DKS_loss: 0.1100 (0.5742)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0684 (1.0667)  data: 0.0865 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:14:10  iter: 1700  loss: 2.8188 (9.7453)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.2321 (3.3772)  1_CE_loss: 0.0198 (0.4268)  2_CE_loss: 0.0681 (0.3522)  2_DKS_loss: 0.0004 (0.0171)  3_CE_loss: 0.0678 (0.6230)  3_DKS_loss: 0.0023 (0.0536)  4_CE_loss: 0.5009 (2.1326)  4_DKS_loss: 0.0117 (0.1087)  5_CE_loss: 0.6812 (2.1034)  5_DKS_loss: 0.1102 (0.5473)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0666 (1.0664)  data: 0.0880 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:12:26  iter: 1800  loss: 2.7266 (9.3655)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 1.1934 (3.2624)  1_CE_loss: 0.0266 (0.4050)  2_CE_loss: 0.0655 (0.3370)  2_DKS_loss: 0.0005 (0.0162)  3_CE_loss: 0.0515 (0.5923)  3_DKS_loss: 0.0034 (0.0509)  4_CE_loss: 0.4851 (2.0423)  4_DKS_loss: 0.0092 (0.1035)  5_CE_loss: 0.7569 (2.0296)  5_DKS_loss: 0.1143 (0.5229)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0703 (1.0666)  data: 0.0905 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:10:36  iter: 1900  loss: 2.6427 (9.0161)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0028 (0.0033)  rel_ce_loss: 1.2112 (3.1561)  1_CE_loss: 0.0062 (0.3850)  2_CE_loss: 0.0596 (0.3235)  2_DKS_loss: 0.0004 (0.0154)  3_CE_loss: 0.0456 (0.5651)  3_DKS_loss: 0.0026 (0.0484)  4_CE_loss: 0.4254 (1.9592)  4_DKS_loss: 0.0088 (0.0987)  5_CE_loss: 0.6856 (1.9610)  5_DKS_loss: 0.0825 (0.5004)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0254 (1.0664)  data: 0.0866 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:08:52  iter: 2000  loss: 2.6542 (8.6918)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 1.1570 (3.0557)  1_CE_loss: 0.0192 (0.3670)  2_CE_loss: 0.0476 (0.3105)  2_DKS_loss: 0.0004 (0.0147)  3_CE_loss: 0.0586 (0.5400)  3_DKS_loss: 0.0027 (0.0462)  4_CE_loss: 0.4379 (1.8835)  4_DKS_loss: 0.0087 (0.0944)  5_CE_loss: 0.6123 (1.8972)  5_DKS_loss: 0.0961 (0.4794)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0717 (1.0666)  data: 0.0880 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:07:04  iter: 2100  loss: 2.3211 (8.3986)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.0308 (2.9641)  1_CE_loss: 0.0042 (0.3507)  2_CE_loss: 0.0494 (0.2990)  2_DKS_loss: 0.0004 (0.0140)  3_CE_loss: 0.0519 (0.5177)  3_DKS_loss: 0.0031 (0.0442)  4_CE_loss: 0.4293 (1.8161)  4_DKS_loss: 0.0097 (0.0905)  5_CE_loss: 0.5955 (1.8387)  5_DKS_loss: 0.0707 (0.4604)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0613 (1.0665)  data: 0.0860 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:05:18  iter: 2200  loss: 2.3930 (8.1253)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0029 (0.0033)  rel_ce_loss: 1.0206 (2.8785)  1_CE_loss: 0.0125 (0.3360)  2_CE_loss: 0.0637 (0.2883)  2_DKS_loss: 0.0007 (0.0134)  3_CE_loss: 0.0462 (0.4968)  3_DKS_loss: 0.0030 (0.0424)  4_CE_loss: 0.3935 (1.7526)  4_DKS_loss: 0.0099 (0.0869)  5_CE_loss: 0.5596 (1.7842)  5_DKS_loss: 0.0704 (0.4429)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0731 (1.0665)  data: 0.0877 (0.0891)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:03:28  iter: 2300  loss: 2.2824 (7.8731)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.0368 (2.7986)  1_CE_loss: 0.0194 (0.3223)  2_CE_loss: 0.0568 (0.2784)  2_DKS_loss: 0.0006 (0.0128)  3_CE_loss: 0.0534 (0.4778)  3_DKS_loss: 0.0024 (0.0407)  4_CE_loss: 0.4541 (1.6952)  4_DKS_loss: 0.0083 (0.0837)  5_CE_loss: 0.5825 (1.7335)  5_DKS_loss: 0.0684 (0.4268)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0732 (1.0663)  data: 0.0879 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 4:01:44  iter: 2400  loss: 2.0613 (7.6342)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 0.9256 (2.7223)  1_CE_loss: 0.0216 (0.3097)  2_CE_loss: 0.0505 (0.2693)  2_DKS_loss: 0.0007 (0.0123)  3_CE_loss: 0.0482 (0.4604)  3_DKS_loss: 0.0031 (0.0391)  4_CE_loss: 0.3553 (1.6409)  4_DKS_loss: 0.0116 (0.0807)  5_CE_loss: 0.4958 (1.6844)  5_DKS_loss: 0.0604 (0.4118)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0853 (1.0665)  data: 0.0881 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:59:55  iter: 2500  loss: 2.2107 (7.4173)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 0.9681 (2.6531)  1_CE_loss: 0.0195 (0.2981)  2_CE_loss: 0.0515 (0.2608)  2_DKS_loss: 0.0007 (0.0119)  3_CE_loss: 0.0559 (0.4443)  3_DKS_loss: 0.0047 (0.0377)  4_CE_loss: 0.3146 (1.5917)  4_DKS_loss: 0.0111 (0.0780)  5_CE_loss: 0.4910 (1.6404)  5_DKS_loss: 0.0550 (0.3979)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0787 (1.0664)  data: 0.0864 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:58:07  iter: 2600  loss: 1.9748 (7.2111)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0033)  rel_ce_loss: 0.9037 (2.5871)  1_CE_loss: 0.0175 (0.2874)  2_CE_loss: 0.0579 (0.2530)  2_DKS_loss: 0.0005 (0.0115)  3_CE_loss: 0.0302 (0.4293)  3_DKS_loss: 0.0034 (0.0365)  4_CE_loss: 0.3712 (1.5449)  4_DKS_loss: 0.0111 (0.0754)  5_CE_loss: 0.4207 (1.5980)  5_DKS_loss: 0.0387 (0.3846)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0796 (1.0663)  data: 0.0873 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:56:20  iter: 2700  loss: 1.9568 (7.0197)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 0.9054 (2.5253)  1_CE_loss: 0.0099 (0.2775)  2_CE_loss: 0.0498 (0.2459)  2_DKS_loss: 0.0006 (0.0111)  3_CE_loss: 0.0568 (0.4155)  3_DKS_loss: 0.0062 (0.0355)  4_CE_loss: 0.3415 (1.5014)  4_DKS_loss: 0.0145 (0.0732)  5_CE_loss: 0.4834 (1.5584)  5_DKS_loss: 0.0549 (0.3727)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0640 (1.0662)  data: 0.0889 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:54:32  iter: 2800  loss: 1.8392 (6.8383)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0033)  rel_ce_loss: 0.8540 (2.4668)  1_CE_loss: 0.0155 (0.2682)  2_CE_loss: 0.0557 (0.2390)  2_DKS_loss: 0.0004 (0.0107)  3_CE_loss: 0.0430 (0.4025)  3_DKS_loss: 0.0064 (0.0344)  4_CE_loss: 0.3319 (1.4606)  4_DKS_loss: 0.0097 (0.0710)  5_CE_loss: 0.4717 (1.5205)  5_DKS_loss: 0.0403 (0.3612)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0580 (1.0661)  data: 0.0872 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:52:44  iter: 2900  loss: 1.9451 (6.6698)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.8716 (2.4123)  1_CE_loss: 0.0124 (0.2596)  2_CE_loss: 0.0564 (0.2327)  2_DKS_loss: 0.0012 (0.0104)  3_CE_loss: 0.0529 (0.3904)  3_DKS_loss: 0.0039 (0.0334)  4_CE_loss: 0.3666 (1.4228)  4_DKS_loss: 0.0117 (0.0689)  5_CE_loss: 0.4966 (1.4853)  5_DKS_loss: 0.0517 (0.3506)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0584 (1.0660)  data: 0.0868 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:50:59  iter: 3000  loss: 1.8709 (6.5092)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0031 (0.0034)  rel_ce_loss: 0.8644 (2.3600)  1_CE_loss: 0.0096 (0.2513)  2_CE_loss: 0.0534 (0.2267)  2_DKS_loss: 0.0009 (0.0101)  3_CE_loss: 0.0428 (0.3790)  3_DKS_loss: 0.0046 (0.0324)  4_CE_loss: 0.3448 (1.3869)  4_DKS_loss: 0.0141 (0.0671)  5_CE_loss: 0.4482 (1.4516)  5_DKS_loss: 0.0449 (0.3406)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0696 (1.0661)  data: 0.0857 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:49:15  iter: 3100  loss: 2.2786 (6.3652)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 1.0419 (2.3135)  1_CE_loss: 0.0149 (0.2437)  2_CE_loss: 0.0492 (0.2210)  2_DKS_loss: 0.0010 (0.0098)  3_CE_loss: 0.0386 (0.3684)  3_DKS_loss: 0.0040 (0.0315)  4_CE_loss: 0.4793 (1.3543)  4_DKS_loss: 0.0153 (0.0654)  5_CE_loss: 0.5858 (1.4227)  5_DKS_loss: 0.0737 (0.3314)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0695 (1.0663)  data: 0.0861 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:47:32  iter: 3200  loss: 1.7985 (6.2261)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.8267 (2.2687)  1_CE_loss: 0.0111 (0.2367)  2_CE_loss: 0.0323 (0.2155)  2_DKS_loss: 0.0007 (0.0095)  3_CE_loss: 0.0283 (0.3583)  3_DKS_loss: 0.0029 (0.0307)  4_CE_loss: 0.3223 (1.3226)  4_DKS_loss: 0.0086 (0.0637)  5_CE_loss: 0.4622 (1.3942)  5_DKS_loss: 0.0450 (0.3227)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0879 (1.0666)  data: 0.0889 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:45:44  iter: 3300  loss: 1.7810 (6.0987)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.8031 (2.2276)  1_CE_loss: 0.0070 (0.2301)  2_CE_loss: 0.0394 (0.2106)  2_DKS_loss: 0.0008 (0.0093)  3_CE_loss: 0.0413 (0.3490)  3_DKS_loss: 0.0039 (0.0299)  4_CE_loss: 0.3387 (1.2934)  4_DKS_loss: 0.0082 (0.0621)  5_CE_loss: 0.4325 (1.3691)  5_DKS_loss: 0.0270 (0.3145)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0306 (1.0665)  data: 0.0882 (0.0890)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:43:57  iter: 3400  loss: 1.7163 (5.9738)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7880 (2.1865)  1_CE_loss: 0.0026 (0.2237)  2_CE_loss: 0.0309 (0.2059)  2_DKS_loss: 0.0006 (0.0090)  3_CE_loss: 0.0408 (0.3401)  3_DKS_loss: 0.0030 (0.0292)  4_CE_loss: 0.3559 (1.2660)  4_DKS_loss: 0.0068 (0.0606)  5_CE_loss: 0.4194 (1.3427)  5_DKS_loss: 0.0318 (0.3067)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0687 (1.0665)  data: 0.0884 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:42:11  iter: 3500  loss: 1.6129 (5.8530)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7488 (2.1463)  1_CE_loss: 0.0092 (0.2177)  2_CE_loss: 0.0354 (0.2014)  2_DKS_loss: 0.0005 (0.0088)  3_CE_loss: 0.0328 (0.3317)  3_DKS_loss: 0.0023 (0.0285)  4_CE_loss: 0.3442 (1.2400)  4_DKS_loss: 0.0100 (0.0592)  5_CE_loss: 0.4082 (1.3171)  5_DKS_loss: 0.0283 (0.2990)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0810 (1.0665)  data: 0.0877 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:40:26  iter: 3600  loss: 1.7063 (5.7386)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.8099 (2.1089)  1_CE_loss: 0.0094 (0.2121)  2_CE_loss: 0.0397 (0.1971)  2_DKS_loss: 0.0008 (0.0086)  3_CE_loss: 0.0296 (0.3235)  3_DKS_loss: 0.0045 (0.0278)  4_CE_loss: 0.2995 (1.2146)  4_DKS_loss: 0.0094 (0.0579)  5_CE_loss: 0.4216 (1.2928)  5_DKS_loss: 0.0353 (0.2919)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0527 (1.0667)  data: 0.0874 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:38:37  iter: 3700  loss: 1.7229 (5.6309)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7420 (2.0734)  1_CE_loss: 0.0119 (0.2068)  2_CE_loss: 0.0497 (0.1931)  2_DKS_loss: 0.0007 (0.0084)  3_CE_loss: 0.0349 (0.3159)  3_DKS_loss: 0.0025 (0.0272)  4_CE_loss: 0.3728 (1.1910)  4_DKS_loss: 0.0072 (0.0565)  5_CE_loss: 0.4351 (1.2701)  5_DKS_loss: 0.0341 (0.2851)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0716 (1.0665)  data: 0.0915 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:36:48  iter: 3800  loss: 1.7515 (5.5314)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7638 (2.0403)  1_CE_loss: 0.0122 (0.2018)  2_CE_loss: 0.0309 (0.1892)  2_DKS_loss: 0.0010 (0.0082)  3_CE_loss: 0.0292 (0.3088)  3_DKS_loss: 0.0027 (0.0266)  4_CE_loss: 0.3090 (1.1694)  4_DKS_loss: 0.0068 (0.0553)  5_CE_loss: 0.4743 (1.2498)  5_DKS_loss: 0.0410 (0.2788)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0703 (1.0662)  data: 0.0884 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:34:59  iter: 3900  loss: 1.6710 (5.4354)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7659 (2.0084)  1_CE_loss: 0.0050 (0.1969)  2_CE_loss: 0.0354 (0.1854)  2_DKS_loss: 0.0006 (0.0080)  3_CE_loss: 0.0336 (0.3019)  3_DKS_loss: 0.0020 (0.0260)  4_CE_loss: 0.3482 (1.1484)  4_DKS_loss: 0.0056 (0.0541)  5_CE_loss: 0.4417 (1.2304)  5_DKS_loss: 0.0162 (0.2725)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0478 (1.0661)  data: 0.0870 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:---Total norm 8.85791 clip coef 0.56447-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.36163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.04122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 3.49516, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.70461, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.61006, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.48545, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.47131, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.46225, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.10167, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 1.07890, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.06429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.06429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.06429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.06429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.97709, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.86651, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.83560, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.75686, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.73479, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.72472, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.69900, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.67071, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.64262, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.63609, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.61297, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.59936, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.58635, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.53110, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.51838, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.47768, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.46230, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.43911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.43723, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.40870, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.40043, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.38759, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.38684, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.38272, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.37305, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.37296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.35527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.35094, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.34835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.29696, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.28982, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.27644, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26633, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26383, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25912, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25101, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24936, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24386, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24322, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23500, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.23356, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23323, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23304, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.22439, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20938, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20848, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20781, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20399, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.20214, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.17802, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.17802, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.17251, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.16969, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16432, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16423, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16258, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16200, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.16188, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15847, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15450, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15408, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15103, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14990, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14893, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14776, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14753, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14310, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.14225, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13590, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.13383, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13351, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.12232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11869, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11508, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11373, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11229, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11223, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11119, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11067, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11045, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10995, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10990, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10983, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10537, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10521, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10415, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10339, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.10338, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10287, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10127, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10119, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09971, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09882, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09847, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09558, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09448, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09405, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09331, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08814, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08757, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08678, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.08491, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08361, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08270, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08142, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07961, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07912, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07878, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07847, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07791, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07786, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07579, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07548, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07448, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07217, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06756, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06666, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06483, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06070, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05813, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05564, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05364, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05180, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05180, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.05164, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.05164, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05068, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05027, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05006, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04948, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04796, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04690, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04670, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04664, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04642, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04632, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04575, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04554, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04546, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04481, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04456, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04454, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.04424, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.04424, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04300, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04269, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04183, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.04155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.04092, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03950, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03886, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03825, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03602, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.03559, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03459, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03270, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03231, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03177, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.03039, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03015, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02879, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02776, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02759, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02715, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02655, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02605, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02550, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02532, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.02399, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02365, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02336, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02301, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02288, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02266, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02260, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02211, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02193, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02099, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02096, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02055, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01941, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01936, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01902, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01898, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01896, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01865, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01851, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01824, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01819, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01794, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01786, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01775, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01685, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01396, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01347, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01295, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01294, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01294, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01164, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01154, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01132, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01127, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01111, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01091, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01067, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01019, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00992, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00965, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00962, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.00959, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00953, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00944, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00914, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00851, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00851, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00841, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00838, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00833, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00833, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00831, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00819, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00815, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00814, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00813, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00805, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00803, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00796, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00793, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00792, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00790, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00766, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00761, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00761, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00757, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00738, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00730, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00726, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00724, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00722, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00720, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00720, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00718, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00715, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00712, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00710, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00710, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00707, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00700, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00692, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00685, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00679, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00674, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00672, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00669, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00662, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00662, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00655, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00645, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00622, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00617, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00616, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00615, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00611, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00603, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00590, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00585, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00572, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00558, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00550, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00543, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00498, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00498, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00480, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00479, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00450, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00405, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00398, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00389, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00386, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00339, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00326, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00318, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00315, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00304, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00298, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00291, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00282, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00279, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00277, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00276, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00276, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00275, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00265, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00264, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00263, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00252, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00245, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00245, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00241, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00241, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00240, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00239, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00235, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00227, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00211, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00200, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00196, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00181, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00161, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00159, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00106, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00094, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00080, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00072, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00071, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00070, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00063, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00050, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00050, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00050, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00038, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00015, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:33:11  iter: 4000  loss: 1.7732 (5.3414)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7624 (1.9771)  1_CE_loss: 0.0022 (0.1923)  2_CE_loss: 0.0462 (0.1819)  2_DKS_loss: 0.0015 (0.0079)  3_CE_loss: 0.0432 (0.2955)  3_DKS_loss: 0.0067 (0.0255)  4_CE_loss: 0.3434 (1.1281)  4_DKS_loss: 0.0104 (0.0529)  5_CE_loss: 0.4275 (1.2106)  5_DKS_loss: 0.0342 (0.2664)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0642 (1.0659)  data: 0.0859 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:31:25  iter: 4100  loss: 1.7364 (5.2525)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7559 (1.9477)  1_CE_loss: 0.0068 (0.1880)  2_CE_loss: 0.0531 (0.1787)  2_DKS_loss: 0.0007 (0.0077)  3_CE_loss: 0.0451 (0.2893)  3_DKS_loss: 0.0033 (0.0250)  4_CE_loss: 0.3611 (1.1085)  4_DKS_loss: 0.0076 (0.0519)  5_CE_loss: 0.4354 (1.1916)  5_DKS_loss: 0.0297 (0.2607)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0711 (1.0660)  data: 0.0882 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:29:37  iter: 4200  loss: 1.5696 (5.1675)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7317 (1.9194)  1_CE_loss: 0.0086 (0.1838)  2_CE_loss: 0.0347 (0.1755)  2_DKS_loss: 0.0009 (0.0075)  3_CE_loss: 0.0298 (0.2834)  3_DKS_loss: 0.0035 (0.0245)  4_CE_loss: 0.2908 (1.0901)  4_DKS_loss: 0.0078 (0.0509)  5_CE_loss: 0.4414 (1.1736)  5_DKS_loss: 0.0316 (0.2553)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0779 (1.0659)  data: 0.0872 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:27:51  iter: 4300  loss: 1.6582 (5.0874)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.7350 (1.8927)  1_CE_loss: 0.0094 (0.1800)  2_CE_loss: 0.0362 (0.1724)  2_DKS_loss: 0.0008 (0.0074)  3_CE_loss: 0.0313 (0.2779)  3_DKS_loss: 0.0034 (0.0241)  4_CE_loss: 0.3441 (1.0726)  4_DKS_loss: 0.0098 (0.0500)  5_CE_loss: 0.3907 (1.1566)  5_DKS_loss: 0.0450 (0.2503)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.0659)  data: 0.0882 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:26:06  iter: 4400  loss: 1.4958 (5.0087)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7218 (1.8669)  1_CE_loss: 0.0076 (0.1762)  2_CE_loss: 0.0208 (0.1695)  2_DKS_loss: 0.0006 (0.0073)  3_CE_loss: 0.0313 (0.2725)  3_DKS_loss: 0.0025 (0.0236)  4_CE_loss: 0.3032 (1.0554)  4_DKS_loss: 0.0054 (0.0491)  5_CE_loss: 0.3977 (1.1396)  5_DKS_loss: 0.0241 (0.2453)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0662 (1.0660)  data: 0.0877 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:24:20  iter: 4500  loss: 1.6639 (4.9344)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.7652 (1.8424)  1_CE_loss: 0.0096 (0.1725)  2_CE_loss: 0.0369 (0.1667)  2_DKS_loss: 0.0007 (0.0071)  3_CE_loss: 0.0372 (0.2674)  3_DKS_loss: 0.0026 (0.0232)  4_CE_loss: 0.3323 (1.0391)  4_DKS_loss: 0.0074 (0.0482)  5_CE_loss: 0.4275 (1.1239)  5_DKS_loss: 0.0234 (0.2405)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0544 (1.0661)  data: 0.0890 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:22:34  iter: 4600  loss: 1.5610 (4.8623)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.6910 (1.8184)  1_CE_loss: 0.0150 (0.1692)  2_CE_loss: 0.0384 (0.1641)  2_DKS_loss: 0.0005 (0.0070)  3_CE_loss: 0.0412 (0.2625)  3_DKS_loss: 0.0023 (0.0227)  4_CE_loss: 0.2919 (1.0235)  4_DKS_loss: 0.0073 (0.0473)  5_CE_loss: 0.3877 (1.1084)  5_DKS_loss: 0.0248 (0.2359)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0634 (1.0661)  data: 0.0878 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:20:48  iter: 4700  loss: 1.5161 (4.7928)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6921 (1.7955)  1_CE_loss: 0.0130 (0.1658)  2_CE_loss: 0.0408 (0.1616)  2_DKS_loss: 0.0005 (0.0069)  3_CE_loss: 0.0386 (0.2579)  3_DKS_loss: 0.0022 (0.0223)  4_CE_loss: 0.3064 (1.0082)  4_DKS_loss: 0.0059 (0.0464)  5_CE_loss: 0.3759 (1.0934)  5_DKS_loss: 0.0177 (0.2314)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0669 (1.0663)  data: 0.0879 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:18:59  iter: 4800  loss: 1.8616 (4.7284)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7643 (1.7738)  1_CE_loss: 0.0097 (0.1626)  2_CE_loss: 0.0365 (0.1591)  2_DKS_loss: 0.0009 (0.0067)  3_CE_loss: 0.0470 (0.2534)  3_DKS_loss: 0.0049 (0.0219)  4_CE_loss: 0.3652 (0.9944)  4_DKS_loss: 0.0099 (0.0456)  5_CE_loss: 0.5000 (1.0799)  5_DKS_loss: 0.0381 (0.2274)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0687 (1.0661)  data: 0.0894 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:17:12  iter: 4900  loss: 1.6057 (4.6664)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7304 (1.7536)  1_CE_loss: 0.0095 (0.1596)  2_CE_loss: 0.0318 (0.1567)  2_DKS_loss: 0.0011 (0.0066)  3_CE_loss: 0.0236 (0.2491)  3_DKS_loss: 0.0035 (0.0216)  4_CE_loss: 0.3079 (0.9807)  4_DKS_loss: 0.0062 (0.0449)  5_CE_loss: 0.4277 (1.0671)  5_DKS_loss: 0.0242 (0.2232)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0726 (1.0660)  data: 0.0870 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:15:24  iter: 5000  loss: 1.6455 (4.6061)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7169 (1.7334)  1_CE_loss: 0.0130 (0.1567)  2_CE_loss: 0.0405 (0.1545)  2_DKS_loss: 0.0006 (0.0065)  3_CE_loss: 0.0460 (0.2450)  3_DKS_loss: 0.0031 (0.0213)  4_CE_loss: 0.3208 (0.9675)  4_DKS_loss: 0.0062 (0.0441)  5_CE_loss: 0.4232 (1.0543)  5_DKS_loss: 0.0349 (0.2195)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0589 (1.0659)  data: 0.0868 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:13:37  iter: 5100  loss: 1.5257 (4.5467)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.6976 (1.7135)  1_CE_loss: 0.0049 (0.1538)  2_CE_loss: 0.0338 (0.1524)  2_DKS_loss: 0.0005 (0.0064)  3_CE_loss: 0.0400 (0.2410)  3_DKS_loss: 0.0030 (0.0209)  4_CE_loss: 0.3080 (0.9546)  4_DKS_loss: 0.0075 (0.0434)  5_CE_loss: 0.3771 (1.0416)  5_DKS_loss: 0.0234 (0.2157)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0618 (1.0658)  data: 0.0856 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:11:50  iter: 5200  loss: 1.6129 (4.4905)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7310 (1.6948)  1_CE_loss: 0.0044 (0.1511)  2_CE_loss: 0.0471 (0.1503)  2_DKS_loss: 0.0005 (0.0063)  3_CE_loss: 0.0421 (0.2372)  3_DKS_loss: 0.0021 (0.0206)  4_CE_loss: 0.3006 (0.9424)  4_DKS_loss: 0.0065 (0.0427)  5_CE_loss: 0.4007 (1.0296)  5_DKS_loss: 0.0215 (0.2120)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0674 (1.0658)  data: 0.0879 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:10:05  iter: 5300  loss: 1.4295 (4.4343)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6939 (1.6762)  1_CE_loss: 0.0059 (0.1485)  2_CE_loss: 0.0372 (0.1482)  2_DKS_loss: 0.0006 (0.0062)  3_CE_loss: 0.0286 (0.2335)  3_DKS_loss: 0.0018 (0.0202)  4_CE_loss: 0.2642 (0.9301)  4_DKS_loss: 0.0052 (0.0420)  5_CE_loss: 0.3261 (1.0174)  5_DKS_loss: 0.0265 (0.2085)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0761 (1.0659)  data: 0.0876 (0.0889)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:08:19  iter: 5400  loss: 1.4475 (4.3820)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7052 (1.6587)  1_CE_loss: 0.0092 (0.1460)  2_CE_loss: 0.0327 (0.1463)  2_DKS_loss: 0.0009 (0.0061)  3_CE_loss: 0.0334 (0.2299)  3_DKS_loss: 0.0042 (0.0199)  4_CE_loss: 0.3028 (0.9187)  4_DKS_loss: 0.0071 (0.0414)  5_CE_loss: 0.3886 (1.0064)  5_DKS_loss: 0.0253 (0.2051)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0624 (1.0660)  data: 0.0891 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:06:31  iter: 5500  loss: 1.4716 (4.3315)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7162 (1.6420)  1_CE_loss: 0.0096 (0.1436)  2_CE_loss: 0.0310 (0.1444)  2_DKS_loss: 0.0007 (0.0060)  3_CE_loss: 0.0362 (0.2265)  3_DKS_loss: 0.0032 (0.0197)  4_CE_loss: 0.2732 (0.9075)  4_DKS_loss: 0.0063 (0.0408)  5_CE_loss: 0.3617 (0.9955)  5_DKS_loss: 0.0194 (0.2020)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0692 (1.0659)  data: 0.0877 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:04:44  iter: 5600  loss: 1.4840 (4.2830)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.6966 (1.6258)  1_CE_loss: 0.0109 (0.1413)  2_CE_loss: 0.0391 (0.1426)  2_DKS_loss: 0.0005 (0.0060)  3_CE_loss: 0.0405 (0.2232)  3_DKS_loss: 0.0023 (0.0194)  4_CE_loss: 0.3078 (0.8971)  4_DKS_loss: 0.0065 (0.0402)  5_CE_loss: 0.3596 (0.9851)  5_DKS_loss: 0.0219 (0.1988)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0659)  data: 0.0873 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:02:57  iter: 5700  loss: 1.5596 (4.2355)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7338 (1.6098)  1_CE_loss: 0.0034 (0.1390)  2_CE_loss: 0.0482 (0.1409)  2_DKS_loss: 0.0007 (0.0059)  3_CE_loss: 0.0411 (0.2200)  3_DKS_loss: 0.0026 (0.0191)  4_CE_loss: 0.3255 (0.8870)  4_DKS_loss: 0.0060 (0.0397)  5_CE_loss: 0.4072 (0.9749)  5_DKS_loss: 0.0236 (0.1959)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0519 (1.0658)  data: 0.0883 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 3:01:11  iter: 5800  loss: 1.4876 (4.1892)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0034)  rel_ce_loss: 0.6793 (1.5943)  1_CE_loss: 0.0102 (0.1369)  2_CE_loss: 0.0434 (0.1392)  2_DKS_loss: 0.0007 (0.0058)  3_CE_loss: 0.0438 (0.2169)  3_DKS_loss: 0.0027 (0.0189)  4_CE_loss: 0.3158 (0.8769)  4_DKS_loss: 0.0050 (0.0391)  5_CE_loss: 0.3668 (0.9649)  5_DKS_loss: 0.0223 (0.1929)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0658)  data: 0.0863 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:59:25  iter: 5900  loss: 1.4631 (4.1455)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7358 (1.5798)  1_CE_loss: 0.0032 (0.1348)  2_CE_loss: 0.0481 (0.1376)  2_DKS_loss: 0.0007 (0.0057)  3_CE_loss: 0.0345 (0.2139)  3_DKS_loss: 0.0026 (0.0186)  4_CE_loss: 0.2738 (0.8675)  4_DKS_loss: 0.0059 (0.0386)  5_CE_loss: 0.3306 (0.9553)  5_DKS_loss: 0.0208 (0.1902)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0699 (1.0659)  data: 0.0885 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:57:38  iter: 6000  loss: 1.5779 (4.1026)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7322 (1.5655)  1_CE_loss: 0.0057 (0.1328)  2_CE_loss: 0.0358 (0.1361)  2_DKS_loss: 0.0006 (0.0056)  3_CE_loss: 0.0397 (0.2110)  3_DKS_loss: 0.0027 (0.0184)  4_CE_loss: 0.3354 (0.8583)  4_DKS_loss: 0.0055 (0.0381)  5_CE_loss: 0.3916 (0.9460)  5_DKS_loss: 0.0216 (0.1874)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0724 (1.0659)  data: 0.0870 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:55:53  iter: 6100  loss: 1.5608 (4.0611)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7096 (1.5516)  1_CE_loss: 0.0063 (0.1308)  2_CE_loss: 0.0371 (0.1346)  2_DKS_loss: 0.0004 (0.0056)  3_CE_loss: 0.0339 (0.2082)  3_DKS_loss: 0.0021 (0.0181)  4_CE_loss: 0.3220 (0.8494)  4_DKS_loss: 0.0040 (0.0375)  5_CE_loss: 0.3911 (0.9371)  5_DKS_loss: 0.0331 (0.1848)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0491 (1.0660)  data: 0.0872 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:54:06  iter: 6200  loss: 1.5318 (4.0215)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6925 (1.5384)  1_CE_loss: 0.0053 (0.1289)  2_CE_loss: 0.0422 (0.1331)  2_DKS_loss: 0.0005 (0.0055)  3_CE_loss: 0.0284 (0.2056)  3_DKS_loss: 0.0017 (0.0178)  4_CE_loss: 0.2939 (0.8406)  4_DKS_loss: 0.0043 (0.0370)  5_CE_loss: 0.4015 (0.9288)  5_DKS_loss: 0.0193 (0.1823)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0542 (1.0660)  data: 0.0863 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:52:21  iter: 6300  loss: 1.6173 (3.9822)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.7061 (1.5253)  1_CE_loss: 0.0105 (0.1270)  2_CE_loss: 0.0368 (0.1317)  2_DKS_loss: 0.0007 (0.0054)  3_CE_loss: 0.0363 (0.2030)  3_DKS_loss: 0.0031 (0.0176)  4_CE_loss: 0.3148 (0.8321)  4_DKS_loss: 0.0051 (0.0365)  5_CE_loss: 0.3967 (0.9203)  5_DKS_loss: 0.0202 (0.1798)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0704 (1.0662)  data: 0.0890 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:50:35  iter: 6400  loss: 1.4175 (3.9443)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6469 (1.5127)  1_CE_loss: 0.0076 (0.1252)  2_CE_loss: 0.0395 (0.1303)  2_DKS_loss: 0.0007 (0.0054)  3_CE_loss: 0.0417 (0.2005)  3_DKS_loss: 0.0024 (0.0174)  4_CE_loss: 0.2761 (0.8239)  4_DKS_loss: 0.0046 (0.0360)  5_CE_loss: 0.3540 (0.9122)  5_DKS_loss: 0.0165 (0.1773)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0667 (1.0662)  data: 0.0868 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:48:48  iter: 6500  loss: 1.3632 (3.9069)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6563 (1.5002)  1_CE_loss: 0.0092 (0.1235)  2_CE_loss: 0.0340 (0.1289)  2_DKS_loss: 0.0015 (0.0053)  3_CE_loss: 0.0258 (0.1980)  3_DKS_loss: 0.0030 (0.0172)  4_CE_loss: 0.2885 (0.8158)  4_DKS_loss: 0.0044 (0.0356)  5_CE_loss: 0.3622 (0.9041)  5_DKS_loss: 0.0187 (0.1749)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0611 (1.0662)  data: 0.0877 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:47:01  iter: 6600  loss: 1.4479 (3.8705)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.7005 (1.4880)  1_CE_loss: 0.0078 (0.1218)  2_CE_loss: 0.0335 (0.1276)  2_DKS_loss: 0.0014 (0.0052)  3_CE_loss: 0.0242 (0.1956)  3_DKS_loss: 0.0019 (0.0170)  4_CE_loss: 0.2783 (0.8079)  4_DKS_loss: 0.0040 (0.0351)  5_CE_loss: 0.3811 (0.8962)  5_DKS_loss: 0.0210 (0.1725)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0530 (1.0661)  data: 0.0882 (0.0888)  lr: 0.016000  max mem: 10626
INFO:maskrcnn_benchmark:eta: 2:45:14  iter: 6700  loss: 1.4339 (3.8368)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6733 (1.4768)  1_CE_loss: 0.0014 (0.1201)  2_CE_loss: 0.0279 (0.1265)  2_DKS_loss: 0.0004 (0.0052)  3_CE_loss: 0.0320 (0.1933)  3_DKS_loss: 0.0020 (0.0168)  4_CE_loss: 0.2866 (0.8004)  4_DKS_loss: 0.0044 (0.0347)  5_CE_loss: 0.3367 (0.8890)  5_DKS_loss: 0.0225 (0.1704)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0629 (1.0661)  data: 0.0856 (0.0888)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:43:26  iter: 6800  loss: 1.4316 (3.8034)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.7057 (1.4658)  1_CE_loss: 0.0025 (0.1186)  2_CE_loss: 0.0368 (0.1253)  2_DKS_loss: 0.0011 (0.0051)  3_CE_loss: 0.0360 (0.1911)  3_DKS_loss: 0.0020 (0.0166)  4_CE_loss: 0.2751 (0.7931)  4_DKS_loss: 0.0045 (0.0343)  5_CE_loss: 0.3513 (0.8818)  5_DKS_loss: 0.0174 (0.1682)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0766 (1.0660)  data: 0.0882 (0.0888)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:41:39  iter: 6900  loss: 1.4894 (3.7704)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6895 (1.4548)  1_CE_loss: 0.0079 (0.1171)  2_CE_loss: 0.0470 (0.1242)  2_DKS_loss: 0.0006 (0.0051)  3_CE_loss: 0.0476 (0.1890)  3_DKS_loss: 0.0031 (0.0164)  4_CE_loss: 0.2888 (0.7858)  4_DKS_loss: 0.0058 (0.0339)  5_CE_loss: 0.3560 (0.8746)  5_DKS_loss: 0.0204 (0.1661)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0376 (1.0659)  data: 0.0863 (0.0887)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:39:52  iter: 7000  loss: 1.4331 (3.7392)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6798 (1.4444)  1_CE_loss: 0.0100 (0.1156)  2_CE_loss: 0.0410 (0.1230)  2_DKS_loss: 0.0006 (0.0050)  3_CE_loss: 0.0395 (0.1869)  3_DKS_loss: 0.0025 (0.0162)  4_CE_loss: 0.2886 (0.7791)  4_DKS_loss: 0.0060 (0.0335)  5_CE_loss: 0.3782 (0.8680)  5_DKS_loss: 0.0197 (0.1641)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0443 (1.0658)  data: 0.0877 (0.0887)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:38:05  iter: 7100  loss: 1.4330 (3.7079)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6662 (1.4338)  1_CE_loss: 0.0012 (0.1141)  2_CE_loss: 0.0288 (0.1218)  2_DKS_loss: 0.0008 (0.0050)  3_CE_loss: 0.0238 (0.1848)  3_DKS_loss: 0.0031 (0.0160)  4_CE_loss: 0.2829 (0.7723)  4_DKS_loss: 0.0067 (0.0331)  5_CE_loss: 0.3826 (0.8614)  5_DKS_loss: 0.0194 (0.1621)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0591 (1.0658)  data: 0.0858 (0.0887)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:36:18  iter: 7200  loss: 1.5470 (3.6772)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0034 (0.0035)  rel_ce_loss: 0.6849 (1.4234)  1_CE_loss: 0.0057 (0.1127)  2_CE_loss: 0.0480 (0.1208)  2_DKS_loss: 0.0004 (0.0049)  3_CE_loss: 0.0432 (0.1828)  3_DKS_loss: 0.0016 (0.0158)  4_CE_loss: 0.2932 (0.7657)  4_DKS_loss: 0.0063 (0.0328)  5_CE_loss: 0.3878 (0.8548)  5_DKS_loss: 0.0161 (0.1600)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0689 (1.0657)  data: 0.0861 (0.0887)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:34:31  iter: 7300  loss: 1.3982 (3.6466)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6478 (1.4131)  1_CE_loss: 0.0072 (0.1113)  2_CE_loss: 0.0388 (0.1197)  2_DKS_loss: 0.0006 (0.0048)  3_CE_loss: 0.0343 (0.1808)  3_DKS_loss: 0.0038 (0.0157)  4_CE_loss: 0.2743 (0.7591)  4_DKS_loss: 0.0059 (0.0324)  5_CE_loss: 0.3732 (0.8481)  5_DKS_loss: 0.0168 (0.1581)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0682 (1.0657)  data: 0.0852 (0.0887)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:32:44  iter: 7400  loss: 1.3349 (3.6170)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6100 (1.4029)  1_CE_loss: 0.0042 (0.1100)  2_CE_loss: 0.0290 (0.1187)  2_DKS_loss: 0.0011 (0.0048)  3_CE_loss: 0.0251 (0.1789)  3_DKS_loss: 0.0024 (0.0155)  4_CE_loss: 0.2754 (0.7528)  4_DKS_loss: 0.0050 (0.0320)  5_CE_loss: 0.3473 (0.8417)  5_DKS_loss: 0.0211 (0.1562)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0470 (1.0656)  data: 0.0839 (0.0886)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:30:57  iter: 7500  loss: 1.4553 (3.5880)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6648 (1.3931)  1_CE_loss: 0.0018 (0.1087)  2_CE_loss: 0.0335 (0.1176)  2_DKS_loss: 0.0004 (0.0047)  3_CE_loss: 0.0291 (0.1770)  3_DKS_loss: 0.0021 (0.0153)  4_CE_loss: 0.2941 (0.7466)  4_DKS_loss: 0.0036 (0.0317)  5_CE_loss: 0.3410 (0.8354)  5_DKS_loss: 0.0131 (0.1543)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0596 (1.0656)  data: 0.0859 (0.0886)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:29:09  iter: 7600  loss: 1.4051 (3.5605)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6695 (1.3838)  1_CE_loss: 0.0106 (0.1074)  2_CE_loss: 0.0349 (0.1167)  2_DKS_loss: 0.0003 (0.0047)  3_CE_loss: 0.0327 (0.1752)  3_DKS_loss: 0.0027 (0.0152)  4_CE_loss: 0.2735 (0.7406)  4_DKS_loss: 0.0041 (0.0313)  5_CE_loss: 0.3565 (0.8296)  5_DKS_loss: 0.0133 (0.1525)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0612 (1.0655)  data: 0.0861 (0.0886)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:27:23  iter: 7700  loss: 1.4236 (3.5335)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6541 (1.3745)  1_CE_loss: 0.0012 (0.1061)  2_CE_loss: 0.0301 (0.1157)  2_DKS_loss: 0.0007 (0.0046)  3_CE_loss: 0.0252 (0.1734)  3_DKS_loss: 0.0039 (0.0150)  4_CE_loss: 0.2718 (0.7347)  4_DKS_loss: 0.0035 (0.0310)  5_CE_loss: 0.3651 (0.8239)  5_DKS_loss: 0.0219 (0.1508)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0802 (1.0654)  data: 0.0850 (0.0886)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:25:35  iter: 7800  loss: 1.4409 (3.5067)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6297 (1.3654)  1_CE_loss: 0.0066 (0.1049)  2_CE_loss: 0.0308 (0.1147)  2_DKS_loss: 0.0003 (0.0046)  3_CE_loss: 0.0346 (0.1717)  3_DKS_loss: 0.0012 (0.0149)  4_CE_loss: 0.3213 (0.7291)  4_DKS_loss: 0.0036 (0.0306)  5_CE_loss: 0.3762 (0.8181)  5_DKS_loss: 0.0103 (0.1491)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0350 (1.0653)  data: 0.0849 (0.0886)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:23:48  iter: 7900  loss: 1.4423 (3.4807)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6594 (1.3566)  1_CE_loss: 0.0067 (0.1037)  2_CE_loss: 0.0348 (0.1138)  2_DKS_loss: 0.0004 (0.0045)  3_CE_loss: 0.0302 (0.1700)  3_DKS_loss: 0.0012 (0.0147)  4_CE_loss: 0.2971 (0.7235)  4_DKS_loss: 0.0033 (0.0303)  5_CE_loss: 0.3601 (0.8126)  5_DKS_loss: 0.0169 (0.1474)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0591 (1.0653)  data: 0.0855 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:---Total norm 2.45319 clip coef 2.03816-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.97945, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.91127, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.75118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.62263, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.53636, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.51401, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.49223, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.41509, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.40788, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.40459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.38935, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.38734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.34939, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.32039, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.31443, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.30220, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29850, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.28375, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27327, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.25311, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.23121, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18845, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.18286, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.15292, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14075, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.13095, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.12956, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12517, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12309, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.12082, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10755, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10730, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10674, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.10164, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10000, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09736, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09695, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.09591, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09499, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09452, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08868, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08767, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08675, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08472, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08327, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08280, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08218, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07860, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.07460, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07101, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.06983, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06587, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06370, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06220, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06219, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06166, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.06129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05974, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.05376, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05201, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04857, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04695, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.04673, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04663, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04659, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04585, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04568, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04557, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.04533, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.04533, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04467, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04460, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04425, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04305, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04194, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04159, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04129, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.04108, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04025, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03930, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03901, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.03874, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03731, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03720, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03712, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03658, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03656, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03577, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03550, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03532, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03523, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03458, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03454, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03449, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03369, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03310, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03308, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03279, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03260, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03259, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03222, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03163, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03163, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03137, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03126, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03121, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03115, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03103, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03014, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.02991, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02942, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02820, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02752, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02711, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.02690, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02686, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02660, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02630, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02580, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02530, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02524, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02480, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02449, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.02433, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02431, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02400, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02349, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02297, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02281, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02271, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02254, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02111, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02097, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02085, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.01978, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.01972, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01965, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01965, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.01962, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.01962, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01891, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01879, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01867, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01862, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01822, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01796, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.01773, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01753, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01749, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01722, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01700, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01695, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01656, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01613, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01593, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01533, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.01519, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01487, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01425, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.01383, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01371, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01332, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01320, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01292, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01260, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01254, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.01236, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01181, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01171, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01160, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01136, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01130, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01050, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01048, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01035, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01026, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00996, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00992, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00985, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00977, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00970, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00966, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00963, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.00950, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00946, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00907, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00905, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00898, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00890, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00886, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00807, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00784, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00784, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.00777, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00749, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00734, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00684, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00624, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00615, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00610, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00608, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.00595, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00592, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00558, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00535, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00531, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00531, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00505, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00487, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00455, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00451, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00433, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00425, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00423, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00410, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00410, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00391, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00380, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00365, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00348, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00342, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00336, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00333, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00318, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00310, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00307, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00303, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00300, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00291, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00289, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00279, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00274, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00273, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00271, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00271, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00269, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00259, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00252, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00250, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00247, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00246, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00242, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00239, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00239, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00237, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00237, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00236, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00235, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00234, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00234, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00233, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00233, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00227, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00225, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00223, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00221, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00221, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00221, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00216, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00216, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00215, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00211, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00208, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00207, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00204, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00203, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00202, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00202, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00198, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00192, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00186, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00133, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00133, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00126, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00122, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00119, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00108, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00105, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00102, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00099, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00096, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00094, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00093, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00092, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00092, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00091, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00090, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00088, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00088, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00085, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00085, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00082, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00081, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00081, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00081, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00080, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00078, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00076, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00076, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00072, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00071, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00071, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00070, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00067, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00059, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00059, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00057, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00052, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00049, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00047, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00043, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00040, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00040, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00038, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00032, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00027, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00027, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00023, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00017, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00006, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 2:22:02  iter: 8000  loss: 1.4415 (3.4550)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.6391 (1.3480)  1_CE_loss: 0.0079 (0.1026)  2_CE_loss: 0.0349 (0.1128)  2_DKS_loss: 0.0004 (0.0045)  3_CE_loss: 0.0446 (0.1683)  3_DKS_loss: 0.0016 (0.0146)  4_CE_loss: 0.2844 (0.7179)  4_DKS_loss: 0.0036 (0.0300)  5_CE_loss: 0.3436 (0.8070)  5_DKS_loss: 0.0109 (0.1458)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0864 (1.0653)  data: 0.0866 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:20:14  iter: 8100  loss: 1.4950 (3.4301)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6916 (1.3395)  1_CE_loss: 0.0076 (0.1015)  2_CE_loss: 0.0373 (0.1120)  2_DKS_loss: 0.0003 (0.0044)  3_CE_loss: 0.0385 (0.1668)  3_DKS_loss: 0.0025 (0.0144)  4_CE_loss: 0.2954 (0.7126)  4_DKS_loss: 0.0042 (0.0297)  5_CE_loss: 0.3638 (0.8016)  5_DKS_loss: 0.0126 (0.1441)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0442 (1.0652)  data: 0.0878 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:18:28  iter: 8200  loss: 1.3602 (3.4059)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6099 (1.3314)  1_CE_loss: 0.0021 (0.1004)  2_CE_loss: 0.0361 (0.1111)  2_DKS_loss: 0.0003 (0.0044)  3_CE_loss: 0.0353 (0.1652)  3_DKS_loss: 0.0016 (0.0143)  4_CE_loss: 0.2804 (0.7073)  4_DKS_loss: 0.0028 (0.0294)  5_CE_loss: 0.3514 (0.7964)  5_DKS_loss: 0.0165 (0.1426)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0609 (1.0652)  data: 0.0871 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:16:42  iter: 8300  loss: 1.4860 (3.3823)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6312 (1.3235)  1_CE_loss: 0.0103 (0.0993)  2_CE_loss: 0.0320 (0.1102)  2_DKS_loss: 0.0008 (0.0043)  3_CE_loss: 0.0333 (0.1636)  3_DKS_loss: 0.0021 (0.0141)  4_CE_loss: 0.2859 (0.7022)  4_DKS_loss: 0.0034 (0.0290)  5_CE_loss: 0.3690 (0.7914)  5_DKS_loss: 0.0128 (0.1411)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0600 (1.0652)  data: 0.0869 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:14:55  iter: 8400  loss: 1.3630 (3.3597)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.6559 (1.3158)  1_CE_loss: 0.0091 (0.0983)  2_CE_loss: 0.0264 (0.1094)  2_DKS_loss: 0.0010 (0.0043)  3_CE_loss: 0.0346 (0.1622)  3_DKS_loss: 0.0023 (0.0140)  4_CE_loss: 0.2663 (0.6973)  4_DKS_loss: 0.0047 (0.0288)  5_CE_loss: 0.3671 (0.7865)  5_DKS_loss: 0.0158 (0.1396)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0607 (1.0652)  data: 0.0842 (0.0885)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:13:08  iter: 8500  loss: 1.3818 (3.3377)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6500 (1.3084)  1_CE_loss: 0.0042 (0.0973)  2_CE_loss: 0.0247 (0.1086)  2_DKS_loss: 0.0006 (0.0043)  3_CE_loss: 0.0320 (0.1608)  3_DKS_loss: 0.0020 (0.0138)  4_CE_loss: 0.2686 (0.6925)  4_DKS_loss: 0.0037 (0.0285)  5_CE_loss: 0.3466 (0.7819)  5_DKS_loss: 0.0152 (0.1382)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0593 (1.0651)  data: 0.0861 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:11:21  iter: 8600  loss: 1.4656 (3.3156)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6649 (1.3009)  1_CE_loss: 0.0104 (0.0963)  2_CE_loss: 0.0298 (0.1078)  2_DKS_loss: 0.0007 (0.0042)  3_CE_loss: 0.0201 (0.1593)  3_DKS_loss: 0.0035 (0.0137)  4_CE_loss: 0.2876 (0.6877)  4_DKS_loss: 0.0051 (0.0282)  5_CE_loss: 0.4059 (0.7771)  5_DKS_loss: 0.0224 (0.1368)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0493 (1.0651)  data: 0.0845 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:09:35  iter: 8700  loss: 1.5138 (3.2938)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6579 (1.2935)  1_CE_loss: 0.0117 (0.0953)  2_CE_loss: 0.0355 (0.1070)  2_DKS_loss: 0.0003 (0.0042)  3_CE_loss: 0.0321 (0.1578)  3_DKS_loss: 0.0019 (0.0136)  4_CE_loss: 0.2741 (0.6830)  4_DKS_loss: 0.0042 (0.0279)  5_CE_loss: 0.3803 (0.7724)  5_DKS_loss: 0.0162 (0.1354)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0797 (1.0652)  data: 0.0838 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:07:49  iter: 8800  loss: 1.3587 (3.2722)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6404 (1.2863)  1_CE_loss: 0.0049 (0.0944)  2_CE_loss: 0.0271 (0.1062)  2_DKS_loss: 0.0003 (0.0042)  3_CE_loss: 0.0378 (0.1564)  3_DKS_loss: 0.0018 (0.0135)  4_CE_loss: 0.2480 (0.6784)  4_DKS_loss: 0.0031 (0.0277)  5_CE_loss: 0.3557 (0.7677)  5_DKS_loss: 0.0121 (0.1340)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0652)  data: 0.0862 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:06:02  iter: 8900  loss: 1.4301 (3.2520)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6600 (1.2795)  1_CE_loss: 0.0122 (0.0934)  2_CE_loss: 0.0292 (0.1055)  2_DKS_loss: 0.0002 (0.0041)  3_CE_loss: 0.0320 (0.1551)  3_DKS_loss: 0.0022 (0.0133)  4_CE_loss: 0.2687 (0.6741)  4_DKS_loss: 0.0045 (0.0274)  5_CE_loss: 0.3756 (0.7633)  5_DKS_loss: 0.0175 (0.1327)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0870 (1.0651)  data: 0.0863 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:04:14  iter: 9000  loss: 1.5331 (3.2319)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6497 (1.2726)  1_CE_loss: 0.0129 (0.0925)  2_CE_loss: 0.0415 (0.1048)  2_DKS_loss: 0.0006 (0.0041)  3_CE_loss: 0.0423 (0.1538)  3_DKS_loss: 0.0020 (0.0132)  4_CE_loss: 0.2987 (0.6698)  4_DKS_loss: 0.0051 (0.0271)  5_CE_loss: 0.3571 (0.7590)  5_DKS_loss: 0.0138 (0.1314)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0596 (1.0650)  data: 0.0858 (0.0884)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:02:28  iter: 9100  loss: 1.4592 (3.2122)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6544 (1.2659)  1_CE_loss: 0.0131 (0.0917)  2_CE_loss: 0.0304 (0.1040)  2_DKS_loss: 0.0010 (0.0040)  3_CE_loss: 0.0367 (0.1525)  3_DKS_loss: 0.0016 (0.0131)  4_CE_loss: 0.2839 (0.6656)  4_DKS_loss: 0.0040 (0.0269)  5_CE_loss: 0.3762 (0.7547)  5_DKS_loss: 0.0140 (0.1302)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0667 (1.0650)  data: 0.0858 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 2:00:41  iter: 9200  loss: 1.3418 (3.1932)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6521 (1.2594)  1_CE_loss: 0.0045 (0.0908)  2_CE_loss: 0.0301 (0.1034)  2_DKS_loss: 0.0005 (0.0040)  3_CE_loss: 0.0280 (0.1513)  3_DKS_loss: 0.0023 (0.0130)  4_CE_loss: 0.2712 (0.6615)  4_DKS_loss: 0.0038 (0.0267)  5_CE_loss: 0.3636 (0.7507)  5_DKS_loss: 0.0133 (0.1289)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0680 (1.0649)  data: 0.0848 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:58:54  iter: 9300  loss: 1.4397 (3.1741)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6562 (1.2528)  1_CE_loss: 0.0111 (0.0900)  2_CE_loss: 0.0318 (0.1026)  2_DKS_loss: 0.0004 (0.0040)  3_CE_loss: 0.0293 (0.1501)  3_DKS_loss: 0.0013 (0.0129)  4_CE_loss: 0.2882 (0.6576)  4_DKS_loss: 0.0033 (0.0264)  5_CE_loss: 0.3849 (0.7466)  5_DKS_loss: 0.0134 (0.1277)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0429 (1.0649)  data: 0.0848 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:57:08  iter: 9400  loss: 1.4609 (3.1558)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6805 (1.2466)  1_CE_loss: 0.0071 (0.0892)  2_CE_loss: 0.0293 (0.1020)  2_DKS_loss: 0.0010 (0.0039)  3_CE_loss: 0.0290 (0.1488)  3_DKS_loss: 0.0020 (0.0128)  4_CE_loss: 0.2670 (0.6536)  4_DKS_loss: 0.0031 (0.0262)  5_CE_loss: 0.3747 (0.7426)  5_DKS_loss: 0.0204 (0.1265)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0758 (1.0649)  data: 0.0876 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:55:21  iter: 9500  loss: 1.5049 (3.1379)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6526 (1.2406)  1_CE_loss: 0.0083 (0.0883)  2_CE_loss: 0.0369 (0.1014)  2_DKS_loss: 0.0004 (0.0039)  3_CE_loss: 0.0329 (0.1477)  3_DKS_loss: 0.0016 (0.0126)  4_CE_loss: 0.3021 (0.6498)  4_DKS_loss: 0.0029 (0.0259)  5_CE_loss: 0.3691 (0.7387)  5_DKS_loss: 0.0154 (0.1253)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0610 (1.0649)  data: 0.0841 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:53:34  iter: 9600  loss: 1.5474 (3.1207)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6874 (1.2348)  1_CE_loss: 0.0113 (0.0876)  2_CE_loss: 0.0400 (0.1008)  2_DKS_loss: 0.0003 (0.0039)  3_CE_loss: 0.0311 (0.1465)  3_DKS_loss: 0.0017 (0.0125)  4_CE_loss: 0.3302 (0.6461)  4_DKS_loss: 0.0036 (0.0257)  5_CE_loss: 0.3831 (0.7351)  5_DKS_loss: 0.0201 (0.1243)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0384 (1.0648)  data: 0.0853 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:51:47  iter: 9700  loss: 1.4839 (3.1038)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6517 (1.2290)  1_CE_loss: 0.0032 (0.0868)  2_CE_loss: 0.0348 (0.1002)  2_DKS_loss: 0.0006 (0.0038)  3_CE_loss: 0.0369 (0.1455)  3_DKS_loss: 0.0019 (0.0124)  4_CE_loss: 0.2834 (0.6424)  4_DKS_loss: 0.0040 (0.0255)  5_CE_loss: 0.4103 (0.7315)  5_DKS_loss: 0.0116 (0.1232)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0595 (1.0647)  data: 0.0850 (0.0883)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:50:00  iter: 9800  loss: 1.4646 (3.0868)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6982 (1.2233)  1_CE_loss: 0.0052 (0.0860)  2_CE_loss: 0.0368 (0.0996)  2_DKS_loss: 0.0006 (0.0038)  3_CE_loss: 0.0297 (0.1444)  3_DKS_loss: 0.0017 (0.0123)  4_CE_loss: 0.2971 (0.6387)  4_DKS_loss: 0.0045 (0.0253)  5_CE_loss: 0.3916 (0.7278)  5_DKS_loss: 0.0144 (0.1221)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0650 (1.0646)  data: 0.0854 (0.0882)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:48:14  iter: 9900  loss: 1.4155 (3.0702)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6629 (1.2177)  1_CE_loss: 0.0089 (0.0853)  2_CE_loss: 0.0380 (0.0990)  2_DKS_loss: 0.0010 (0.0038)  3_CE_loss: 0.0362 (0.1433)  3_DKS_loss: 0.0020 (0.0122)  4_CE_loss: 0.2760 (0.6352)  4_DKS_loss: 0.0034 (0.0251)  5_CE_loss: 0.3579 (0.7242)  5_DKS_loss: 0.0149 (0.1210)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0707 (1.0646)  data: 0.0848 (0.0882)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:46:27  iter: 10000  loss: 1.3217 (3.0538)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6370 (1.2121)  1_CE_loss: 0.0077 (0.0845)  2_CE_loss: 0.0317 (0.0984)  2_DKS_loss: 0.0003 (0.0037)  3_CE_loss: 0.0234 (0.1422)  3_DKS_loss: 0.0014 (0.0121)  4_CE_loss: 0.2437 (0.6317)  4_DKS_loss: 0.0031 (0.0248)  5_CE_loss: 0.3599 (0.7207)  5_DKS_loss: 0.0132 (0.1199)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0822 (1.0647)  data: 0.0856 (0.0882)  lr: 0.016000  max mem: 10942
INFO:maskrcnn_benchmark:Start validating
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
INFO:maskrcnn_benchmark:Total run time: 0:04:24.099809 (0.052819961881637574 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:03:55.114661 (0.04702293210029602 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:     R @ 20: 0.3633;     R @ 50: 0.4481;     R @ 100: 0.4778;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.4064;  ng-R @ 50: 0.5656;  ng-R @ 100: 0.6777;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2895;    zR @ 50: 0.3808;    zR @ 100: 0.4417;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.2991; ng-zR @ 50: 0.4705; ng-zR @ 100: 0.5813;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0940;    mR @ 50: 0.1293;    mR @ 100: 0.1501;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0453) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0438) (attached to:0.0128) (behind:0.2558) (belonging to:0.0000) (between:0.0192) (carrying:0.3070) (covered in:0.0000) (covering:0.0000) (eating:0.1429) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.1434) (has:0.6746) (holding:0.4844) (in:0.3295) (in front of:0.0651) (laying on:0.1905) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1688) (of:0.4278) (on:0.5268) (on back of:0.0455) (over:0.0610) (painted on:0.0000) (parked on:0.3731) (part of:0.0000) (playing:0.0000) (riding:0.4762) (says:0.0000) (sitting on:0.0834) (standing on:0.0529) (to:0.1667) (under:0.1233) (using:0.0000) (walking in:0.0000) (walking on:0.9746) (watching:0.1373) (wearing:0.9417) (wears:0.0000) (with:0.1956) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0932;    mR @ 50: 0.1289;    mR @ 100: 0.1510;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0374) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0465) (attached to:0.0189) (behind:0.2653) (belonging to:0.0000) (between:0.0263) (carrying:0.2768) (covered in:0.0000) (covering:0.0000) (eating:0.0435) (flying in:0.0000) (for:0.0270) (from:0.0000) (growing on:0.0000) (hanging from:0.1341) (has:0.7339) (holding:0.4933) (in:0.3299) (in front of:0.0781) (laying on:0.1600) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1722) (of:0.4711) (on:0.5875) (on back of:0.0385) (over:0.0485) (painted on:0.0000) (parked on:0.4192) (part of:0.0000) (playing:0.0000) (riding:0.4646) (says:0.0000) (sitting on:0.0833) (standing on:0.0491) (to:0.2051) (under:0.1160) (using:0.0000) (walking in:0.0000) (walking on:0.9759) (watching:0.1071) (wearing:0.9490) (wears:0.0000) (with:0.1942) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.1079; ng-mR @ 50: 0.1801; ng-mR @ 100: 0.2825;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2691) (across:0.0000) (against:0.0000) (along:0.0769) (and:0.0000) (at:0.1902) (attached to:0.1250) (behind:0.3853) (belonging to:0.0000) (between:0.0385) (carrying:0.6798) (covered in:0.2500) (covering:0.0286) (eating:0.2857) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.4522) (has:0.7594) (holding:0.6233) (in:0.5215) (in front of:0.2415) (laying on:0.2381) (looking at:0.1304) (lying on:0.3333) (made of:0.0000) (mounted on:0.0000) (near:0.3449) (of:0.5947) (on:0.7241) (on back of:0.0682) (over:0.2348) (painted on:0.0000) (parked on:0.7427) (part of:0.0451) (playing:0.0000) (riding:0.6399) (says:1.0000) (sitting on:0.2299) (standing on:0.2022) (to:0.3056) (under:0.2568) (using:0.2308) (walking in:0.0769) (walking on:0.9723) (watching:0.3529) (wearing:0.9647) (wears:0.0398) (with:0.4700) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0878; zs-mR @ 50: 0.1308; zs-mR @ 100: 0.1683;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.2222) (behind:0.5556) (belonging to:0.0000) (between:0.0000) (carrying:1.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.5000) (has:0.6000) (holding:0.0000) (in:0.6706) (in front of:0.3182) (laying on:1.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.3571) (of:0.4000) (on:0.7083) (on back of:0.2500) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.2000) (standing on:0.6667) (to:0.0000) (under:0.2500) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.7143) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.3725;     A @ 50: 0.3736;     A @ 100: 0.3736;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Validation Result: 0.4778
INFO:maskrcnn_benchmark:eta: 1:49:00  iter: 10100  loss: 1.3472 (3.0369)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5985 (1.2062)  1_CE_loss: 0.0026 (0.0838)  2_CE_loss: 0.0269 (0.0978)  2_DKS_loss: 0.0001 (0.0037)  3_CE_loss: 0.0400 (0.1412)  3_DKS_loss: 0.0006 (0.0120)  4_CE_loss: 0.2572 (0.6282)  4_DKS_loss: 0.0012 (0.0246)  5_CE_loss: 0.3402 (0.7171)  5_DKS_loss: 0.0037 (0.1188)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0656 (1.1085)  data: 0.0893 (0.1320)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:47:07  iter: 10200  loss: 1.3921 (3.0199)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6305 (1.2003)  1_CE_loss: 0.0100 (0.0831)  2_CE_loss: 0.0310 (0.0972)  2_DKS_loss: 0.0001 (0.0037)  3_CE_loss: 0.0342 (0.1402)  3_DKS_loss: 0.0006 (0.0119)  4_CE_loss: 0.3020 (0.6246)  4_DKS_loss: 0.0012 (0.0244)  5_CE_loss: 0.3644 (0.7134)  5_DKS_loss: 0.0039 (0.1177)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0680 (1.1081)  data: 0.0901 (0.1316)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:45:13  iter: 10300  loss: 1.1994 (3.0034)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5645 (1.1945)  1_CE_loss: 0.0029 (0.0824)  2_CE_loss: 0.0220 (0.0966)  2_DKS_loss: 0.0001 (0.0036)  3_CE_loss: 0.0286 (0.1391)  3_DKS_loss: 0.0005 (0.0118)  4_CE_loss: 0.2637 (0.6212)  4_DKS_loss: 0.0011 (0.0242)  5_CE_loss: 0.3358 (0.7098)  5_DKS_loss: 0.0036 (0.1166)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0570 (1.1077)  data: 0.0908 (0.1312)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:43:20  iter: 10400  loss: 1.2592 (2.9872)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6109 (1.1890)  1_CE_loss: 0.0092 (0.0817)  2_CE_loss: 0.0307 (0.0960)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0276 (0.1381)  3_DKS_loss: 0.0004 (0.0117)  4_CE_loss: 0.2432 (0.6178)  4_DKS_loss: 0.0010 (0.0239)  5_CE_loss: 0.3239 (0.7063)  5_DKS_loss: 0.0030 (0.1155)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0656 (1.1073)  data: 0.0908 (0.1308)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:41:28  iter: 10500  loss: 1.1931 (2.9712)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5820 (1.1835)  1_CE_loss: 0.0074 (0.0811)  2_CE_loss: 0.0296 (0.0955)  2_DKS_loss: 0.0001 (0.0036)  3_CE_loss: 0.0311 (0.1372)  3_DKS_loss: 0.0004 (0.0116)  4_CE_loss: 0.2461 (0.6144)  4_DKS_loss: 0.0011 (0.0237)  5_CE_loss: 0.3061 (0.7028)  5_DKS_loss: 0.0032 (0.1144)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0707 (1.1070)  data: 0.0902 (0.1305)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:39:35  iter: 10600  loss: 1.2639 (2.9557)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5949 (1.1782)  1_CE_loss: 0.0067 (0.0804)  2_CE_loss: 0.0347 (0.0949)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0327 (0.1362)  3_DKS_loss: 0.0004 (0.0115)  4_CE_loss: 0.2819 (0.6112)  4_DKS_loss: 0.0009 (0.0235)  5_CE_loss: 0.3315 (0.6995)  5_DKS_loss: 0.0027 (0.1134)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0459 (1.1065)  data: 0.0891 (0.1301)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:37:42  iter: 10700  loss: 1.3661 (2.9403)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.6347 (1.1728)  1_CE_loss: 0.0047 (0.0798)  2_CE_loss: 0.0314 (0.0944)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0316 (0.1353)  3_DKS_loss: 0.0005 (0.0114)  4_CE_loss: 0.2853 (0.6079)  4_DKS_loss: 0.0011 (0.0233)  5_CE_loss: 0.3510 (0.6961)  5_DKS_loss: 0.0028 (0.1123)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0702 (1.1062)  data: 0.0884 (0.1297)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:35:49  iter: 10800  loss: 1.2214 (2.9251)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5947 (1.1675)  1_CE_loss: 0.0065 (0.0792)  2_CE_loss: 0.0255 (0.0939)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0307 (0.1343)  3_DKS_loss: 0.0004 (0.0113)  4_CE_loss: 0.2466 (0.6048)  4_DKS_loss: 0.0010 (0.0231)  5_CE_loss: 0.3191 (0.6928)  5_DKS_loss: 0.0027 (0.1113)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0677 (1.1057)  data: 0.0878 (0.1293)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:33:57  iter: 10900  loss: 1.2293 (2.9095)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5563 (1.1621)  1_CE_loss: 0.0095 (0.0785)  2_CE_loss: 0.0269 (0.0933)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0270 (0.1334)  3_DKS_loss: 0.0004 (0.0112)  4_CE_loss: 0.2441 (0.6015)  4_DKS_loss: 0.0010 (0.0229)  5_CE_loss: 0.3124 (0.6894)  5_DKS_loss: 0.0030 (0.1103)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0613 (1.1054)  data: 0.0898 (0.1290)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:32:05  iter: 11000  loss: 1.1766 (2.8942)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5264 (1.1567)  1_CE_loss: 0.0054 (0.0779)  2_CE_loss: 0.0388 (0.0928)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0369 (0.1325)  3_DKS_loss: 0.0004 (0.0111)  4_CE_loss: 0.2304 (0.5984)  4_DKS_loss: 0.0010 (0.0227)  5_CE_loss: 0.2930 (0.6860)  5_DKS_loss: 0.0029 (0.1093)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0613 (1.1050)  data: 0.0879 (0.1286)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:30:12  iter: 11100  loss: 1.2887 (2.8792)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5418 (1.1513)  1_CE_loss: 0.0096 (0.0773)  2_CE_loss: 0.0386 (0.0922)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0320 (0.1316)  3_DKS_loss: 0.0004 (0.0110)  4_CE_loss: 0.2651 (0.5952)  4_DKS_loss: 0.0010 (0.0225)  5_CE_loss: 0.3393 (0.6828)  5_DKS_loss: 0.0029 (0.1084)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0575 (1.1046)  data: 0.0892 (0.1283)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:28:20  iter: 11200  loss: 1.0837 (2.8642)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5232 (1.1461)  1_CE_loss: 0.0051 (0.0767)  2_CE_loss: 0.0295 (0.0917)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0259 (0.1307)  3_DKS_loss: 0.0004 (0.0109)  4_CE_loss: 0.2249 (0.5921)  4_DKS_loss: 0.0009 (0.0223)  5_CE_loss: 0.2977 (0.6795)  5_DKS_loss: 0.0028 (0.1074)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.1043)  data: 0.0902 (0.1279)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:26:29  iter: 11300  loss: 1.1467 (2.8495)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5310 (1.1409)  1_CE_loss: 0.0021 (0.0761)  2_CE_loss: 0.0321 (0.0912)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0259 (0.1298)  3_DKS_loss: 0.0004 (0.0108)  4_CE_loss: 0.2353 (0.5890)  4_DKS_loss: 0.0009 (0.0221)  5_CE_loss: 0.3096 (0.6763)  5_DKS_loss: 0.0027 (0.1065)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0781 (1.1040)  data: 0.0904 (0.1276)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:24:36  iter: 11400  loss: 1.2252 (2.8355)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5624 (1.1359)  1_CE_loss: 0.0052 (0.0755)  2_CE_loss: 0.0311 (0.0908)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0278 (0.1290)  3_DKS_loss: 0.0003 (0.0107)  4_CE_loss: 0.2363 (0.5861)  4_DKS_loss: 0.0010 (0.0219)  5_CE_loss: 0.3030 (0.6732)  5_DKS_loss: 0.0025 (0.1056)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0590 (1.1037)  data: 0.0888 (0.1273)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:22:44  iter: 11500  loss: 1.1542 (2.8213)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5426 (1.1309)  1_CE_loss: 0.0036 (0.0750)  2_CE_loss: 0.0390 (0.0903)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0239 (0.1281)  3_DKS_loss: 0.0003 (0.0106)  4_CE_loss: 0.2320 (0.5831)  4_DKS_loss: 0.0009 (0.0217)  5_CE_loss: 0.3019 (0.6701)  5_DKS_loss: 0.0025 (0.1047)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0629 (1.1033)  data: 0.0895 (0.1270)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:20:53  iter: 11600  loss: 1.2386 (2.8077)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5649 (1.1262)  1_CE_loss: 0.0080 (0.0744)  2_CE_loss: 0.0279 (0.0898)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0386 (0.1274)  3_DKS_loss: 0.0004 (0.0105)  4_CE_loss: 0.2494 (0.5802)  4_DKS_loss: 0.0010 (0.0216)  5_CE_loss: 0.3176 (0.6671)  5_DKS_loss: 0.0026 (0.1038)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0674 (1.1030)  data: 0.0898 (0.1266)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:19:01  iter: 11700  loss: 1.2205 (2.7939)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5766 (1.1213)  1_CE_loss: 0.0053 (0.0739)  2_CE_loss: 0.0384 (0.0893)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0329 (0.1265)  3_DKS_loss: 0.0004 (0.0104)  4_CE_loss: 0.2338 (0.5773)  4_DKS_loss: 0.0009 (0.0214)  5_CE_loss: 0.3023 (0.6640)  5_DKS_loss: 0.0028 (0.1030)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0780 (1.1027)  data: 0.0916 (0.1263)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:17:10  iter: 11800  loss: 1.2240 (2.7804)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5881 (1.1165)  1_CE_loss: 0.0032 (0.0733)  2_CE_loss: 0.0360 (0.0889)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0310 (0.1257)  3_DKS_loss: 0.0004 (0.0103)  4_CE_loss: 0.2520 (0.5745)  4_DKS_loss: 0.0009 (0.0212)  5_CE_loss: 0.3163 (0.6611)  5_DKS_loss: 0.0031 (0.1021)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0599 (1.1024)  data: 0.0886 (0.1260)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:15:18  iter: 11900  loss: 1.1875 (2.7673)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5597 (1.1119)  1_CE_loss: 0.0046 (0.0728)  2_CE_loss: 0.0350 (0.0885)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0291 (0.1250)  3_DKS_loss: 0.0003 (0.0103)  4_CE_loss: 0.2510 (0.5718)  4_DKS_loss: 0.0009 (0.0210)  5_CE_loss: 0.3094 (0.6582)  5_DKS_loss: 0.0024 (0.1013)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0694 (1.1021)  data: 0.0884 (0.1257)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:---Total norm 2.67064 clip coef 1.87221-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.19554, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.98370, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.82363, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.76314, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.65707, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.60188, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.52868, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.41496, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.38070, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.35240, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.33117, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.31469, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.30509, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.30270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.29465, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27714, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.27607, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27460, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.27343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27124, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.23524, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20380, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.17293, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.17054, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.16635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.16517, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.16175, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.16007, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15108, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14670, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14592, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.13648, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13606, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12291, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12183, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12154, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10609, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.10244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10027, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09886, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09630, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09546, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.09183, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09067, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09047, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09030, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08770, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08618, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08501, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08477, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08344, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08082, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07958, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07930, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07913, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07684, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.07575, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07492, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07190, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07151, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06950, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.06940, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06937, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06917, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06895, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06832, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06660, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06397, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.06389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06373, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05920, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05879, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05827, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05689, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05634, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05530, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05410, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05329, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05248, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05225, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.05178, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.05178, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05134, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05055, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04984, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04978, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04898, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04878, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04832, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04731, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04710, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04692, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04644, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04618, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04492, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04471, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04463, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04457, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04455, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04441, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04437, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.04430, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04391, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04372, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04363, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04322, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04280, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04245, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04194, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04131, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04057, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04028, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04010, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03992, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03974, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03822, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03814, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03751, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03649, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03579, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03577, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03571, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03546, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03363, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03242, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02965, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.02954, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02941, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02935, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02925, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02855, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02826, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02770, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02764, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02742, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02742, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02700, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02669, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02664, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02625, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02620, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02585, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.02568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02520, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02512, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02472, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02465, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02434, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02406, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02393, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02308, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02165, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02094, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02076, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02054, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01990, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.01940, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01887, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.01871, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.01871, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01810, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01775, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01758, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01739, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01710, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01684, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01658, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01637, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01550, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01540, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01534, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01533, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01507, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01506, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01496, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.01483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01449, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01399, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01314, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01282, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01214, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01186, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01186, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01162, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01158, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01154, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01145, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01011, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01004, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00985, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00945, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.00923, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00890, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00865, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00847, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00794, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00794, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00721, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00715, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00713, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00712, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00692, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00685, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00672, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00661, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00659, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00651, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00648, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00635, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00631, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00627, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00592, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00583, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00555, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00554, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00554, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00532, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00526, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00522, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00522, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00515, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00494, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00487, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00478, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00471, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00471, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00463, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00446, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00433, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00433, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00414, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00406, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00396, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00394, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00391, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00391, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00379, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00378, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00366, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00365, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00361, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00357, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00355, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00343, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00343, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00342, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00340, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00337, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00336, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00329, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00329, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00329, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00329, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00328, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00327, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00326, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00325, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00319, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00314, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00313, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00310, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00305, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00299, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00274, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00265, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00243, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00209, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00200, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00196, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00186, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00180, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00154, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00145, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00130, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00130, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00129, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00128, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00128, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00126, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00126, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00124, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00119, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00119, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00119, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00116, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00116, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00114, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00110, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00110, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00108, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00106, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00103, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00102, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00097, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00076, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00076, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00072, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00069, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00061, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00057, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00048, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00045, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00044, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00041, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00032, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00027, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00021, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00010, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1:13:27  iter: 12000  loss: 1.1897 (2.7546)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5724 (1.1075)  1_CE_loss: 0.0039 (0.0723)  2_CE_loss: 0.0319 (0.0880)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0273 (0.1242)  3_DKS_loss: 0.0004 (0.0102)  4_CE_loss: 0.2475 (0.5691)  4_DKS_loss: 0.0009 (0.0209)  5_CE_loss: 0.3093 (0.6554)  5_DKS_loss: 0.0026 (0.1005)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0850 (1.1018)  data: 0.0894 (0.1254)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:11:36  iter: 12100  loss: 1.2090 (2.7418)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5500 (1.1029)  1_CE_loss: 0.0087 (0.0718)  2_CE_loss: 0.0305 (0.0876)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0301 (0.1235)  3_DKS_loss: 0.0004 (0.0101)  4_CE_loss: 0.2392 (0.5664)  4_DKS_loss: 0.0009 (0.0207)  5_CE_loss: 0.3136 (0.6525)  5_DKS_loss: 0.0023 (0.0996)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0682 (1.1016)  data: 0.0892 (0.1251)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:09:44  iter: 12200  loss: 1.1497 (2.7291)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5211 (1.0984)  1_CE_loss: 0.0041 (0.0713)  2_CE_loss: 0.0347 (0.0872)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0323 (0.1227)  3_DKS_loss: 0.0003 (0.0100)  4_CE_loss: 0.2137 (0.5638)  4_DKS_loss: 0.0008 (0.0205)  5_CE_loss: 0.3022 (0.6497)  5_DKS_loss: 0.0028 (0.0989)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0709 (1.1013)  data: 0.0886 (0.1249)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:07:53  iter: 12300  loss: 1.2606 (2.7171)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5716 (1.0941)  1_CE_loss: 0.0109 (0.0708)  2_CE_loss: 0.0332 (0.0867)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0374 (0.1220)  3_DKS_loss: 0.0004 (0.0099)  4_CE_loss: 0.2636 (0.5613)  4_DKS_loss: 0.0010 (0.0204)  5_CE_loss: 0.3193 (0.6471)  5_DKS_loss: 0.0026 (0.0981)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.1011)  data: 0.0907 (0.1246)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:06:02  iter: 12400  loss: 1.2457 (2.7049)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5403 (1.0898)  1_CE_loss: 0.0076 (0.0703)  2_CE_loss: 0.0360 (0.0864)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0407 (0.1213)  3_DKS_loss: 0.0004 (0.0099)  4_CE_loss: 0.2574 (0.5588)  4_DKS_loss: 0.0010 (0.0202)  5_CE_loss: 0.3290 (0.6444)  5_DKS_loss: 0.0026 (0.0973)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0650 (1.1008)  data: 0.0876 (0.1243)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:04:11  iter: 12500  loss: 1.2004 (2.6930)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5381 (1.0856)  1_CE_loss: 0.0018 (0.0698)  2_CE_loss: 0.0297 (0.0860)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0276 (0.1207)  3_DKS_loss: 0.0003 (0.0098)  4_CE_loss: 0.2233 (0.5563)  4_DKS_loss: 0.0009 (0.0201)  5_CE_loss: 0.3099 (0.6418)  5_DKS_loss: 0.0024 (0.0965)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.1005)  data: 0.0915 (0.1240)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:02:20  iter: 12600  loss: 1.2711 (2.6813)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.6010 (1.0814)  1_CE_loss: 0.0064 (0.0693)  2_CE_loss: 0.0282 (0.0855)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0260 (0.1200)  3_DKS_loss: 0.0004 (0.0097)  4_CE_loss: 0.2425 (0.5539)  4_DKS_loss: 0.0009 (0.0199)  5_CE_loss: 0.3278 (0.6392)  5_DKS_loss: 0.0023 (0.0958)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0653 (1.1002)  data: 0.0908 (0.1238)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 1:00:29  iter: 12700  loss: 1.1867 (2.6696)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0041 (0.0036)  rel_ce_loss: 0.5965 (1.0773)  1_CE_loss: 0.0057 (0.0688)  2_CE_loss: 0.0273 (0.0851)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0271 (0.1193)  3_DKS_loss: 0.0003 (0.0096)  4_CE_loss: 0.2203 (0.5514)  4_DKS_loss: 0.0009 (0.0198)  5_CE_loss: 0.2880 (0.6367)  5_DKS_loss: 0.0024 (0.0951)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0782 (1.0999)  data: 0.0891 (0.1235)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:58:38  iter: 12800  loss: 1.2633 (2.6586)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5780 (1.0734)  1_CE_loss: 0.0053 (0.0684)  2_CE_loss: 0.0402 (0.0848)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0369 (0.1186)  3_DKS_loss: 0.0004 (0.0096)  4_CE_loss: 0.2618 (0.5491)  4_DKS_loss: 0.0010 (0.0196)  5_CE_loss: 0.3351 (0.6343)  5_DKS_loss: 0.0026 (0.0943)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0617 (1.0996)  data: 0.0901 (0.1233)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:56:48  iter: 12900  loss: 1.2041 (2.6473)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5697 (1.0695)  1_CE_loss: 0.0077 (0.0679)  2_CE_loss: 0.0294 (0.0844)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0312 (0.1180)  3_DKS_loss: 0.0003 (0.0095)  4_CE_loss: 0.2242 (0.5467)  4_DKS_loss: 0.0009 (0.0195)  5_CE_loss: 0.2917 (0.6317)  5_DKS_loss: 0.0024 (0.0936)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0721 (1.0994)  data: 0.0903 (0.1230)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:54:57  iter: 13000  loss: 1.2548 (2.6362)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5746 (1.0656)  1_CE_loss: 0.0094 (0.0675)  2_CE_loss: 0.0306 (0.0840)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0264 (0.1173)  3_DKS_loss: 0.0004 (0.0094)  4_CE_loss: 0.2411 (0.5444)  4_DKS_loss: 0.0009 (0.0193)  5_CE_loss: 0.3241 (0.6294)  5_DKS_loss: 0.0024 (0.0929)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0721 (1.0991)  data: 0.0885 (0.1227)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:53:06  iter: 13100  loss: 1.1830 (2.6252)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5566 (1.0617)  1_CE_loss: 0.0026 (0.0671)  2_CE_loss: 0.0248 (0.0836)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0299 (0.1166)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2339 (0.5421)  4_DKS_loss: 0.0009 (0.0192)  5_CE_loss: 0.3249 (0.6270)  5_DKS_loss: 0.0027 (0.0922)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0602 (1.0989)  data: 0.0926 (0.1225)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:51:16  iter: 13200  loss: 1.1210 (2.6144)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5707 (1.0579)  1_CE_loss: 0.0051 (0.0666)  2_CE_loss: 0.0294 (0.0832)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0290 (0.1160)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2307 (0.5398)  4_DKS_loss: 0.0009 (0.0191)  5_CE_loss: 0.3189 (0.6246)  5_DKS_loss: 0.0025 (0.0916)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0628 (1.0987)  data: 0.0920 (0.1223)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:49:25  iter: 13300  loss: 1.1937 (2.6037)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5221 (1.0541)  1_CE_loss: 0.0060 (0.0662)  2_CE_loss: 0.0308 (0.0828)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0281 (0.1153)  3_DKS_loss: 0.0003 (0.0092)  4_CE_loss: 0.2370 (0.5376)  4_DKS_loss: 0.0009 (0.0189)  5_CE_loss: 0.3129 (0.6222)  5_DKS_loss: 0.0027 (0.0909)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0516 (1.0985)  data: 0.0893 (0.1220)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:47:35  iter: 13400  loss: 1.1591 (2.5934)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5614 (1.0505)  1_CE_loss: 0.0085 (0.0658)  2_CE_loss: 0.0253 (0.0825)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0219 (0.1147)  3_DKS_loss: 0.0004 (0.0091)  4_CE_loss: 0.2201 (0.5354)  4_DKS_loss: 0.0010 (0.0188)  5_CE_loss: 0.3047 (0.6200)  5_DKS_loss: 0.0027 (0.0902)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0689 (1.0982)  data: 0.0889 (0.1218)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:45:44  iter: 13500  loss: 1.1985 (2.5832)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5634 (1.0469)  1_CE_loss: 0.0182 (0.0654)  2_CE_loss: 0.0362 (0.0821)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0359 (0.1141)  3_DKS_loss: 0.0004 (0.0091)  4_CE_loss: 0.2441 (0.5332)  4_DKS_loss: 0.0009 (0.0187)  5_CE_loss: 0.3519 (0.6177)  5_DKS_loss: 0.0027 (0.0896)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0979)  data: 0.0896 (0.1216)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:43:54  iter: 13600  loss: 1.1545 (2.5730)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5437 (1.0433)  1_CE_loss: 0.0020 (0.0650)  2_CE_loss: 0.0210 (0.0817)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0237 (0.1135)  3_DKS_loss: 0.0003 (0.0090)  4_CE_loss: 0.2114 (0.5312)  4_DKS_loss: 0.0008 (0.0185)  5_CE_loss: 0.2941 (0.6155)  5_DKS_loss: 0.0024 (0.0889)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0851 (1.0977)  data: 0.0901 (0.1213)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:42:04  iter: 13700  loss: 1.2094 (2.5630)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5512 (1.0398)  1_CE_loss: 0.0094 (0.0646)  2_CE_loss: 0.0250 (0.0814)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0198 (0.1129)  3_DKS_loss: 0.0003 (0.0090)  4_CE_loss: 0.2389 (0.5290)  4_DKS_loss: 0.0009 (0.0184)  5_CE_loss: 0.3144 (0.6133)  5_DKS_loss: 0.0025 (0.0883)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0651 (1.0975)  data: 0.0906 (0.1211)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:40:13  iter: 13800  loss: 1.2632 (2.5532)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5674 (1.0363)  1_CE_loss: 0.0050 (0.0642)  2_CE_loss: 0.0316 (0.0810)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0282 (0.1123)  3_DKS_loss: 0.0003 (0.0089)  4_CE_loss: 0.2559 (0.5269)  4_DKS_loss: 0.0009 (0.0183)  5_CE_loss: 0.3258 (0.6112)  5_DKS_loss: 0.0027 (0.0877)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0761 (1.0972)  data: 0.0905 (0.1209)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:38:23  iter: 13900  loss: 1.2293 (2.5435)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5690 (1.0329)  1_CE_loss: 0.0078 (0.0638)  2_CE_loss: 0.0252 (0.0807)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0218 (0.1117)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2273 (0.5249)  4_DKS_loss: 0.0009 (0.0181)  5_CE_loss: 0.3290 (0.6091)  5_DKS_loss: 0.0023 (0.0871)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0734 (1.0970)  data: 0.0883 (0.1207)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:36:33  iter: 14000  loss: 1.1513 (2.5339)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5288 (1.0294)  1_CE_loss: 0.0032 (0.0635)  2_CE_loss: 0.0302 (0.0804)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0181 (0.1111)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2346 (0.5229)  4_DKS_loss: 0.0008 (0.0180)  5_CE_loss: 0.3041 (0.6070)  5_DKS_loss: 0.0023 (0.0865)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0505 (1.0968)  data: 0.0892 (0.1204)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:34:43  iter: 14100  loss: 1.1546 (2.5245)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5355 (1.0261)  1_CE_loss: 0.0045 (0.0631)  2_CE_loss: 0.0260 (0.0800)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0254 (0.1106)  3_DKS_loss: 0.0003 (0.0087)  4_CE_loss: 0.2259 (0.5210)  4_DKS_loss: 0.0008 (0.0179)  5_CE_loss: 0.2915 (0.6050)  5_DKS_loss: 0.0023 (0.0859)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0777 (1.0965)  data: 0.0917 (0.1202)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:32:53  iter: 14200  loss: 1.1861 (2.5153)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5546 (1.0229)  1_CE_loss: 0.0128 (0.0627)  2_CE_loss: 0.0305 (0.0797)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0277 (0.1100)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2401 (0.5191)  4_DKS_loss: 0.0009 (0.0178)  5_CE_loss: 0.3055 (0.6029)  5_DKS_loss: 0.0024 (0.0853)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0747 (1.0963)  data: 0.0882 (0.1200)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:31:03  iter: 14300  loss: 1.1803 (2.5059)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5324 (1.0195)  1_CE_loss: 0.0022 (0.0623)  2_CE_loss: 0.0173 (0.0794)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0225 (0.1095)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2534 (0.5171)  4_DKS_loss: 0.0008 (0.0177)  5_CE_loss: 0.3052 (0.6008)  5_DKS_loss: 0.0025 (0.0847)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0698 (1.0961)  data: 0.0895 (0.1198)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:29:13  iter: 14400  loss: 1.1509 (2.4968)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5603 (1.0163)  1_CE_loss: 0.0077 (0.0620)  2_CE_loss: 0.0341 (0.0791)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0256 (0.1089)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2284 (0.5152)  4_DKS_loss: 0.0009 (0.0175)  5_CE_loss: 0.3035 (0.5989)  5_DKS_loss: 0.0024 (0.0841)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0638 (1.0959)  data: 0.0889 (0.1196)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:27:23  iter: 14500  loss: 1.1444 (2.4876)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5124 (1.0131)  1_CE_loss: 0.0034 (0.0616)  2_CE_loss: 0.0273 (0.0788)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0309 (0.1084)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.1982 (0.5132)  4_DKS_loss: 0.0009 (0.0174)  5_CE_loss: 0.3019 (0.5968)  5_DKS_loss: 0.0024 (0.0836)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0947 (1.0956)  data: 0.0888 (0.1194)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:25:33  iter: 14600  loss: 1.2021 (2.4786)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5195 (1.0098)  1_CE_loss: 0.0034 (0.0613)  2_CE_loss: 0.0367 (0.0785)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0334 (0.1079)  3_DKS_loss: 0.0004 (0.0084)  4_CE_loss: 0.2338 (0.5113)  4_DKS_loss: 0.0009 (0.0173)  5_CE_loss: 0.3008 (0.5948)  5_DKS_loss: 0.0026 (0.0830)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0554 (1.0954)  data: 0.0908 (0.1192)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:23:43  iter: 14700  loss: 1.1428 (2.4699)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5086 (1.0067)  1_CE_loss: 0.0030 (0.0609)  2_CE_loss: 0.0352 (0.0782)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0271 (0.1074)  3_DKS_loss: 0.0003 (0.0084)  4_CE_loss: 0.2368 (0.5095)  4_DKS_loss: 0.0009 (0.0172)  5_CE_loss: 0.3076 (0.5929)  5_DKS_loss: 0.0031 (0.0825)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0698 (1.0952)  data: 0.0880 (0.1190)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:21:54  iter: 14800  loss: 1.1926 (2.4611)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0037)  rel_ce_loss: 0.5608 (1.0036)  1_CE_loss: 0.0069 (0.0606)  2_CE_loss: 0.0278 (0.0779)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0232 (0.1069)  3_DKS_loss: 0.0004 (0.0083)  4_CE_loss: 0.2326 (0.5076)  4_DKS_loss: 0.0010 (0.0171)  5_CE_loss: 0.3078 (0.5910)  5_DKS_loss: 0.0025 (0.0819)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0721 (1.0950)  data: 0.0879 (0.1188)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:20:04  iter: 14900  loss: 1.0623 (2.4522)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0041 (0.0037)  rel_ce_loss: 0.4964 (1.0004)  1_CE_loss: 0.0059 (0.0602)  2_CE_loss: 0.0318 (0.0776)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0231 (0.1064)  3_DKS_loss: 0.0003 (0.0083)  4_CE_loss: 0.2229 (0.5058)  4_DKS_loss: 0.0009 (0.0170)  5_CE_loss: 0.2835 (0.5890)  5_DKS_loss: 0.0025 (0.0814)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0698 (1.0949)  data: 0.0900 (0.1186)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:18:14  iter: 15000  loss: 1.1015 (2.4436)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5289 (0.9973)  1_CE_loss: 0.0083 (0.0599)  2_CE_loss: 0.0234 (0.0773)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0262 (0.1059)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2201 (0.5039)  4_DKS_loss: 0.0008 (0.0169)  5_CE_loss: 0.2959 (0.5871)  5_DKS_loss: 0.0024 (0.0809)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0529 (1.0947)  data: 0.0908 (0.1184)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:16:25  iter: 15100  loss: 1.1660 (2.4351)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5288 (0.9942)  1_CE_loss: 0.0045 (0.0596)  2_CE_loss: 0.0324 (0.0770)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0265 (0.1054)  3_DKS_loss: 0.0004 (0.0082)  4_CE_loss: 0.2270 (0.5021)  4_DKS_loss: 0.0010 (0.0168)  5_CE_loss: 0.3099 (0.5853)  5_DKS_loss: 0.0024 (0.0804)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0758 (1.0945)  data: 0.0895 (0.1183)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:14:35  iter: 15200  loss: 1.1292 (2.4268)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.4944 (0.9913)  1_CE_loss: 0.0017 (0.0593)  2_CE_loss: 0.0219 (0.0767)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0238 (0.1049)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2316 (0.5004)  4_DKS_loss: 0.0008 (0.0167)  5_CE_loss: 0.2979 (0.5835)  5_DKS_loss: 0.0023 (0.0798)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0632 (1.0943)  data: 0.0901 (0.1181)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:12:45  iter: 15300  loss: 1.2342 (2.4187)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0037)  rel_ce_loss: 0.5662 (0.9884)  1_CE_loss: 0.0120 (0.0590)  2_CE_loss: 0.0301 (0.0764)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0188 (0.1044)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2347 (0.4987)  4_DKS_loss: 0.0009 (0.0166)  5_CE_loss: 0.3077 (0.5817)  5_DKS_loss: 0.0024 (0.0793)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0815 (1.0942)  data: 0.0892 (0.1179)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:10:56  iter: 15400  loss: 1.1561 (2.4107)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5334 (0.9856)  1_CE_loss: 0.0051 (0.0586)  2_CE_loss: 0.0309 (0.0762)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0272 (0.1039)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2458 (0.4970)  4_DKS_loss: 0.0009 (0.0165)  5_CE_loss: 0.3183 (0.5800)  5_DKS_loss: 0.0023 (0.0788)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0680 (1.0940)  data: 0.0895 (0.1177)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:09:06  iter: 15500  loss: 1.1155 (2.4027)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5253 (0.9827)  1_CE_loss: 0.0079 (0.0583)  2_CE_loss: 0.0307 (0.0759)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0332 (0.1035)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2421 (0.4954)  4_DKS_loss: 0.0009 (0.0164)  5_CE_loss: 0.2936 (0.5782)  5_DKS_loss: 0.0029 (0.0784)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0711 (1.0938)  data: 0.0897 (0.1175)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:07:17  iter: 15600  loss: 1.1243 (2.3949)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0039 (0.0037)  rel_ce_loss: 0.5113 (0.9799)  1_CE_loss: 0.0029 (0.0580)  2_CE_loss: 0.0321 (0.0756)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0262 (0.1030)  3_DKS_loss: 0.0004 (0.0079)  4_CE_loss: 0.2334 (0.4938)  4_DKS_loss: 0.0010 (0.0163)  5_CE_loss: 0.3078 (0.5764)  5_DKS_loss: 0.0025 (0.0779)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0816 (1.0936)  data: 0.0930 (0.1174)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:05:28  iter: 15700  loss: 1.1671 (2.3872)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5224 (0.9771)  1_CE_loss: 0.0035 (0.0577)  2_CE_loss: 0.0279 (0.0754)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0263 (0.1026)  3_DKS_loss: 0.0003 (0.0079)  4_CE_loss: 0.2506 (0.4922)  4_DKS_loss: 0.0010 (0.0162)  5_CE_loss: 0.3233 (0.5748)  5_DKS_loss: 0.0026 (0.0774)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0647 (1.0934)  data: 0.0893 (0.1172)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:03:38  iter: 15800  loss: 1.1592 (2.3795)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5658 (0.9744)  1_CE_loss: 0.0033 (0.0574)  2_CE_loss: 0.0313 (0.0751)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0262 (0.1022)  3_DKS_loss: 0.0004 (0.0078)  4_CE_loss: 0.2297 (0.4905)  4_DKS_loss: 0.0009 (0.0161)  5_CE_loss: 0.3093 (0.5731)  5_DKS_loss: 0.0027 (0.0769)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0760 (1.0933)  data: 0.0914 (0.1170)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:eta: 0:01:49  iter: 15900  loss: 1.1333 (2.3721)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0040 (0.0037)  rel_ce_loss: 0.5382 (0.9717)  1_CE_loss: 0.0079 (0.0571)  2_CE_loss: 0.0276 (0.0749)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0274 (0.1018)  3_DKS_loss: 0.0004 (0.0078)  4_CE_loss: 0.2241 (0.4890)  4_DKS_loss: 0.0010 (0.0160)  5_CE_loss: 0.3263 (0.5715)  5_DKS_loss: 0.0024 (0.0764)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0649 (1.0931)  data: 0.0906 (0.1169)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark:---Total norm 2.86774 clip coef 1.74354-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.32113, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.23728, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.03486, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.82224, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.76877, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.65480, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.53085, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.41923, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.39068, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.37483, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.33919, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.32140, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.27874, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.22480, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21780, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21364, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20777, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.20739, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20660, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.20382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.20382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.20382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.20382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18295, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.16357, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.15610, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.15036, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.14161, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13832, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.12969, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12448, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11690, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.11546, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.11368, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11151, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10789, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10298, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10043, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10037, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10018, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09949, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.09909, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09381, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09205, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09076, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09064, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09063, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09023, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09014, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08803, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08793, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08574, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08515, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08468, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08346, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08247, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08027, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07755, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07708, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.07702, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07625, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07562, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06657, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06485, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06441, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06400, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06307, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06191, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06066, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06063, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06062, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06059, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06032, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05973, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.05946, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05864, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.05742, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05737, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05708, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05675, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05668, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05644, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05539, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05508, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05499, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05480, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05476, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05292, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05207, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05172, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05112, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05078, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05078, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05055, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05049, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05047, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05009, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05006, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04826, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04776, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04758, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04680, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04649, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04595, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04543, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04535, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04485, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.04418, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.04418, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04246, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04243, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04208, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04046, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04034, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.04011, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03991, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03768, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03711, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03644, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03633, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03584, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03454, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.03432, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03409, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03363, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.03354, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03340, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03318, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03226, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03112, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03102, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03098, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03063, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03048, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.03045, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03035, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02980, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02978, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02963, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02956, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02853, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02802, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02781, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02774, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02773, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02755, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02739, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02733, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02721, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02700, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02552, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02484, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02472, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02468, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02377, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.02228, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.02180, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.02180, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.02167, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01998, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.01979, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01947, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01909, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01864, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01859, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01857, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01792, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01756, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01756, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.01720, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01697, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01694, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01545, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01513, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01459, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01416, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01339, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01329, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01307, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01299, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01295, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01264, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01181, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01181, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01113, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01004, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01003, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01003, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00997, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00950, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00945, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00938, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00928, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00899, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00864, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00853, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00837, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00768, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00743, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.00729, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00729, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00728, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00687, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00680, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00669, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00668, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00668, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00654, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00641, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00638, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00631, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00627, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00625, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00625, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00622, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00605, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00602, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00596, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00593, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00593, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00558, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00535, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00534, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00494, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00492, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00483, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00481, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00462, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00455, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00454, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00451, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00445, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00443, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00442, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00441, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00439, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00439, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00428, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00425, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00422, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00421, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00416, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00413, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00412, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00410, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00408, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00403, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00396, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00396, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00394, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00384, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00384, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00375, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00373, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00368, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00367, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00351, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00348, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00342, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00342, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00301, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00300, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00300, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00273, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00256, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00245, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00242, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00239, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00196, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00193, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00177, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00169, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00159, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00158, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00158, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00154, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00153, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00152, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00147, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00147, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00147, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00145, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00145, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00139, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00138, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00138, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00136, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00132, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00131, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00131, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00128, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00127, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00113, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00104, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00101, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00100, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00098, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00097, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00093, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00078, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00072, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00067, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00067, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00053, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00053, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00053, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00046, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00041, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00036, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00024, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00023, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00011, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 0:00:00  iter: 16000  loss: 1.0967 (2.3644)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.0041 (0.0037)  rel_ce_loss: 0.5274 (0.9689)  1_CE_loss: 0.0021 (0.0568)  2_CE_loss: 0.0335 (0.0746)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0329 (0.1013)  3_DKS_loss: 0.0003 (0.0077)  4_CE_loss: 0.2338 (0.4874)  4_DKS_loss: 0.0010 (0.0159)  5_CE_loss: 0.2972 (0.5698)  5_DKS_loss: 0.0024 (0.0760)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0738 (1.0929)  data: 0.0904 (0.1167)  lr: 0.001600  max mem: 10942
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-predcls0.001/model_0016000.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-predcls0.001/model_final.pth
INFO:maskrcnn_benchmark:Total training time: 4:51:30.831287 (1.0932 s / it)
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_test dataset(26446 images).
INFO:maskrcnn_benchmark:Total run time: 0:24:31.688052 (0.055648795750429095 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:21:42.132320 (0.049237401503654014 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9995
====================================================================================================
SGG eval:     R @ 20: 0.4479;     R @ 50: 0.5496;     R @ 100: 0.5850;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.4884;  ng-R @ 50: 0.6587;  ng-R @ 100: 0.7630;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2696;    zR @ 50: 0.3475;    zR @ 100: 0.3834;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.2905; ng-zR @ 50: 0.4251; ng-zR @ 100: 0.5434;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0917;    mR @ 50: 0.1408;    mR @ 100: 0.1683;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0724) (across:0.0000) (against:0.0000) (along:0.0183) (and:0.0079) (at:0.1022) (attached to:0.0274) (behind:0.2777) (belonging to:0.0153) (between:0.0069) (carrying:0.3176) (covered in:0.1881) (covering:0.0321) (eating:0.2204) (flying in:0.0000) (for:0.0191) (from:0.0141) (growing on:0.0000) (hanging from:0.0938) (has:0.6825) (holding:0.5455) (in:0.2776) (in front of:0.1631) (laying on:0.1625) (looking at:0.0910) (lying on:0.0357) (made of:0.0312) (mounted on:0.0156) (near:0.2194) (of:0.5919) (on:0.6993) (on back of:0.0355) (over:0.1132) (painted on:0.0259) (parked on:0.0871) (part of:0.0535) (playing:0.0455) (riding:0.4769) (says:0.0000) (sitting on:0.2498) (standing on:0.1021) (to:0.0164) (under:0.2564) (using:0.1846) (walking in:0.0423) (walking on:0.3194) (watching:0.2419) (wearing:0.9316) (wears:0.1173) (with:0.1869) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0933;    mR @ 50: 0.1465;    mR @ 100: 0.1771;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0599) (across:0.0000) (against:0.0000) (along:0.0129) (and:0.0093) (at:0.1469) (attached to:0.0322) (behind:0.2778) (belonging to:0.0162) (between:0.0046) (carrying:0.3333) (covered in:0.2245) (covering:0.0412) (eating:0.2485) (flying in:0.0000) (for:0.0169) (from:0.0102) (growing on:0.0000) (hanging from:0.0835) (has:0.7332) (holding:0.5856) (in:0.3296) (in front of:0.1630) (laying on:0.1737) (looking at:0.0917) (lying on:0.0471) (made of:0.0270) (mounted on:0.0118) (near:0.2221) (of:0.6498) (on:0.7241) (on back of:0.0337) (over:0.1141) (painted on:0.0196) (parked on:0.0925) (part of:0.0352) (playing:0.0345) (riding:0.5252) (says:0.0000) (sitting on:0.2809) (standing on:0.1214) (to:0.0131) (under:0.2469) (using:0.2036) (walking in:0.0265) (walking on:0.3596) (watching:0.2628) (wearing:0.9323) (wears:0.0998) (with:0.1767) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.1080; ng-mR @ 50: 0.2030; ng-mR @ 100: 0.2993;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.2723) (across:0.0317) (against:0.0081) (along:0.0535) (and:0.0671) (at:0.2422) (attached to:0.1378) (behind:0.4198) (belonging to:0.0783) (between:0.0799) (carrying:0.5273) (covered in:0.2940) (covering:0.1288) (eating:0.3708) (flying in:0.0000) (for:0.1449) (from:0.0141) (growing on:0.0000) (hanging from:0.2491) (has:0.7768) (holding:0.6959) (in:0.4510) (in front of:0.2991) (laying on:0.4553) (looking at:0.2580) (lying on:0.2930) (made of:0.0312) (mounted on:0.0885) (near:0.3973) (of:0.7853) (on:0.8597) (on back of:0.0993) (over:0.2076) (painted on:0.1121) (parked on:0.3305) (part of:0.1317) (playing:0.0265) (riding:0.7854) (says:0.0000) (sitting on:0.4804) (standing on:0.3749) (to:0.1885) (under:0.4393) (using:0.2783) (walking in:0.1338) (walking on:0.5795) (watching:0.3454) (wearing:0.9527) (wears:0.5543) (with:0.4357) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.1158; zs-mR @ 50: 0.1664; zs-mR @ 100: 0.1912;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.1937) (across:0.0000) (against:0.0000) (along:0.0370) (and:0.0312) (at:0.0755) (attached to:0.0705) (behind:0.3472) (belonging to:0.1852) (between:0.0145) (carrying:0.2833) (covered in:0.0833) (covering:0.0000) (eating:0.3088) (flying in:0.0000) (for:0.0125) (from:0.0385) (growing on:0.0000) (hanging from:0.1741) (has:0.5358) (holding:0.3805) (in:0.4268) (in front of:0.2979) (laying on:0.3542) (looking at:0.2833) (lying on:0.1081) (made of:0.0526) (mounted on:0.0600) (near:0.5219) (of:0.3786) (on:0.7452) (on back of:0.1250) (over:0.1511) (painted on:0.0536) (parked on:0.0000) (part of:0.1364) (playing:0.0769) (riding:0.1667) (says:0.0000) (sitting on:0.2877) (standing on:0.2965) (to:0.0263) (under:0.4115) (using:0.0000) (walking in:0.1875) (walking on:0.2647) (watching:0.3878) (wearing:0.2215) (wears:0.2829) (with:0.4849) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.4110;     A @ 50: 0.4114;     A @ 100: 0.4114;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.01', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'False', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'LxyPredictor1', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/LAMBDA/Lxy1-sgcls0.001'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.01
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: LxyPredictor1
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/LAMBDA/Lxy1-sgcls0.001
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.weight of shape (5, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.weight of shape (11, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.weight of shape (20, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.weight of shape (39, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.weight of shape (5, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.weight of shape (11, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.weight of shape (20, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.weight of shape (39, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/LAMBDA/Lxy1-sgcls0.001/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: inf, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: inf, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: inf, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: inf, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: inf, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: inf, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 300741.34375, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 199738.79688, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 155803.46875, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 84581.03906, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 77841.49219, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 74020.74219, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 65817.08594, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: nan, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: nan, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2767657.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2700572.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2520910.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2494174.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2477208.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1795441.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2763944.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 2105079.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2080768.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2520910.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1927483.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1908910.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1743201.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1696459.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : inf, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : inf, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: inf, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: inf, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: inf, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: inf, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: inf, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2763944.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2664510.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2664510.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2664510.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2664510.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2644307.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2627488.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2614049.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2610010.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2044742.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1908510.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1846508.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1752684.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1721383.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1699725.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1665240.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1664139.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1606717.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 248052.67188, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 230321.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 210290.98438, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 175624.53125, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 142828.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 69421.33594, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 69421.33594, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 65811.62500, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 65811.62500, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 62795.73438, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 51053.73438, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 51053.73438, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 49532.54297, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 49.70343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 46.98066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 42.14237, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 41.60014, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: nan, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: nan, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: nan, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 62795.73438, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 5:11:50  iter: 1  loss: 427.4478 (427.4478)  obj_loss: 5.7305 (5.7305)  loss_cal: 0.0260 (0.0260)  rel_ce_loss: 78.1671 (78.1671)  1_CE_loss: 26.9200 (26.9200)  2_CE_loss: 22.4949 (22.4949)  2_DKS_loss: 1.4289 (1.4289)  3_CE_loss: 95.4358 (95.4358)  3_DKS_loss: 2.8453 (2.8453)  4_CE_loss: 107.6127 (107.6127)  4_DKS_loss: 4.3911 (4.3911)  5_CE_loss: 74.3856 (74.3856)  5_DKS_loss: 8.0100 (8.0100)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1695 (1.1695)  data: 0.0979 (0.0979)  lr: 0.001600  max mem: 7921
INFO:maskrcnn_benchmark:eta: 4:47:04  iter: 100  loss: 23.5995 (77.6201)  obj_loss: 4.9922 (5.4586)  loss_cal: 0.0309 (0.0305)  rel_ce_loss: 7.4812 (18.4367)  1_CE_loss: 0.1021 (3.1859)  2_CE_loss: 0.4043 (2.6278)  2_DKS_loss: 0.0002 (0.2840)  3_CE_loss: 0.3822 (9.9024)  3_DKS_loss: 0.0041 (0.6598)  4_CE_loss: 4.4391 (18.2009)  4_DKS_loss: 0.0206 (1.5479)  5_CE_loss: 4.3222 (13.6779)  5_DKS_loss: 1.6114 (3.6076)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0637 (1.0833)  data: 0.1027 (0.1100)  lr: 0.004451  max mem: 9652
INFO:maskrcnn_benchmark:eta: 4:44:22  iter: 200  loss: 17.9778 (50.1507)  obj_loss: 4.3711 (5.0302)  loss_cal: 0.0299 (0.0316)  rel_ce_loss: 5.1240 (12.7391)  1_CE_loss: 0.0660 (1.6496)  2_CE_loss: 0.3003 (1.5734)  2_DKS_loss: 0.0033 (0.1451)  3_CE_loss: 0.3048 (5.1978)  3_DKS_loss: 0.0087 (0.3376)  4_CE_loss: 3.1367 (11.0382)  4_DKS_loss: 0.0351 (0.7997)  5_CE_loss: 2.9054 (9.0424)  5_DKS_loss: 1.4097 (2.5661)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0739 (1.0799)  data: 0.1064 (0.1086)  lr: 0.007331  max mem: 9661
INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.01', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'NPredictor101', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/NPredictor101-predcls-loss'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  

INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.01
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: NPredictor101
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/NPredictor101-predcls-loss
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/NPredictor101-predcls-loss/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/NPredictor101-predcls-loss/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark:eta: 4:42:29  iter: 300  loss: 14.4531 (38.9039)  obj_loss: 3.4434 (4.6474)  loss_cal: 0.0317 (0.0321)  rel_ce_loss: 4.1349 (10.1121)  1_CE_loss: 0.0000 (1.1248)  2_CE_loss: 0.2665 (1.1749)  2_DKS_loss: 0.0021 (0.0992)  3_CE_loss: 0.2598 (3.5903)  3_DKS_loss: 0.0078 (0.2296)  4_CE_loss: 2.7292 (8.2941)  4_DKS_loss: 0.0394 (0.5513)  5_CE_loss: 2.2346 (7.0272)  5_DKS_loss: 0.6495 (2.0210)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0625 (1.0796)  data: 0.0969 (0.1078)  lr: 0.010211  max mem: 9661
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/NPredictor101-predcls-loss/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/NPredictor101-predcls-loss/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1 day, 15:04:53  iter: 1  loss: 2210.8127 (2210.8127)  obj_loss: 0.0000 (0.0000)  loss_fg: 86.3704 (86.3704)  loss_bg: 2124.4424 (2124.4424)  time: 8.7939 (8.7939)  data: 0.1111 (0.1111)  lr: 0.001600  max mem: 7918
INFO:maskrcnn_benchmark:eta: 4:41:38  iter: 400  loss: 9.0467 (31.9086)  obj_loss: 2.2637 (4.1620)  loss_cal: 0.0321 (0.0323)  rel_ce_loss: 2.9925 (8.3984)  1_CE_loss: 0.0428 (0.8655)  2_CE_loss: 0.1315 (0.9529)  2_DKS_loss: 0.0050 (0.0765)  3_CE_loss: 0.2442 (2.7697)  3_DKS_loss: 0.0042 (0.1758)  4_CE_loss: 1.3486 (6.6597)  4_DKS_loss: 0.0226 (0.4256)  5_CE_loss: 1.4001 (5.7418)  5_DKS_loss: 0.3784 (1.6484)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1075 (1.0832)  data: 0.1077 (0.1100)  lr: 0.013091  max mem: 9661
INFO:maskrcnn_benchmark:eta: 5:08:55  iter: 100  loss: 46.6228 (385.1088)  obj_loss: 0.0000 (0.0000)  loss_fg: 37.3432 (64.8765)  loss_bg: 5.6963 (320.2324)  time: 1.0439 (1.1658)  data: 0.1216 (0.1503)  lr: 0.004451  max mem: 9651
INFO:maskrcnn_benchmark:eta: 4:42:16  iter: 500  loss: 8.5510 (27.3484)  obj_loss: 1.6914 (3.7180)  loss_cal: 0.0308 (0.0326)  rel_ce_loss: 2.7664 (7.2810)  1_CE_loss: 0.0285 (0.7061)  2_CE_loss: 0.2086 (0.8125)  2_DKS_loss: 0.0005 (0.0626)  3_CE_loss: 0.2140 (2.2678)  3_DKS_loss: 0.0067 (0.1436)  4_CE_loss: 1.2109 (5.6365)  4_DKS_loss: 0.0228 (0.3489)  5_CE_loss: 1.3644 (4.9377)  5_DKS_loss: 0.3566 (1.4012)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0779 (1.0927)  data: 0.1090 (0.1175)  lr: 0.015971  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:56:07  iter: 200  loss: 32.8480 (221.1807)  obj_loss: 0.0000 (0.0000)  loss_fg: 29.9197 (50.2151)  loss_bg: 4.4370 (170.9656)  time: 1.0501 (1.1246)  data: 0.1008 (0.1414)  lr: 0.007331  max mem: 9651
INFO:maskrcnn_benchmark:eta: 4:41:32  iter: 600  loss: 6.4141 (23.9824)  obj_loss: 1.4893 (3.3613)  loss_cal: 0.0299 (0.0327)  rel_ce_loss: 2.0164 (6.4600)  1_CE_loss: 0.0483 (0.5975)  2_CE_loss: 0.1228 (0.7041)  2_DKS_loss: 0.0008 (0.0531)  3_CE_loss: 0.0919 (1.9200)  3_DKS_loss: 0.0014 (0.1214)  4_CE_loss: 0.9710 (4.8715)  4_DKS_loss: 0.0093 (0.2949)  5_CE_loss: 1.1287 (4.3474)  5_DKS_loss: 0.2290 (1.2185)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0736 (1.0969)  data: 0.0990 (0.1207)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:51:12  iter: 300  loss: 26.1547 (158.6123)  obj_loss: 0.0000 (0.0000)  loss_fg: 21.3431 (41.4149)  loss_bg: 2.0344 (117.1975)  time: 1.0679 (1.1129)  data: 0.1094 (0.1422)  lr: 0.010211  max mem: 9658
INFO:maskrcnn_benchmark:eta: 4:41:16  iter: 700  loss: 5.9688 (21.4379)  obj_loss: 1.2314 (3.0705)  loss_cal: 0.0313 (0.0327)  rel_ce_loss: 1.9280 (5.8284)  1_CE_loss: 0.0078 (0.5186)  2_CE_loss: 0.1196 (0.6236)  2_DKS_loss: 0.0017 (0.0459)  3_CE_loss: 0.1217 (1.6687)  3_DKS_loss: 0.0037 (0.1050)  4_CE_loss: 0.8187 (4.3041)  4_DKS_loss: 0.0160 (0.2555)  5_CE_loss: 1.0180 (3.9035)  5_DKS_loss: 0.2654 (1.0814)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0769 (1.1031)  data: 0.0965 (0.1256)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:47:24  iter: 400  loss: 24.2248 (127.3793)  obj_loss: 0.0000 (0.0000)  loss_fg: 17.4653 (35.8816)  loss_bg: 1.9251 (91.4977)  time: 1.0498 (1.1054)  data: 0.1184 (0.1419)  lr: 0.013091  max mem: 10092
INFO:maskrcnn_benchmark:eta: 4:40:50  iter: 800  loss: 5.7317 (19.4522)  obj_loss: 1.2666 (2.8437)  loss_cal: 0.0318 (0.0327)  rel_ce_loss: 1.7611 (5.3295)  1_CE_loss: 0.0296 (0.4598)  2_CE_loss: 0.1139 (0.5602)  2_DKS_loss: 0.0005 (0.0405)  3_CE_loss: 0.1058 (1.4764)  3_DKS_loss: 0.0012 (0.0926)  4_CE_loss: 0.7587 (3.8667)  4_DKS_loss: 0.0080 (0.2256)  5_CE_loss: 1.0830 (3.5494)  5_DKS_loss: 0.1951 (0.9751)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1278 (1.1086)  data: 0.1259 (0.1302)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:46:06  iter: 500  loss: 22.0157 (113.6232)  obj_loss: 0.0000 (0.0000)  loss_fg: 12.1077 (31.7886)  loss_bg: 3.2650 (81.8346)  time: 1.0412 (1.1075)  data: 0.1015 (0.1476)  lr: 0.015971  max mem: 10092
INFO:maskrcnn_benchmark:eta: 4:40:03  iter: 900  loss: 5.0480 (17.8653)  obj_loss: 1.0967 (2.6580)  loss_cal: 0.0279 (0.0326)  rel_ce_loss: 1.5907 (4.9291)  1_CE_loss: 0.0244 (0.4128)  2_CE_loss: 0.1004 (0.5098)  2_DKS_loss: 0.0013 (0.0362)  3_CE_loss: 0.1336 (1.3262)  3_DKS_loss: 0.0038 (0.0829)  4_CE_loss: 0.6769 (3.5177)  4_DKS_loss: 0.0101 (0.2020)  5_CE_loss: 1.0467 (3.2677)  5_DKS_loss: 0.1913 (0.8904)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0913 (1.1128)  data: 0.1269 (0.1343)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:44:31  iter: 600  loss: 16.6302 (98.5171)  obj_loss: 0.0000 (0.0000)  loss_fg: 12.0162 (28.8117)  loss_bg: 1.4712 (69.7054)  time: 1.0689 (1.1085)  data: 0.1024 (0.1516)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:38:41  iter: 1000  loss: 4.7465 (16.5738)  obj_loss: 1.0615 (2.5053)  loss_cal: 0.0309 (0.0326)  rel_ce_loss: 1.5544 (4.6007)  1_CE_loss: 0.0004 (0.3745)  2_CE_loss: 0.1286 (0.4709)  2_DKS_loss: 0.0002 (0.0327)  3_CE_loss: 0.1174 (1.2068)  3_DKS_loss: 0.0013 (0.0751)  4_CE_loss: 0.7606 (3.2324)  4_DKS_loss: 0.0049 (0.1828)  5_CE_loss: 0.8717 (3.0410)  5_DKS_loss: 0.1512 (0.8189)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0934 (1.1148)  data: 0.1162 (0.1366)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:43:11  iter: 700  loss: 14.2287 (87.1256)  obj_loss: 0.0000 (0.0000)  loss_fg: 9.7724 (26.3374)  loss_bg: 2.3946 (60.7882)  time: 1.0298 (1.1105)  data: 0.0984 (0.1566)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:37:56  iter: 1100  loss: 4.4146 (15.4946)  obj_loss: 1.1250 (2.3791)  loss_cal: 0.0326 (0.0327)  rel_ce_loss: 1.4980 (4.3231)  1_CE_loss: 0.0025 (0.3433)  2_CE_loss: 0.0847 (0.4370)  2_DKS_loss: 0.0009 (0.0299)  3_CE_loss: 0.0776 (1.1066)  3_DKS_loss: 0.0018 (0.0686)  4_CE_loss: 0.6002 (2.9987)  4_DKS_loss: 0.0065 (0.1671)  5_CE_loss: 0.8293 (2.8477)  5_DKS_loss: 0.1748 (0.7608)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1065 (1.1192)  data: 0.1104 (0.1404)  lr: 0.016000  max mem: 9838
INFO:maskrcnn_benchmark:eta: 4:41:32  iter: 800  loss: 13.4868 (78.8603)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.4743 (24.4026)  loss_bg: 1.2508 (54.4577)  time: 1.0565 (1.1113)  data: 0.1010 (0.1576)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:36:58  iter: 1200  loss: 4.3135 (14.5919)  obj_loss: 1.0674 (2.2706)  loss_cal: 0.0290 (0.0327)  rel_ce_loss: 1.4853 (4.0942)  1_CE_loss: 0.0130 (0.3169)  2_CE_loss: 0.0803 (0.4084)  2_DKS_loss: 0.0004 (0.0275)  3_CE_loss: 0.1226 (1.0247)  3_DKS_loss: 0.0020 (0.0632)  4_CE_loss: 0.5409 (2.8024)  4_DKS_loss: 0.0076 (0.1542)  5_CE_loss: 0.8092 (2.6862)  5_DKS_loss: 0.1738 (0.7109)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0991 (1.1229)  data: 0.1086 (0.1430)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:39:24  iter: 900  loss: 17.0842 (73.3622)  obj_loss: 0.0000 (0.0000)  loss_fg: 10.0556 (22.9205)  loss_bg: 1.9721 (50.4417)  time: 1.0413 (1.1103)  data: 0.0951 (0.1585)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:35:38  iter: 1300  loss: 4.2655 (13.8168)  obj_loss: 1.0459 (2.1758)  loss_cal: 0.0294 (0.0327)  rel_ce_loss: 1.4147 (3.8933)  1_CE_loss: 0.0075 (0.2949)  2_CE_loss: 0.0744 (0.3845)  2_DKS_loss: 0.0006 (0.0255)  3_CE_loss: 0.0580 (0.9542)  3_DKS_loss: 0.0019 (0.0587)  4_CE_loss: 0.5551 (2.6356)  4_DKS_loss: 0.0058 (0.1432)  5_CE_loss: 0.8308 (2.5500)  5_DKS_loss: 0.1480 (0.6683)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1045 (1.1251)  data: 0.1069 (0.1451)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:36:48  iter: 1000  loss: 18.2860 (68.5511)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.7524 (21.7237)  loss_bg: 1.8695 (46.8274)  time: 1.0647 (1.1072)  data: 0.1051 (0.1569)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:33:54  iter: 1400  loss: 4.1421 (13.1453)  obj_loss: 1.0293 (2.0915)  loss_cal: 0.0307 (0.0327)  rel_ce_loss: 1.3290 (3.7216)  1_CE_loss: 0.0118 (0.2752)  2_CE_loss: 0.0890 (0.3636)  2_DKS_loss: 0.0003 (0.0237)  3_CE_loss: 0.0774 (0.8938)  3_DKS_loss: 0.0021 (0.0547)  4_CE_loss: 0.5457 (2.4895)  4_DKS_loss: 0.0081 (0.1338)  5_CE_loss: 0.8210 (2.4336)  5_DKS_loss: 0.1264 (0.6315)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0883 (1.1257)  data: 0.1232 (0.1456)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:35:22  iter: 1100  loss: 13.5896 (63.9351)  obj_loss: 0.0000 (0.0000)  loss_fg: 9.7522 (20.6933)  loss_bg: 1.4288 (43.2417)  time: 1.0406 (1.1089)  data: 0.0960 (0.1595)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:33:02  iter: 1500  loss: 4.0728 (12.5475)  obj_loss: 0.9844 (2.0196)  loss_cal: 0.0332 (0.0327)  rel_ce_loss: 1.4271 (3.5649)  1_CE_loss: 0.0128 (0.2581)  2_CE_loss: 0.0641 (0.3448)  2_DKS_loss: 0.0002 (0.0222)  3_CE_loss: 0.0912 (0.8401)  3_DKS_loss: 0.0016 (0.0513)  4_CE_loss: 0.5663 (2.3624)  4_DKS_loss: 0.0081 (0.1255)  5_CE_loss: 0.7203 (2.3271)  5_DKS_loss: 0.1311 (0.5986)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1958 (1.1298)  data: 0.1815 (0.1498)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:33:26  iter: 1200  loss: 12.8270 (59.7391)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.1542 (19.7377)  loss_bg: 1.6050 (40.0015)  time: 1.0579 (1.1086)  data: 0.1113 (0.1598)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:31:12  iter: 1600  loss: 3.7233 (12.0100)  obj_loss: 0.9331 (1.9557)  loss_cal: 0.0329 (0.0327)  rel_ce_loss: 1.2220 (3.4236)  1_CE_loss: 0.0111 (0.2431)  2_CE_loss: 0.0707 (0.3283)  2_DKS_loss: 0.0003 (0.0209)  3_CE_loss: 0.0876 (0.7929)  3_DKS_loss: 0.0024 (0.0482)  4_CE_loss: 0.5115 (2.2475)  4_DKS_loss: 0.0068 (0.1182)  5_CE_loss: 0.7057 (2.2305)  5_DKS_loss: 0.1159 (0.5684)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0993 (1.1300)  data: 0.1281 (0.1497)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:31:32  iter: 1300  loss: 11.4069 (56.1529)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.7359 (18.9134)  loss_bg: 2.0013 (37.2395)  time: 1.0535 (1.1083)  data: 0.1017 (0.1599)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:29:37  iter: 1700  loss: 3.5238 (11.5324)  obj_loss: 0.9316 (1.8984)  loss_cal: 0.0305 (0.0328)  rel_ce_loss: 1.0618 (3.2971)  1_CE_loss: 0.0059 (0.2299)  2_CE_loss: 0.0545 (0.3132)  2_DKS_loss: 0.0003 (0.0197)  3_CE_loss: 0.0668 (0.7508)  3_DKS_loss: 0.0020 (0.0456)  4_CE_loss: 0.5156 (2.1467)  4_DKS_loss: 0.0073 (0.1117)  5_CE_loss: 0.6218 (2.1444)  5_DKS_loss: 0.0816 (0.5421)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0957 (1.1313)  data: 0.1161 (0.1509)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:29:51  iter: 1400  loss: 11.8067 (53.1606)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.2775 (18.2465)  loss_bg: 1.5620 (34.9141)  time: 1.0747 (1.1090)  data: 0.1186 (0.1609)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:28:12  iter: 1800  loss: 3.8615 (11.1035)  obj_loss: 0.9658 (1.8474)  loss_cal: 0.0285 (0.0328)  rel_ce_loss: 1.1935 (3.1829)  1_CE_loss: 0.0027 (0.2181)  2_CE_loss: 0.0599 (0.2997)  2_DKS_loss: 0.0007 (0.0186)  3_CE_loss: 0.0610 (0.7128)  3_DKS_loss: 0.0016 (0.0432)  4_CE_loss: 0.5237 (2.0562)  4_DKS_loss: 0.0056 (0.1060)  5_CE_loss: 0.6915 (2.0676)  5_DKS_loss: 0.1020 (0.5180)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0756 (1.1333)  data: 0.1019 (0.1526)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:27:51  iter: 1500  loss: 11.2054 (50.7816)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.4062 (17.6321)  loss_bg: 1.2171 (33.1495)  time: 1.0408 (1.1084)  data: 0.0937 (0.1608)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:26:31  iter: 1900  loss: 3.5734 (10.7089)  obj_loss: 0.9761 (1.8014)  loss_cal: 0.0320 (0.0328)  rel_ce_loss: 1.1372 (3.0773)  1_CE_loss: 0.0017 (0.2076)  2_CE_loss: 0.0618 (0.2878)  2_DKS_loss: 0.0003 (0.0177)  3_CE_loss: 0.0782 (0.6793)  3_DKS_loss: 0.0027 (0.0410)  4_CE_loss: 0.4487 (1.9725)  4_DKS_loss: 0.0104 (0.1009)  5_CE_loss: 0.6422 (1.9951)  5_DKS_loss: 0.0991 (0.4954)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1026 (1.1341)  data: 0.1110 (0.1534)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:26:07  iter: 1600  loss: 12.1843 (48.3809)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.2581 (17.1007)  loss_bg: 1.6609 (31.2802)  time: 1.0805 (1.1089)  data: 0.1013 (0.1615)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:24:35  iter: 2000  loss: 3.3831 (10.3525)  obj_loss: 0.9614 (1.7605)  loss_cal: 0.0325 (0.0328)  rel_ce_loss: 1.0671 (2.9805)  1_CE_loss: 0.0149 (0.1982)  2_CE_loss: 0.0396 (0.2765)  2_DKS_loss: 0.0002 (0.0168)  3_CE_loss: 0.0559 (0.6487)  3_DKS_loss: 0.0018 (0.0391)  4_CE_loss: 0.4272 (1.8978)  4_DKS_loss: 0.0080 (0.0962)  5_CE_loss: 0.5979 (1.9303)  5_DKS_loss: 0.0778 (0.4752)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0966 (1.1340)  data: 0.1192 (0.1534)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:23:51  iter: 1700  loss: 10.7014 (46.3555)  obj_loss: 0.0000 (0.0000)  loss_fg: 7.6962 (16.5933)  loss_bg: 1.6238 (29.7622)  time: 1.0618 (1.1071)  data: 0.0960 (0.1602)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:22:30  iter: 2100  loss: 3.2981 (10.0259)  obj_loss: 0.9165 (1.7221)  loss_cal: 0.0328 (0.0328)  rel_ce_loss: 1.0465 (2.8919)  1_CE_loss: 0.0086 (0.1897)  2_CE_loss: 0.0417 (0.2663)  2_DKS_loss: 0.0000 (0.0161)  3_CE_loss: 0.0567 (0.6212)  3_DKS_loss: 0.0011 (0.0374)  4_CE_loss: 0.4311 (1.8293)  4_DKS_loss: 0.0034 (0.0921)  5_CE_loss: 0.6583 (1.8703)  5_DKS_loss: 0.0541 (0.4569)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1190 (1.1332)  data: 0.1163 (0.1527)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:22:00  iter: 1800  loss: 11.6726 (44.4818)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.7988 (16.1480)  loss_bg: 1.6414 (28.3338)  time: 1.0809 (1.1071)  data: 0.0998 (0.1598)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:20:41  iter: 2200  loss: 3.1068 (9.7199)  obj_loss: 0.9209 (1.6876)  loss_cal: 0.0318 (0.0329)  rel_ce_loss: 0.9890 (2.8073)  1_CE_loss: 0.0113 (0.1819)  2_CE_loss: 0.0514 (0.2572)  2_DKS_loss: 0.0003 (0.0153)  3_CE_loss: 0.0465 (0.5960)  3_DKS_loss: 0.0023 (0.0358)  4_CE_loss: 0.4110 (1.7657)  4_DKS_loss: 0.0100 (0.0883)  5_CE_loss: 0.5765 (1.8128)  5_DKS_loss: 0.0662 (0.4391)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1115 (1.1335)  data: 0.1145 (0.1532)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:20:00  iter: 1900  loss: 10.6968 (42.7696)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.2660 (15.7381)  loss_bg: 1.3703 (27.0315)  time: 1.0464 (1.1064)  data: 0.0955 (0.1591)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:19:14  iter: 2300  loss: 3.2108 (9.4384)  obj_loss: 1.0176 (1.6561)  loss_cal: 0.0319 (0.0329)  rel_ce_loss: 0.9553 (2.7291)  1_CE_loss: 0.0078 (0.1745)  2_CE_loss: 0.0329 (0.2486)  2_DKS_loss: 0.0002 (0.0147)  3_CE_loss: 0.0391 (0.5727)  3_DKS_loss: 0.0016 (0.0343)  4_CE_loss: 0.3531 (1.7073)  4_DKS_loss: 0.0075 (0.0848)  5_CE_loss: 0.5423 (1.7597)  5_DKS_loss: 0.0543 (0.4235)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.2345 (1.1354)  data: 0.2757 (0.1550)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:18:30  iter: 2000  loss: 10.5544 (41.2998)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.7185 (15.3861)  loss_bg: 1.3034 (25.9138)  time: 1.0552 (1.1079)  data: 0.1000 (0.1607)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:17:39  iter: 2400  loss: 3.0003 (9.1752)  obj_loss: 0.9463 (1.6273)  loss_cal: 0.0323 (0.0329)  rel_ce_loss: 0.8761 (2.6552)  1_CE_loss: 0.0049 (0.1680)  2_CE_loss: 0.0434 (0.2409)  2_DKS_loss: 0.0002 (0.0141)  3_CE_loss: 0.0599 (0.5514)  3_DKS_loss: 0.0025 (0.0330)  4_CE_loss: 0.4043 (1.6531)  4_DKS_loss: 0.0090 (0.0816)  5_CE_loss: 0.4796 (1.7096)  5_DKS_loss: 0.0474 (0.4082)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1131 (1.1367)  data: 0.1169 (0.1560)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:16:46  iter: 2100  loss: 10.5444 (39.8892)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.8966 (15.0176)  loss_bg: 1.8987 (24.8716)  time: 1.0724 (1.1084)  data: 0.1013 (0.1615)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:15:02  iter: 2200  loss: 10.6892 (38.7246)  obj_loss: 0.0000 (0.0000)  loss_fg: 8.1171 (14.7248)  loss_bg: 2.0654 (23.9998)  time: 1.0705 (1.1089)  data: 0.1221 (0.1622)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:15:57  iter: 2500  loss: 3.3032 (8.9469)  obj_loss: 0.9375 (1.6005)  loss_cal: 0.0311 (0.0329)  rel_ce_loss: 1.0119 (2.5932)  1_CE_loss: 0.0050 (0.1620)  2_CE_loss: 0.0512 (0.2336)  2_DKS_loss: 0.0004 (0.0136)  3_CE_loss: 0.0419 (0.5316)  3_DKS_loss: 0.0032 (0.0318)  4_CE_loss: 0.3886 (1.6040)  4_DKS_loss: 0.0115 (0.0787)  5_CE_loss: 0.5682 (1.6686)  5_DKS_loss: 0.1210 (0.3964)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1266 (1.1376)  data: 0.1130 (0.1566)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:13:04  iter: 2300  loss: 9.0427 (37.5699)  obj_loss: 0.0000 (0.0000)  loss_fg: 7.0491 (14.4191)  loss_bg: 1.3737 (23.1508)  time: 1.0518 (1.1084)  data: 0.1056 (0.1617)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:14:02  iter: 2600  loss: 2.9682 (8.7285)  obj_loss: 0.9004 (1.5740)  loss_cal: 0.0317 (0.0329)  rel_ce_loss: 0.9229 (2.5312)  1_CE_loss: 0.0064 (0.1565)  2_CE_loss: 0.0286 (0.2268)  2_DKS_loss: 0.0001 (0.0131)  3_CE_loss: 0.0328 (0.5133)  3_DKS_loss: 0.0015 (0.0307)  4_CE_loss: 0.3637 (1.5607)  4_DKS_loss: 0.0059 (0.0761)  5_CE_loss: 0.5023 (1.6276)  5_DKS_loss: 0.0676 (0.3856)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0962 (1.1375)  data: 0.1064 (0.1565)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:11:05  iter: 2400  loss: 9.8330 (36.4811)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.5331 (14.1399)  loss_bg: 1.4030 (22.3412)  time: 1.0508 (1.1078)  data: 0.0957 (0.1611)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:12:10  iter: 2700  loss: 3.1650 (8.5213)  obj_loss: 0.9312 (1.5506)  loss_cal: 0.0328 (0.0329)  rel_ce_loss: 1.0300 (2.4729)  1_CE_loss: 0.0093 (0.1513)  2_CE_loss: 0.0531 (0.2205)  2_DKS_loss: 0.0002 (0.0126)  3_CE_loss: 0.0415 (0.4963)  3_DKS_loss: 0.0021 (0.0296)  4_CE_loss: 0.3872 (1.5181)  4_DKS_loss: 0.0066 (0.0736)  5_CE_loss: 0.5023 (1.5893)  5_DKS_loss: 0.0506 (0.3737)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0869 (1.1376)  data: 0.1057 (0.1565)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:09:12  iter: 2500  loss: 11.0074 (35.4705)  obj_loss: 0.0000 (0.0000)  loss_fg: 7.7023 (13.8757)  loss_bg: 1.9298 (21.5949)  time: 1.0428 (1.1076)  data: 0.0967 (0.1612)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:10:22  iter: 2800  loss: 3.0868 (8.3268)  obj_loss: 0.9893 (1.5300)  loss_cal: 0.0320 (0.0330)  rel_ce_loss: 0.9152 (2.4179)  1_CE_loss: 0.0072 (0.1464)  2_CE_loss: 0.0549 (0.2146)  2_DKS_loss: 0.0003 (0.0122)  3_CE_loss: 0.0403 (0.4802)  3_DKS_loss: 0.0018 (0.0286)  4_CE_loss: 0.3724 (1.4779)  4_DKS_loss: 0.0070 (0.0713)  5_CE_loss: 0.5242 (1.5528)  5_DKS_loss: 0.0440 (0.3620)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0871 (1.1380)  data: 0.0980 (0.1568)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:07:07  iter: 2600  loss: 9.1034 (34.5179)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.6260 (13.6252)  loss_bg: 1.2778 (20.8927)  time: 1.0608 (1.1066)  data: 0.0998 (0.1603)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:08:21  iter: 2900  loss: 2.8245 (8.1429)  obj_loss: 0.9272 (1.5101)  loss_cal: 0.0324 (0.0330)  rel_ce_loss: 0.8544 (2.3659)  1_CE_loss: 0.0047 (0.1419)  2_CE_loss: 0.0410 (0.2093)  2_DKS_loss: 0.0002 (0.0117)  3_CE_loss: 0.0404 (0.4654)  3_DKS_loss: 0.0023 (0.0277)  4_CE_loss: 0.3551 (1.4396)  4_DKS_loss: 0.0071 (0.0691)  5_CE_loss: 0.4888 (1.5180)  5_DKS_loss: 0.0342 (0.3512)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0962 (1.1375)  data: 0.0971 (0.1563)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:05:04  iter: 2700  loss: 10.1136 (33.6184)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.7174 (13.3821)  loss_bg: 1.0382 (20.2363)  time: 1.0472 (1.1056)  data: 0.1169 (0.1595)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:06:26  iter: 3000  loss: 2.7567 (7.9710)  obj_loss: 0.8970 (1.4907)  loss_cal: 0.0333 (0.0330)  rel_ce_loss: 0.7935 (2.3162)  1_CE_loss: 0.0021 (0.1376)  2_CE_loss: 0.0315 (0.2040)  2_DKS_loss: 0.0002 (0.0114)  3_CE_loss: 0.0266 (0.4515)  3_DKS_loss: 0.0015 (0.0269)  4_CE_loss: 0.3412 (1.4065)  4_DKS_loss: 0.0055 (0.0670)  5_CE_loss: 0.4526 (1.4849)  5_DKS_loss: 0.0409 (0.3415)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1184 (1.1374)  data: 0.1042 (0.1562)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:03:08  iter: 2800  loss: 9.3363 (32.7915)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.5457 (13.1519)  loss_bg: 1.5229 (19.6396)  time: 1.0687 (1.1052)  data: 0.0999 (0.1591)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:04:18  iter: 3100  loss: 2.9098 (7.8396)  obj_loss: 0.8232 (1.4715)  loss_cal: 0.0323 (0.0331)  rel_ce_loss: 0.8946 (2.2824)  1_CE_loss: 0.0071 (0.1336)  2_CE_loss: 0.0433 (0.1994)  2_DKS_loss: 0.0003 (0.0110)  3_CE_loss: 0.0285 (0.4388)  3_DKS_loss: 0.0027 (0.0262)  4_CE_loss: 0.3576 (1.3787)  4_DKS_loss: 0.0073 (0.0652)  5_CE_loss: 0.5685 (1.4655)  5_DKS_loss: 0.0695 (0.3342)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0916 (1.1363)  data: 0.0987 (0.1552)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 4:01:05  iter: 2900  loss: 9.0222 (32.0407)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.2894 (12.9433)  loss_bg: 1.4594 (19.0974)  time: 1.0371 (1.1042)  data: 0.0942 (0.1584)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:02:17  iter: 3200  loss: 2.8111 (7.6865)  obj_loss: 0.9150 (1.4541)  loss_cal: 0.0324 (0.0331)  rel_ce_loss: 0.8291 (2.2390)  1_CE_loss: 0.0026 (0.1299)  2_CE_loss: 0.0457 (0.1950)  2_DKS_loss: 0.0005 (0.0107)  3_CE_loss: 0.0393 (0.4267)  3_DKS_loss: 0.0025 (0.0255)  4_CE_loss: 0.4306 (1.3477)  4_DKS_loss: 0.0071 (0.0635)  5_CE_loss: 0.4685 (1.4357)  5_DKS_loss: 0.0387 (0.3258)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0751 (1.1358)  data: 0.1009 (0.1548)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:59:02  iter: 3000  loss: 8.8086 (31.3209)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.9496 (12.7405)  loss_bg: 0.9168 (18.5804)  time: 1.0376 (1.1033)  data: 0.1018 (0.1579)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 4:00:24  iter: 3300  loss: 2.7212 (7.5496)  obj_loss: 0.8691 (1.4379)  loss_cal: 0.0322 (0.0331)  rel_ce_loss: 0.7997 (2.2016)  1_CE_loss: 0.0077 (0.1263)  2_CE_loss: 0.0592 (0.1907)  2_DKS_loss: 0.0003 (0.0104)  3_CE_loss: 0.0427 (0.4152)  3_DKS_loss: 0.0020 (0.0248)  4_CE_loss: 0.3761 (1.3187)  4_DKS_loss: 0.0065 (0.0618)  5_CE_loss: 0.4353 (1.4111)  5_DKS_loss: 0.0417 (0.3181)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0923 (1.1358)  data: 0.1018 (0.1548)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:57:07  iter: 3100  loss: 8.7835 (30.5923)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.2722 (12.5309)  loss_bg: 0.8823 (18.0614)  time: 1.0481 (1.1029)  data: 0.0975 (0.1577)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:58:28  iter: 3400  loss: 3.3973 (7.4632)  obj_loss: 0.8647 (1.4222)  loss_cal: 0.0324 (0.0333)  rel_ce_loss: 1.0583 (2.1848)  1_CE_loss: 0.0068 (0.1231)  2_CE_loss: 0.0423 (0.1867)  2_DKS_loss: 0.0002 (0.0101)  3_CE_loss: 0.0486 (0.4047)  3_DKS_loss: 0.0020 (0.0241)  4_CE_loss: 0.5133 (1.2984)  4_DKS_loss: 0.0069 (0.0602)  5_CE_loss: 0.6724 (1.4044)  5_DKS_loss: 0.0580 (0.3112)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0532 (1.1356)  data: 0.0994 (0.1547)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:55:02  iter: 3200  loss: 8.8826 (29.9119)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.3085 (12.3347)  loss_bg: 1.3660 (17.5772)  time: 1.0447 (1.1017)  data: 0.0980 (0.1566)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:56:27  iter: 3500  loss: 2.8017 (7.3594)  obj_loss: 0.8657 (1.4063)  loss_cal: 0.0315 (0.0334)  rel_ce_loss: 0.7935 (2.1599)  1_CE_loss: 0.0077 (0.1200)  2_CE_loss: 0.0418 (0.1828)  2_DKS_loss: 0.0001 (0.0098)  3_CE_loss: 0.0357 (0.3945)  3_DKS_loss: 0.0013 (0.0235)  4_CE_loss: 0.3480 (1.2741)  4_DKS_loss: 0.0052 (0.0587)  5_CE_loss: 0.4717 (1.3916)  5_DKS_loss: 0.0451 (0.3048)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0835 (1.1350)  data: 0.0943 (0.1541)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:53:02  iter: 3300  loss: 8.7693 (29.2632)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.6811 (12.1482)  loss_bg: 1.5887 (17.1150)  time: 1.0465 (1.1010)  data: 0.0928 (0.1561)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:54:29  iter: 3600  loss: 2.8485 (7.2383)  obj_loss: 0.8516 (1.3921)  loss_cal: 0.0335 (0.0334)  rel_ce_loss: 0.8631 (2.1252)  1_CE_loss: 0.0080 (0.1171)  2_CE_loss: 0.0366 (0.1791)  2_DKS_loss: 0.0003 (0.0096)  3_CE_loss: 0.0365 (0.3848)  3_DKS_loss: 0.0013 (0.0229)  4_CE_loss: 0.3831 (1.2500)  4_DKS_loss: 0.0057 (0.0573)  5_CE_loss: 0.4525 (1.3682)  5_DKS_loss: 0.0269 (0.2985)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0947 (1.1346)  data: 0.0952 (0.1537)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:50:56  iter: 3400  loss: 9.1233 (28.6924)  obj_loss: 0.0000 (0.0000)  loss_fg: 6.7874 (11.9716)  loss_bg: 1.2480 (16.7208)  time: 1.0335 (1.0997)  data: 0.0885 (0.1550)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:52:20  iter: 3700  loss: 2.6622 (7.1209)  obj_loss: 0.7744 (1.3781)  loss_cal: 0.0328 (0.0335)  rel_ce_loss: 0.8096 (2.0916)  1_CE_loss: 0.0097 (0.1143)  2_CE_loss: 0.0297 (0.1757)  2_DKS_loss: 0.0002 (0.0093)  3_CE_loss: 0.0294 (0.3758)  3_DKS_loss: 0.0015 (0.0223)  4_CE_loss: 0.3813 (1.2271)  4_DKS_loss: 0.0054 (0.0559)  5_CE_loss: 0.4771 (1.3457)  5_DKS_loss: 0.0311 (0.2915)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0691 (1.1333)  data: 0.0906 (0.1527)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:48:54  iter: 3500  loss: 7.9583 (28.1034)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.5224 (11.7962)  loss_bg: 0.9547 (16.3072)  time: 1.0299 (1.0988)  data: 0.0905 (0.1543)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:50:23  iter: 3800  loss: 2.7376 (7.0049)  obj_loss: 0.8701 (1.3646)  loss_cal: 0.0320 (0.0335)  rel_ce_loss: 0.7825 (2.0584)  1_CE_loss: 0.0100 (0.1116)  2_CE_loss: 0.0262 (0.1723)  2_DKS_loss: 0.0001 (0.0091)  3_CE_loss: 0.0447 (0.3672)  3_DKS_loss: 0.0015 (0.0218)  4_CE_loss: 0.3495 (1.2042)  4_DKS_loss: 0.0067 (0.0547)  5_CE_loss: 0.4453 (1.3229)  5_DKS_loss: 0.0321 (0.2847)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.1331)  data: 0.0891 (0.1524)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:46:56  iter: 3600  loss: 7.3992 (27.5394)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.4217 (11.6253)  loss_bg: 1.3742 (15.9140)  time: 1.0298 (1.0981)  data: 0.0894 (0.1538)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:48:19  iter: 3900  loss: 2.6091 (6.8988)  obj_loss: 0.8750 (1.3515)  loss_cal: 0.0329 (0.0335)  rel_ce_loss: 0.7473 (2.0278)  1_CE_loss: 0.0030 (0.1091)  2_CE_loss: 0.0486 (0.1691)  2_DKS_loss: 0.0001 (0.0088)  3_CE_loss: 0.0485 (0.3590)  3_DKS_loss: 0.0013 (0.0213)  4_CE_loss: 0.3530 (1.1841)  4_DKS_loss: 0.0042 (0.0534)  5_CE_loss: 0.4289 (1.3029)  5_DKS_loss: 0.0259 (0.2786)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0456 (1.1322)  data: 0.0880 (0.1516)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:44:56  iter: 3700  loss: 7.2447 (27.0215)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.2593 (11.4632)  loss_bg: 1.0197 (15.5584)  time: 1.0544 (1.0973)  data: 0.0882 (0.1531)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:---Total norm 15.41894 clip coef 0.32428-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 7.68283, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 7.47249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.98779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.68557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 2.46522, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 2.18855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 2.11524, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 2.00532, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.96932, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.94603, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.90109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.86082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.79680, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.55744, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 1.51092, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 1.45437, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 1.38820, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.25287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.25287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.25287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.25287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 1.12662, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.08803, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.97412, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.96523, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.90234, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.86578, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.86204, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.75918, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.70451, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.60261, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.58789, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.58675, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.57235, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.55501, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.54976, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.54955, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.54611, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.53102, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.52265, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.52029, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.51284, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.50955, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.50606, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.50426, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.50301, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.50165, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.50029, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.49652, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.49389, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.49347, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.49278, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.48990, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.48948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.48201, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.48135, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.47372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.46345, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44662, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.43937, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.42846, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.42308, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.40773, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.40248, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.40191, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.40130, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.40032, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.39775, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.39142, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.39139, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.39084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.38909, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.38899, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.38473, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.37956, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.37790, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.37275, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.36751, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.36697, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.35193, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.34981, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.34639, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.34047, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33637, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.33580, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.33471, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.33471, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.32988, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.32972, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.32843, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.32485, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.32257, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.31994, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.31967, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.31223, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29514, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.28994, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.28281, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.28243, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.28078, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27643, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.27098, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26505, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.26300, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25865, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25717, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25663, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25622, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25458, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25335, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.24946, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.24586, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.24473, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.24370, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.24289, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.24244, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23508, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23341, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23335, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22792, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.22185, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.21703, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.21337, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.20352, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20041, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19997, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19954, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.19473, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19429, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19404, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18824, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.18767, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18400, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18267, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17565, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17205, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.16996, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16753, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.16684, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16605, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.16295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.15440, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15237, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15169, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15033, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14976, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14928, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14904, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.14761, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14631, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14612, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14585, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14372, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.14339, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13783, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.13748, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13362, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.13025, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12614, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12509, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12000, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11808, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11797, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.10984, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10959, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10867, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10666, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10659, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.09434, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.09082, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.09028, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.08835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08784, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08726, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.08407, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08384, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.08268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.07405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07309, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07091, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06986, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.06901, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06741, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.06246, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.06246, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.06147, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06002, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05940, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.05725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05631, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05525, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05498, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.05459, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05167, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05099, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05036, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04871, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04668, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04661, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.04563, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04468, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04247, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.04116, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04094, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03491, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03489, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.03487, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03435, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03435, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03365, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03355, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03327, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03324, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03311, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03302, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03279, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03266, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03248, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03208, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03208, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03190, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03181, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.03126, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.03126, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03102, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03023, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03007, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02990, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02980, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02964, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02925, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02906, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02845, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02819, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.02804, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02783, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02763, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02735, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02724, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.02612, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02582, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02496, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02488, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02460, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.02453, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02436, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02404, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02396, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02301, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02228, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02208, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02190, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02183, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02166, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02113, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02059, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02029, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01986, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01981, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01947, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01943, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01792, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01780, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01694, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01652, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01649, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.01648, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01602, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01588, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01550, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01515, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01500, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01404, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01342, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01334, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01320, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01318, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01313, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.01311, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.01281, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01258, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01252, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01248, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01233, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01214, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01210, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.01206, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01200, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01197, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01164, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01158, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01151, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01090, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01070, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01069, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01044, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01017, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01009, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01002, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01000, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00992, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00884, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00849, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00834, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00762, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00758, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00728, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00719, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00713, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00679, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00679, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00676, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00660, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00582, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00548, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00524, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00510, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00506, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00505, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00497, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00478, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00443, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00317, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00301, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00212, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00199, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00161, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00124, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00124, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:46:23  iter: 4000  loss: 2.4725 (6.7937)  obj_loss: 0.8560 (1.3397)  loss_cal: 0.0304 (0.0335)  rel_ce_loss: 0.7030 (1.9973)  1_CE_loss: 0.0136 (0.1067)  2_CE_loss: 0.0366 (0.1660)  2_DKS_loss: 0.0003 (0.0086)  3_CE_loss: 0.0332 (0.3510)  3_DKS_loss: 0.0014 (0.0208)  4_CE_loss: 0.3338 (1.1633)  4_DKS_loss: 0.0044 (0.0522)  5_CE_loss: 0.4158 (1.2824)  5_DKS_loss: 0.0184 (0.2725)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0733 (1.1320)  data: 0.0891 (0.1514)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:43:00  iter: 3800  loss: 7.3333 (26.5048)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.2183 (11.3046)  loss_bg: 0.9404 (15.2002)  time: 1.0244 (1.0968)  data: 0.0874 (0.1528)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:44:23  iter: 4100  loss: 2.4880 (6.6996)  obj_loss: 0.8320 (1.3281)  loss_cal: 0.0363 (0.0336)  rel_ce_loss: 0.7240 (1.9702)  1_CE_loss: 0.0073 (0.1045)  2_CE_loss: 0.0334 (0.1630)  2_DKS_loss: 0.0005 (0.0084)  3_CE_loss: 0.0343 (0.3435)  3_DKS_loss: 0.0018 (0.0203)  4_CE_loss: 0.3350 (1.1448)  4_DKS_loss: 0.0062 (0.0512)  5_CE_loss: 0.3734 (1.2645)  5_DKS_loss: 0.0304 (0.2679)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0837 (1.1314)  data: 0.0895 (0.1507)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:41:11  iter: 3900  loss: 7.5079 (26.0164)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.0379 (11.1531)  loss_bg: 1.2766 (14.8633)  time: 1.0643 (1.0968)  data: 0.0894 (0.1529)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:42:32  iter: 4200  loss: 2.7669 (6.6089)  obj_loss: 0.8506 (1.3173)  loss_cal: 0.0311 (0.0336)  rel_ce_loss: 0.8074 (1.9442)  1_CE_loss: 0.0019 (0.1023)  2_CE_loss: 0.0336 (0.1603)  2_DKS_loss: 0.0002 (0.0082)  3_CE_loss: 0.0384 (0.3364)  3_DKS_loss: 0.0017 (0.0199)  4_CE_loss: 0.3472 (1.1269)  4_DKS_loss: 0.0033 (0.0501)  5_CE_loss: 0.4758 (1.2474)  5_DKS_loss: 0.0440 (0.2626)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0741 (1.1316)  data: 0.0891 (0.1509)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:---Total norm 23.06051 clip coef 0.21682-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : 13.73447, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 6.27957, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 4.38367, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 4.21242, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 4.20828, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 4.03215, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 3.92931, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 3.75004, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 3.70608, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 3.57583, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 3.26497, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 3.10645, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 3.08297, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 3.06362, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 2.87283, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : 2.86745, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 2.66476, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 2.56133, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 2.39234, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 2.24682, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 2.13450, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2.11402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2.11402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2.11402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2.11402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 2.11034, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.99454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 1.95954, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.87413, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.85303, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.84666, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 1.84530, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 1.83816, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.78176, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.76384, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 1.74715, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.71583, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.62527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 1.56182, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.24087, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.11208, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.06621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.04547, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 1.00898, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 1.00137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.98272, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.96868, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.94400, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.94104, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.93997, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.92276, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.92157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.91083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.78308, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.73601, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.73057, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.69003, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.68653, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.66715, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.60983, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.57577, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.54338, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.53157, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.52457, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.47867, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.47744, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.47686, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.46863, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.46312, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.46228, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.45855, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.45661, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.45507, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.44263, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.44151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.43829, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.43123, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.43068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.42588, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.42237, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.41724, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.41531, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.41476, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.41378, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.37388, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : 0.37027, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : 0.37027, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.34841, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.34064, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.32293, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.32087, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.31143, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.30844, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.30826, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.30161, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.30153, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29746, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29582, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29300, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29278, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29039, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29002, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.28763, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.28536, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27841, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27836, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27691, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27614, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27542, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27409, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27201, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26820, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26582, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25679, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.24695, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23948, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23270, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22750, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.22436, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22281, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22275, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22163, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.21927, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21845, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.21736, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21624, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21607, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20788, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19681, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18067, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.16663, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.16663, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.16529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16336, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15944, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15767, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15600, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.15548, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15434, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.15285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14898, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14808, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.14571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14470, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14367, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14163, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14123, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13929, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13925, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.13861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.13849, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.13770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.13770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13589, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13437, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13419, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.13384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13229, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13177, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12994, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12725, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.12557, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.11938, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.11836, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11616, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11568, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.11457, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.11413, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11305, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11151, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.11047, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10442, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10168, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09741, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09734, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09466, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09383, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.09376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.09231, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08583, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08434, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07824, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.07663, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07653, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.07556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.07516, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07474, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.07379, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07378, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.07292, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07265, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.07196, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07152, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.07151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06895, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06885, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06668, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06659, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06570, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06553, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06549, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06305, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.06299, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06174, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06048, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.06035, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.05986, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05826, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05741, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05709, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05708, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.05524, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.05478, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.05478, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.05363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.04834, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04803, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.04781, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04742, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04226, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04192, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04154, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03642, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03642, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03642, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03642, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.03641, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.03217, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03216, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03185, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03152, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03107, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03101, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03090, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.03046, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03010, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02997, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.02920, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.02915, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02787, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02747, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02497, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02308, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02308, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02225, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02195, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02194, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02176, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02174, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02145, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02128, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02107, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02053, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02018, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02005, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01980, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01970, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01944, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01929, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01913, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01913, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01908, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01907, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01896, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01884, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01880, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01876, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01867, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01847, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01846, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01846, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01817, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01771, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01754, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01738, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01738, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01738, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01736, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01730, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01698, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01698, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01695, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01637, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01634, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01622, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01553, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01469, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01396, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01165, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01138, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01136, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01110, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01052, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00961, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00909, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00889, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00869, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00866, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00851, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00847, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00802, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00783, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00780, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00776, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00775, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00764, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00760, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00754, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00749, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00743, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00734, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00733, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00726, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00721, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00717, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00691, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00687, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00676, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00674, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00670, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00666, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00650, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00612, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00601, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00582, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00571, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00432, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00354, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00278, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00276, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00220, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00195, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00194, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00189, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00168, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00118, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00092, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00055, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:39:09  iter: 4000  loss: 6.3560 (25.5667)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.8499 (11.0079)  loss_bg: 1.1138 (14.5588)  time: 1.0554 (1.0958)  data: 0.0890 (0.1520)  lr: 0.016000  max mem: 10494
INFO:maskrcnn_benchmark:eta: 3:40:26  iter: 4300  loss: 2.5603 (6.5189)  obj_loss: 0.8022 (1.3065)  loss_cal: 0.0310 (0.0336)  rel_ce_loss: 0.7802 (1.9182)  1_CE_loss: 0.0108 (0.1003)  2_CE_loss: 0.0341 (0.1576)  2_DKS_loss: 0.0003 (0.0081)  3_CE_loss: 0.0300 (0.3296)  3_DKS_loss: 0.0019 (0.0195)  4_CE_loss: 0.3807 (1.1094)  4_DKS_loss: 0.0057 (0.0490)  5_CE_loss: 0.4694 (1.2300)  5_DKS_loss: 0.0273 (0.2575)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0648 (1.1304)  data: 0.0875 (0.1501)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:37:14  iter: 4100  loss: 6.7173 (25.1413)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.8729 (10.8665)  loss_bg: 1.1556 (14.2748)  time: 1.0510 (1.0953)  data: 0.0883 (0.1512)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:38:27  iter: 4400  loss: 2.4992 (6.4313)  obj_loss: 0.9160 (1.2964)  loss_cal: 0.0336 (0.0336)  rel_ce_loss: 0.7754 (1.8931)  1_CE_loss: 0.0024 (0.0983)  2_CE_loss: 0.0358 (0.1552)  2_DKS_loss: 0.0004 (0.0079)  3_CE_loss: 0.0373 (0.3232)  3_DKS_loss: 0.0019 (0.0191)  4_CE_loss: 0.3166 (1.0918)  4_DKS_loss: 0.0045 (0.0481)  5_CE_loss: 0.4168 (1.2126)  5_DKS_loss: 0.0222 (0.2524)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0633 (1.1300)  data: 0.0891 (0.1497)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:35:20  iter: 4200  loss: 6.8897 (24.7203)  obj_loss: 0.0000 (0.0000)  loss_fg: 5.3158 (10.7311)  loss_bg: 1.0619 (13.9892)  time: 1.0452 (1.0950)  data: 0.0879 (0.1509)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:36:32  iter: 4500  loss: 2.7380 (6.3472)  obj_loss: 0.8901 (1.2876)  loss_cal: 0.0328 (0.0336)  rel_ce_loss: 0.8135 (1.8684)  1_CE_loss: 0.0102 (0.0964)  2_CE_loss: 0.0523 (0.1528)  2_DKS_loss: 0.0004 (0.0077)  3_CE_loss: 0.0390 (0.3170)  3_DKS_loss: 0.0021 (0.0187)  4_CE_loss: 0.3468 (1.0750)  4_DKS_loss: 0.0053 (0.0471)  5_CE_loss: 0.4518 (1.1958)  5_DKS_loss: 0.0253 (0.2474)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0683 (1.1297)  data: 0.0920 (0.1493)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:33:21  iter: 4300  loss: 6.7174 (24.3057)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6966 (10.5968)  loss_bg: 0.7510 (13.7090)  time: 1.0355 (1.0942)  data: 0.0908 (0.1502)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:34:28  iter: 4600  loss: 2.6094 (6.2789)  obj_loss: 0.7798 (1.2781)  loss_cal: 0.0336 (0.0336)  rel_ce_loss: 0.7442 (1.8497)  1_CE_loss: 0.0174 (0.0947)  2_CE_loss: 0.0456 (0.1505)  2_DKS_loss: 0.0002 (0.0076)  3_CE_loss: 0.0382 (0.3110)  3_DKS_loss: 0.0017 (0.0184)  4_CE_loss: 0.3425 (1.0604)  4_DKS_loss: 0.0033 (0.0462)  5_CE_loss: 0.4253 (1.1845)  5_DKS_loss: 0.0361 (0.2443)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0671 (1.1288)  data: 0.0864 (0.1485)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:31:25  iter: 4400  loss: 6.8847 (23.9213)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6146 (10.4709)  loss_bg: 1.2586 (13.4504)  time: 1.0512 (1.0936)  data: 0.0921 (0.1496)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:32:25  iter: 4700  loss: 2.4502 (6.2142)  obj_loss: 0.8477 (1.2693)  loss_cal: 0.0348 (0.0337)  rel_ce_loss: 0.7484 (1.8328)  1_CE_loss: 0.0065 (0.0929)  2_CE_loss: 0.0384 (0.1483)  2_DKS_loss: 0.0002 (0.0074)  3_CE_loss: 0.0353 (0.3054)  3_DKS_loss: 0.0017 (0.0181)  4_CE_loss: 0.3221 (1.0458)  4_DKS_loss: 0.0045 (0.0454)  5_CE_loss: 0.3941 (1.1744)  5_DKS_loss: 0.0171 (0.2409)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0624 (1.1279)  data: 0.0881 (0.1477)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:29:31  iter: 4500  loss: 9.4239 (23.5689)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1286 (10.3511)  loss_bg: 2.9803 (13.2178)  time: 1.0758 (1.0931)  data: 0.0926 (0.1490)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:30:26  iter: 4800  loss: 2.4327 (6.1381)  obj_loss: 0.7500 (1.2605)  loss_cal: 0.0330 (0.0337)  rel_ce_loss: 0.7234 (1.8104)  1_CE_loss: 0.0125 (0.0913)  2_CE_loss: 0.0379 (0.1461)  2_DKS_loss: 0.0004 (0.0073)  3_CE_loss: 0.0334 (0.2999)  3_DKS_loss: 0.0022 (0.0177)  4_CE_loss: 0.3717 (1.0311)  4_DKS_loss: 0.0056 (0.0446)  5_CE_loss: 0.4355 (1.1592)  5_DKS_loss: 0.0349 (0.2365)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0368 (1.1274)  data: 0.0861 (0.1473)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:27:35  iter: 4600  loss: 6.2539 (23.2027)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.8451 (10.2377)  loss_bg: 0.9849 (12.9650)  time: 1.0434 (1.0926)  data: 0.0871 (0.1485)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:28:28  iter: 4900  loss: 2.8220 (6.0671)  obj_loss: 0.9297 (1.2528)  loss_cal: 0.0338 (0.0337)  rel_ce_loss: 0.7904 (1.7895)  1_CE_loss: 0.0117 (0.0897)  2_CE_loss: 0.0510 (0.1441)  2_DKS_loss: 0.0001 (0.0071)  3_CE_loss: 0.0456 (0.2948)  3_DKS_loss: 0.0024 (0.0174)  4_CE_loss: 0.3346 (1.0170)  4_DKS_loss: 0.0043 (0.0438)  5_CE_loss: 0.5132 (1.1451)  5_DKS_loss: 0.0753 (0.2324)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0672 (1.1269)  data: 0.0894 (0.1470)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:25:41  iter: 4700  loss: 7.6617 (22.8932)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5757 (10.1287)  loss_bg: 0.7774 (12.7646)  time: 1.0449 (1.0921)  data: 0.0910 (0.1481)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:26:30  iter: 5000  loss: 2.6263 (5.9999)  obj_loss: 0.8521 (1.2454)  loss_cal: 0.0324 (0.0337)  rel_ce_loss: 0.7845 (1.7700)  1_CE_loss: 0.0092 (0.0882)  2_CE_loss: 0.0374 (0.1422)  2_DKS_loss: 0.0005 (0.0070)  3_CE_loss: 0.0322 (0.2897)  3_DKS_loss: 0.0016 (0.0171)  4_CE_loss: 0.3183 (1.0034)  4_DKS_loss: 0.0058 (0.0430)  5_CE_loss: 0.4682 (1.1320)  5_DKS_loss: 0.0244 (0.2284)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0876 (1.1264)  data: 0.0909 (0.1465)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:23:43  iter: 4800  loss: 6.5061 (22.5600)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6780 (10.0218)  loss_bg: 1.1830 (12.5382)  time: 1.0327 (1.0914)  data: 0.0916 (0.1476)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:24:30  iter: 5100  loss: 2.4791 (5.9448)  obj_loss: 0.8491 (1.2378)  loss_cal: 0.0347 (0.0338)  rel_ce_loss: 0.7666 (1.7556)  1_CE_loss: 0.0088 (0.0867)  2_CE_loss: 0.0341 (0.1404)  2_DKS_loss: 0.0003 (0.0069)  3_CE_loss: 0.0331 (0.2850)  3_DKS_loss: 0.0019 (0.0169)  4_CE_loss: 0.3177 (0.9909)  4_DKS_loss: 0.0057 (0.0423)  5_CE_loss: 0.4200 (1.1233)  5_DKS_loss: 0.0227 (0.2254)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0885 (1.1257)  data: 0.0903 (0.1458)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:21:44  iter: 4900  loss: 6.0167 (22.2273)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.7932 (9.9169)  loss_bg: 1.1378 (12.3104)  time: 1.0243 (1.0905)  data: 0.0906 (0.1469)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:20:02  iter: 5000  loss: 6.6645 (21.9106)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5373 (9.8107)  loss_bg: 1.2087 (12.0999)  time: 1.0664 (1.0911)  data: 0.0947 (0.1477)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:22:47  iter: 5200  loss: 3.1712 (5.8846)  obj_loss: 0.7925 (1.2306)  loss_cal: 0.0304 (0.0338)  rel_ce_loss: 0.9833 (1.7384)  1_CE_loss: 0.0118 (0.0853)  2_CE_loss: 0.0333 (0.1385)  2_DKS_loss: 0.0003 (0.0067)  3_CE_loss: 0.0391 (0.2804)  3_DKS_loss: 0.0022 (0.0166)  4_CE_loss: 0.4054 (0.9787)  4_DKS_loss: 0.0069 (0.0416)  5_CE_loss: 0.6492 (1.1122)  5_DKS_loss: 0.0474 (0.2219)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0897 (1.1266)  data: 0.0930 (0.1465)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:18:08  iter: 5100  loss: 5.8685 (21.5999)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5976 (9.7069)  loss_bg: 0.9805 (11.8930)  time: 1.0316 (1.0907)  data: 0.0916 (0.1473)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:20:50  iter: 5300  loss: 2.4981 (5.8221)  obj_loss: 0.8262 (1.2233)  loss_cal: 0.0353 (0.0338)  rel_ce_loss: 0.7714 (1.7201)  1_CE_loss: 0.0090 (0.0839)  2_CE_loss: 0.0333 (0.1367)  2_DKS_loss: 0.0002 (0.0066)  3_CE_loss: 0.0413 (0.2759)  3_DKS_loss: 0.0017 (0.0163)  4_CE_loss: 0.3442 (0.9666)  4_DKS_loss: 0.0047 (0.0409)  5_CE_loss: 0.4164 (1.0998)  5_DKS_loss: 0.0272 (0.2184)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0722 (1.1262)  data: 0.0893 (0.1462)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:16:13  iter: 5200  loss: 5.7263 (21.3024)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.3251 (9.6100)  loss_bg: 1.0041 (11.6924)  time: 1.0405 (1.0901)  data: 0.0893 (0.1469)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:18:55  iter: 5400  loss: 2.6355 (5.7619)  obj_loss: 0.9014 (1.2167)  loss_cal: 0.0355 (0.0338)  rel_ce_loss: 0.7420 (1.7021)  1_CE_loss: 0.0086 (0.0826)  2_CE_loss: 0.0421 (0.1351)  2_DKS_loss: 0.0001 (0.0065)  3_CE_loss: 0.0416 (0.2717)  3_DKS_loss: 0.0011 (0.0161)  4_CE_loss: 0.3428 (0.9550)  4_DKS_loss: 0.0038 (0.0403)  5_CE_loss: 0.4414 (1.0876)  5_DKS_loss: 0.0251 (0.2148)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0630 (1.1260)  data: 0.0878 (0.1458)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:14:16  iter: 5300  loss: 6.2109 (21.0195)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5995 (9.5153)  loss_bg: 1.2080 (11.5041)  time: 1.0464 (1.0894)  data: 0.0883 (0.1464)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:16:56  iter: 5500  loss: 2.5017 (5.7041)  obj_loss: 0.8979 (1.2108)  loss_cal: 0.0334 (0.0338)  rel_ce_loss: 0.7566 (1.6848)  1_CE_loss: 0.0069 (0.0813)  2_CE_loss: 0.0386 (0.1334)  2_DKS_loss: 0.0003 (0.0064)  3_CE_loss: 0.0327 (0.2675)  3_DKS_loss: 0.0016 (0.0158)  4_CE_loss: 0.3065 (0.9437)  4_DKS_loss: 0.0043 (0.0397)  5_CE_loss: 0.4029 (1.0757)  5_DKS_loss: 0.0167 (0.2114)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0643 (1.1254)  data: 0.0872 (0.1453)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:12:25  iter: 5400  loss: 5.8323 (20.7441)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5971 (9.4257)  loss_bg: 1.0843 (11.3184)  time: 1.0313 (1.0892)  data: 0.0915 (0.1463)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:15:01  iter: 5600  loss: 2.5809 (5.6483)  obj_loss: 0.8516 (1.2049)  loss_cal: 0.0342 (0.0338)  rel_ce_loss: 0.7218 (1.6681)  1_CE_loss: 0.0131 (0.0801)  2_CE_loss: 0.0312 (0.1318)  2_DKS_loss: 0.0002 (0.0063)  3_CE_loss: 0.0421 (0.2635)  3_DKS_loss: 0.0013 (0.0156)  4_CE_loss: 0.3228 (0.9329)  4_DKS_loss: 0.0049 (0.0391)  5_CE_loss: 0.4479 (1.0644)  5_DKS_loss: 0.0234 (0.2081)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0596 (1.1252)  data: 0.0904 (0.1452)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:10:37  iter: 5500  loss: 6.2701 (20.4794)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1577 (9.3374)  loss_bg: 1.1474 (11.1420)  time: 1.0847 (1.0892)  data: 0.1020 (0.1462)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:13:08  iter: 5700  loss: 2.4811 (5.5954)  obj_loss: 0.7905 (1.1989)  loss_cal: 0.0339 (0.0339)  rel_ce_loss: 0.7291 (1.6526)  1_CE_loss: 0.0115 (0.0790)  2_CE_loss: 0.0309 (0.1304)  2_DKS_loss: 0.0003 (0.0062)  3_CE_loss: 0.0355 (0.2597)  3_DKS_loss: 0.0019 (0.0153)  4_CE_loss: 0.3180 (0.9223)  4_DKS_loss: 0.0039 (0.0385)  5_CE_loss: 0.4089 (1.0539)  5_DKS_loss: 0.0150 (0.2049)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0755 (1.1251)  data: 0.0898 (0.1452)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:08:46  iter: 5600  loss: 6.0440 (20.2220)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5615 (9.2527)  loss_bg: 1.0550 (10.9693)  time: 1.0182 (1.0891)  data: 0.0926 (0.1463)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:11:16  iter: 5800  loss: 2.4365 (5.5431)  obj_loss: 0.8203 (1.1932)  loss_cal: 0.0336 (0.0339)  rel_ce_loss: 0.7130 (1.6371)  1_CE_loss: 0.0028 (0.0778)  2_CE_loss: 0.0383 (0.1289)  2_DKS_loss: 0.0003 (0.0061)  3_CE_loss: 0.0301 (0.2560)  3_DKS_loss: 0.0017 (0.0151)  4_CE_loss: 0.3262 (0.9123)  4_DKS_loss: 0.0039 (0.0379)  5_CE_loss: 0.4034 (1.0433)  5_DKS_loss: 0.0143 (0.2018)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0683 (1.1252)  data: 0.0903 (0.1453)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:06:53  iter: 5700  loss: 5.5401 (19.9698)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.2564 (9.1697)  loss_bg: 0.9758 (10.8000)  time: 1.0210 (1.0887)  data: 0.0896 (0.1460)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:09:21  iter: 5900  loss: 2.4682 (5.4924)  obj_loss: 0.8364 (1.1875)  loss_cal: 0.0327 (0.0339)  rel_ce_loss: 0.7356 (1.6222)  1_CE_loss: 0.0038 (0.0767)  2_CE_loss: 0.0394 (0.1275)  2_DKS_loss: 0.0002 (0.0060)  3_CE_loss: 0.0343 (0.2525)  3_DKS_loss: 0.0017 (0.0149)  4_CE_loss: 0.3069 (0.9024)  4_DKS_loss: 0.0036 (0.0373)  5_CE_loss: 0.4029 (1.0330)  5_DKS_loss: 0.0233 (0.1988)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0740 (1.1250)  data: 0.0867 (0.1451)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:04:59  iter: 5800  loss: 5.7455 (19.7630)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0599 (9.0909)  loss_bg: 1.2980 (10.6721)  time: 1.0134 (1.0882)  data: 0.0888 (0.1456)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:07:24  iter: 6000  loss: 2.4716 (5.4510)  obj_loss: 0.7695 (1.1819)  loss_cal: 0.0339 (0.0339)  rel_ce_loss: 0.7711 (1.6111)  1_CE_loss: 0.0139 (0.0757)  2_CE_loss: 0.0356 (0.1262)  2_DKS_loss: 0.0002 (0.0059)  3_CE_loss: 0.0325 (0.2489)  3_DKS_loss: 0.0011 (0.0147)  4_CE_loss: 0.3059 (0.8934)  4_DKS_loss: 0.0041 (0.0368)  5_CE_loss: 0.4240 (1.0261)  5_DKS_loss: 0.0199 (0.1965)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0646 (1.1244)  data: 0.0882 (0.1446)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:03:08  iter: 5900  loss: 5.9200 (19.5333)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9885 (9.0136)  loss_bg: 0.8779 (10.5197)  time: 1.0400 (1.0880)  data: 0.0886 (0.1455)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:05:29  iter: 6100  loss: 2.6204 (5.4065)  obj_loss: 0.7695 (1.1762)  loss_cal: 0.0344 (0.0339)  rel_ce_loss: 0.7560 (1.5985)  1_CE_loss: 0.0031 (0.0747)  2_CE_loss: 0.0400 (0.1248)  2_DKS_loss: 0.0002 (0.0058)  3_CE_loss: 0.0330 (0.2456)  3_DKS_loss: 0.0033 (0.0146)  4_CE_loss: 0.3389 (0.8845)  4_DKS_loss: 0.0081 (0.0363)  5_CE_loss: 0.5077 (1.0178)  5_DKS_loss: 0.0250 (0.1940)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0415 (1.1242)  data: 0.0895 (0.1443)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 3:01:15  iter: 6000  loss: 6.1155 (19.3069)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6288 (8.9399)  loss_bg: 0.9839 (10.3670)  time: 1.0302 (1.0875)  data: 0.0876 (0.1450)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:03:35  iter: 6200  loss: 2.3377 (5.3598)  obj_loss: 0.8003 (1.1705)  loss_cal: 0.0354 (0.0340)  rel_ce_loss: 0.7500 (1.5849)  1_CE_loss: 0.0032 (0.0737)  2_CE_loss: 0.0424 (0.1236)  2_DKS_loss: 0.0002 (0.0057)  3_CE_loss: 0.0315 (0.2423)  3_DKS_loss: 0.0018 (0.0144)  4_CE_loss: 0.2985 (0.8754)  4_DKS_loss: 0.0035 (0.0358)  5_CE_loss: 0.3885 (1.0085)  5_DKS_loss: 0.0154 (0.1913)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0795 (1.1241)  data: 0.0905 (0.1442)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:59:25  iter: 6100  loss: 6.7715 (19.1056)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6197 (8.8712)  loss_bg: 1.1183 (10.2345)  time: 1.0464 (1.0874)  data: 0.0931 (0.1450)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 3:01:41  iter: 6300  loss: 2.5387 (5.3145)  obj_loss: 0.8799 (1.1656)  loss_cal: 0.0345 (0.0340)  rel_ce_loss: 0.7385 (1.5713)  1_CE_loss: 0.0029 (0.0727)  2_CE_loss: 0.0431 (0.1223)  2_DKS_loss: 0.0005 (0.0056)  3_CE_loss: 0.0309 (0.2391)  3_DKS_loss: 0.0026 (0.0142)  4_CE_loss: 0.3290 (0.8667)  4_DKS_loss: 0.0052 (0.0353)  5_CE_loss: 0.4252 (0.9991)  5_DKS_loss: 0.0341 (0.1887)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0685 (1.1239)  data: 0.0900 (0.1440)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:57:31  iter: 6200  loss: 5.6457 (18.8914)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9770 (8.8005)  loss_bg: 0.9980 (10.0909)  time: 1.0544 (1.0869)  data: 0.0912 (0.1445)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:59:44  iter: 6400  loss: 2.5779 (5.2709)  obj_loss: 0.8022 (1.1607)  loss_cal: 0.0333 (0.0340)  rel_ce_loss: 0.7500 (1.5584)  1_CE_loss: 0.0039 (0.0718)  2_CE_loss: 0.0393 (0.1210)  2_DKS_loss: 0.0003 (0.0055)  3_CE_loss: 0.0483 (0.2360)  3_DKS_loss: 0.0017 (0.0140)  4_CE_loss: 0.3388 (0.8583)  4_DKS_loss: 0.0037 (0.0349)  5_CE_loss: 0.4084 (0.9904)  5_DKS_loss: 0.0210 (0.1861)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0706 (1.1234)  data: 0.0875 (0.1436)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:55:36  iter: 6300  loss: 5.8302 (18.6841)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8183 (8.7300)  loss_bg: 1.0808 (9.9541)  time: 1.0425 (1.0862)  data: 0.0891 (0.1439)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:57:47  iter: 6500  loss: 2.5715 (5.2302)  obj_loss: 0.8521 (1.1558)  loss_cal: 0.0359 (0.0340)  rel_ce_loss: 0.7133 (1.5467)  1_CE_loss: 0.0108 (0.0709)  2_CE_loss: 0.0359 (0.1199)  2_DKS_loss: 0.0002 (0.0055)  3_CE_loss: 0.0308 (0.2330)  3_DKS_loss: 0.0013 (0.0138)  4_CE_loss: 0.3359 (0.8501)  4_DKS_loss: 0.0037 (0.0344)  5_CE_loss: 0.4272 (0.9824)  5_DKS_loss: 0.0291 (0.1839)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0812 (1.1229)  data: 0.0902 (0.1431)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:53:46  iter: 6400  loss: 5.5433 (18.4796)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0529 (8.6592)  loss_bg: 0.9762 (9.8203)  time: 1.0428 (1.0861)  data: 0.0901 (0.1439)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:55:56  iter: 6600  loss: 2.4580 (5.1921)  obj_loss: 0.7720 (1.1508)  loss_cal: 0.0341 (0.0340)  rel_ce_loss: 0.7421 (1.5359)  1_CE_loss: 0.0076 (0.0700)  2_CE_loss: 0.0351 (0.1187)  2_DKS_loss: 0.0003 (0.0054)  3_CE_loss: 0.0458 (0.2301)  3_DKS_loss: 0.0014 (0.0137)  4_CE_loss: 0.3014 (0.8427)  4_DKS_loss: 0.0034 (0.0339)  5_CE_loss: 0.4253 (0.9755)  5_DKS_loss: 0.0256 (0.1817)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0602 (1.1231)  data: 0.0867 (0.1433)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:51:54  iter: 6500  loss: 5.4398 (18.2921)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.5042 (8.5930)  loss_bg: 1.2142 (9.6991)  time: 1.0301 (1.0857)  data: 0.0905 (0.1437)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:53:59  iter: 6700  loss: 2.4531 (5.1515)  obj_loss: 0.8364 (1.1464)  loss_cal: 0.0333 (0.0340)  rel_ce_loss: 0.7038 (1.5237)  1_CE_loss: 0.0123 (0.0691)  2_CE_loss: 0.0530 (0.1176)  2_DKS_loss: 0.0002 (0.0053)  3_CE_loss: 0.0317 (0.2273)  3_DKS_loss: 0.0016 (0.0135)  4_CE_loss: 0.2899 (0.8348)  4_DKS_loss: 0.0043 (0.0335)  5_CE_loss: 0.4016 (0.9671)  5_DKS_loss: 0.0168 (0.1793)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0653 (1.1225)  data: 0.0878 (0.1428)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:50:03  iter: 6600  loss: 5.4876 (18.0998)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9375 (8.5284)  loss_bg: 0.8621 (9.5714)  time: 1.0237 (1.0855)  data: 0.0871 (0.1435)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:52:05  iter: 6800  loss: 2.5022 (5.1129)  obj_loss: 0.8076 (1.1422)  loss_cal: 0.0349 (0.0341)  rel_ce_loss: 0.7112 (1.5122)  1_CE_loss: 0.0106 (0.0684)  2_CE_loss: 0.0376 (0.1166)  2_DKS_loss: 0.0002 (0.0052)  3_CE_loss: 0.0289 (0.2246)  3_DKS_loss: 0.0020 (0.0133)  4_CE_loss: 0.3203 (0.8273)  4_DKS_loss: 0.0039 (0.0331)  5_CE_loss: 0.4300 (0.9592)  5_DKS_loss: 0.0220 (0.1771)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0489 (1.1223)  data: 0.0866 (0.1426)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:48:10  iter: 6700  loss: 5.8623 (17.9232)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0909 (8.4671)  loss_bg: 0.7431 (9.4561)  time: 1.0383 (1.0850)  data: 0.0878 (0.1430)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:50:09  iter: 6900  loss: 2.5371 (5.0750)  obj_loss: 0.8765 (1.1380)  loss_cal: 0.0354 (0.0341)  rel_ce_loss: 0.7456 (1.5009)  1_CE_loss: 0.0024 (0.0675)  2_CE_loss: 0.0307 (0.1154)  2_DKS_loss: 0.0003 (0.0052)  3_CE_loss: 0.0349 (0.2219)  3_DKS_loss: 0.0017 (0.0131)  4_CE_loss: 0.3303 (0.8200)  4_DKS_loss: 0.0048 (0.0327)  5_CE_loss: 0.4187 (0.9515)  5_DKS_loss: 0.0279 (0.1749)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0521 (1.1220)  data: 0.0894 (0.1424)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:46:19  iter: 6800  loss: 5.7634 (17.7502)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0632 (8.4095)  loss_bg: 1.1916 (9.3407)  time: 1.0449 (1.0848)  data: 0.0901 (0.1427)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:48:13  iter: 7000  loss: 2.3938 (5.0384)  obj_loss: 0.8589 (1.1339)  loss_cal: 0.0351 (0.0341)  rel_ce_loss: 0.6682 (1.4899)  1_CE_loss: 0.0024 (0.0667)  2_CE_loss: 0.0462 (0.1145)  2_DKS_loss: 0.0002 (0.0051)  3_CE_loss: 0.0386 (0.2193)  3_DKS_loss: 0.0013 (0.0130)  4_CE_loss: 0.2864 (0.8130)  4_DKS_loss: 0.0030 (0.0323)  5_CE_loss: 0.3971 (0.9440)  5_DKS_loss: 0.0207 (0.1728)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0724 (1.1215)  data: 0.0868 (0.1420)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:44:29  iter: 6900  loss: 5.4905 (17.5769)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.2863 (8.3510)  loss_bg: 0.9107 (9.2259)  time: 1.0448 (1.0845)  data: 0.0877 (0.1424)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:46:21  iter: 7100  loss: 2.6553 (5.0044)  obj_loss: 0.8325 (1.1298)  loss_cal: 0.0347 (0.0341)  rel_ce_loss: 0.8150 (1.4802)  1_CE_loss: 0.0106 (0.0660)  2_CE_loss: 0.0428 (0.1135)  2_DKS_loss: 0.0003 (0.0050)  3_CE_loss: 0.0417 (0.2168)  3_DKS_loss: 0.0022 (0.0128)  4_CE_loss: 0.3183 (0.8061)  4_DKS_loss: 0.0044 (0.0319)  5_CE_loss: 0.4250 (0.9375)  5_DKS_loss: 0.0337 (0.1709)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0856 (1.1215)  data: 0.0873 (0.1419)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:42:38  iter: 7000  loss: 5.2700 (17.4062)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0746 (8.2921)  loss_bg: 0.8014 (9.1141)  time: 1.0336 (1.0843)  data: 0.0905 (0.1423)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:44:26  iter: 7200  loss: 2.5018 (4.9698)  obj_loss: 0.8130 (1.1253)  loss_cal: 0.0334 (0.0341)  rel_ce_loss: 0.7210 (1.4700)  1_CE_loss: 0.0091 (0.0652)  2_CE_loss: 0.0243 (0.1125)  2_DKS_loss: 0.0002 (0.0050)  3_CE_loss: 0.0270 (0.2143)  3_DKS_loss: 0.0014 (0.0127)  4_CE_loss: 0.3519 (0.7994)  4_DKS_loss: 0.0037 (0.0315)  5_CE_loss: 0.4798 (0.9308)  5_DKS_loss: 0.0231 (0.1691)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0788 (1.1212)  data: 0.0879 (0.1416)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:40:50  iter: 7100  loss: 5.2356 (17.2411)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0585 (8.2361)  loss_bg: 1.0434 (9.0050)  time: 1.0646 (1.0843)  data: 0.0997 (0.1424)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:42:35  iter: 7300  loss: 2.2543 (4.9342)  obj_loss: 0.7866 (1.1206)  loss_cal: 0.0354 (0.0341)  rel_ce_loss: 0.6914 (1.4595)  1_CE_loss: 0.0090 (0.0645)  2_CE_loss: 0.0255 (0.1115)  2_DKS_loss: 0.0002 (0.0049)  3_CE_loss: 0.0217 (0.2119)  3_DKS_loss: 0.0011 (0.0125)  4_CE_loss: 0.3012 (0.7928)  4_DKS_loss: 0.0033 (0.0311)  5_CE_loss: 0.4154 (0.9238)  5_DKS_loss: 0.0178 (0.1670)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0611 (1.1213)  data: 0.0894 (0.1417)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:38:56  iter: 7200  loss: 5.8976 (17.0837)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.6277 (8.1841)  loss_bg: 0.9288 (8.8996)  time: 1.0122 (1.0837)  data: 0.0893 (0.1419)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:40:40  iter: 7400  loss: 2.3669 (4.9001)  obj_loss: 0.7490 (1.1157)  loss_cal: 0.0338 (0.0341)  rel_ce_loss: 0.7447 (1.4498)  1_CE_loss: 0.0030 (0.0638)  2_CE_loss: 0.0447 (0.1106)  2_DKS_loss: 0.0002 (0.0048)  3_CE_loss: 0.0433 (0.2096)  3_DKS_loss: 0.0011 (0.0124)  4_CE_loss: 0.2830 (0.7863)  4_DKS_loss: 0.0027 (0.0308)  5_CE_loss: 0.3707 (0.9172)  5_DKS_loss: 0.0163 (0.1651)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0667 (1.1209)  data: 0.0885 (0.1414)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:37:08  iter: 7300  loss: 5.7606 (16.9262)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.2725 (8.1297)  loss_bg: 0.9837 (8.7965)  time: 1.0544 (1.0837)  data: 0.0894 (0.1419)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:38:49  iter: 7500  loss: 2.5414 (4.8658)  obj_loss: 0.7769 (1.1110)  loss_cal: 0.0346 (0.0342)  rel_ce_loss: 0.7484 (1.4396)  1_CE_loss: 0.0037 (0.0631)  2_CE_loss: 0.0355 (0.1096)  2_DKS_loss: 0.0001 (0.0048)  3_CE_loss: 0.0275 (0.2073)  3_DKS_loss: 0.0014 (0.0122)  4_CE_loss: 0.3398 (0.7799)  4_DKS_loss: 0.0055 (0.0304)  5_CE_loss: 0.4469 (0.9105)  5_DKS_loss: 0.0229 (0.1632)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0832 (1.1211)  data: 0.0939 (0.1415)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:35:18  iter: 7400  loss: 5.5451 (16.7758)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1764 (8.0782)  loss_bg: 1.3194 (8.6976)  time: 1.0330 (1.0835)  data: 0.0901 (0.1418)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:36:53  iter: 7600  loss: 2.3704 (4.8334)  obj_loss: 0.7456 (1.1065)  loss_cal: 0.0365 (0.0342)  rel_ce_loss: 0.6888 (1.4303)  1_CE_loss: 0.0078 (0.0624)  2_CE_loss: 0.0390 (0.1087)  2_DKS_loss: 0.0002 (0.0047)  3_CE_loss: 0.0307 (0.2051)  3_DKS_loss: 0.0011 (0.0121)  4_CE_loss: 0.3044 (0.7738)  4_DKS_loss: 0.0027 (0.0301)  5_CE_loss: 0.4131 (0.9042)  5_DKS_loss: 0.0133 (0.1614)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0719 (1.1207)  data: 0.0894 (0.1412)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:33:26  iter: 7500  loss: 5.2997 (16.6268)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.3659 (8.0268)  loss_bg: 0.8925 (8.6000)  time: 1.0435 (1.0831)  data: 0.0887 (0.1414)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:35:00  iter: 7700  loss: 2.1728 (4.8012)  obj_loss: 0.7617 (1.1023)  loss_cal: 0.0356 (0.0342)  rel_ce_loss: 0.6405 (1.4208)  1_CE_loss: 0.0121 (0.0618)  2_CE_loss: 0.0402 (0.1079)  2_DKS_loss: 0.0002 (0.0047)  3_CE_loss: 0.0374 (0.2030)  3_DKS_loss: 0.0009 (0.0120)  4_CE_loss: 0.3283 (0.7678)  4_DKS_loss: 0.0029 (0.0297)  5_CE_loss: 0.3850 (0.8976)  5_DKS_loss: 0.0121 (0.1596)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0684 (1.1206)  data: 0.0887 (0.1411)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:31:37  iter: 7600  loss: 5.3486 (16.4780)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9765 (7.9758)  loss_bg: 0.8710 (8.5021)  time: 1.0442 (1.0830)  data: 0.0932 (0.1414)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:33:07  iter: 7800  loss: 2.3075 (4.7701)  obj_loss: 0.7378 (1.0983)  loss_cal: 0.0362 (0.0342)  rel_ce_loss: 0.6821 (1.4116)  1_CE_loss: 0.0010 (0.0611)  2_CE_loss: 0.0400 (0.1071)  2_DKS_loss: 0.0002 (0.0046)  3_CE_loss: 0.0326 (0.2009)  3_DKS_loss: 0.0011 (0.0118)  4_CE_loss: 0.3184 (0.7620)  4_DKS_loss: 0.0038 (0.0294)  5_CE_loss: 0.3706 (0.8913)  5_DKS_loss: 0.0122 (0.1578)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0779 (1.1205)  data: 0.0901 (0.1410)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:29:47  iter: 7700  loss: 5.5171 (16.3355)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9639 (7.9269)  loss_bg: 0.9280 (8.4086)  time: 1.0172 (1.0828)  data: 0.0885 (0.1414)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:27:56  iter: 7800  loss: 5.0434 (16.1975)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0156 (7.8791)  loss_bg: 0.8187 (8.3184)  time: 1.0412 (1.0825)  data: 0.0894 (0.1411)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:31:13  iter: 7900  loss: 2.3410 (4.7396)  obj_loss: 0.7358 (1.0942)  loss_cal: 0.0347 (0.0342)  rel_ce_loss: 0.6848 (1.4027)  1_CE_loss: 0.0110 (0.0605)  2_CE_loss: 0.0330 (0.1063)  2_DKS_loss: 0.0002 (0.0045)  3_CE_loss: 0.0317 (0.1989)  3_DKS_loss: 0.0014 (0.0117)  4_CE_loss: 0.2896 (0.7561)  4_DKS_loss: 0.0037 (0.0291)  5_CE_loss: 0.3785 (0.8853)  5_DKS_loss: 0.0239 (0.1561)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0734 (1.1202)  data: 0.0915 (0.1408)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:26:07  iter: 7900  loss: 5.3965 (16.0617)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.2237 (7.8329)  loss_bg: 1.0326 (8.2287)  time: 1.0289 (1.0825)  data: 0.0864 (0.1410)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:---Total norm 5.35779 clip coef 0.93322-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.13214, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.92782, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 1.72969, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.64959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.40462, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.16437, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.12301, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.08130, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 1.02629, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.83202, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.63734, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.60827, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.60308, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.55426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.55426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.55426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.55426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.53905, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.51694, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.51655, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.51358, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.50970, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.46928, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.42400, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.36793, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.34680, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.29996, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.29001, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27547, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.26385, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.24141, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23222, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22400, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.21962, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21864, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21801, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.21056, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20974, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20946, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20648, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20607, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20169, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20002, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19809, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19796, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19774, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19213, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19132, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19124, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18866, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17655, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17325, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.17324, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17186, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17118, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17027, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16633, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.16399, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.16075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.15942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15877, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15843, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15809, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15563, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15559, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15527, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.15315, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15309, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14959, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14955, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14718, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14715, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14702, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14661, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14534, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14079, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14045, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.14009, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.13839, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13808, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13801, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13753, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.13744, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13718, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.13704, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.13659, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13659, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13466, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13355, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13194, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12516, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12348, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12277, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12269, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12050, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12038, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11939, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11885, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.11751, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11397, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.11326, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11305, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11247, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11092, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10992, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.10972, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.10972, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10971, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10832, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10815, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10761, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10726, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10486, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10381, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10361, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10232, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10227, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10143, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10041, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09384, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09199, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08903, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08824, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08801, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08752, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.08466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08381, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.08356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08093, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07961, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07916, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07839, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07657, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07613, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07587, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07564, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07528, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.07297, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07136, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07117, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07056, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07031, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06983, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06490, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06410, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06297, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06280, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06209, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06184, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06121, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05836, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05784, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.05647, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05616, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05566, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05562, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05434, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05412, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.05048, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05025, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04681, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04675, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04337, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04299, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04150, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03674, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03630, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03461, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03460, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03441, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03402, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03361, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03300, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.03183, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03150, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03073, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.03041, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.03039, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.03039, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.02973, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02940, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02923, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02831, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02715, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02686, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.02517, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02460, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02416, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.02388, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.02388, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02345, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02300, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02211, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02136, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02071, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02026, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02016, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02016, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01979, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01947, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01863, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01768, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01759, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.01720, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01700, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01692, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01651, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01651, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01643, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01614, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01598, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01598, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01594, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01594, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01586, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01578, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01568, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01529, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01506, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01506, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01498, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01494, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01490, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01478, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01475, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01468, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01468, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01461, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01451, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01447, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01431, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01411, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01411, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01339, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01324, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01322, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01318, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.01300, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.01300, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01273, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01272, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01267, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01251, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01207, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01207, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01205, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01199, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01143, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01049, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01044, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01037, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01029, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01009, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00961, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00895, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00870, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00865, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00860, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00857, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00851, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00841, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00821, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00815, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00806, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00798, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00790, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00784, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00783, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00764, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00752, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00752, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00735, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00728, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00671, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00640, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00631, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00631, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00623, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00619, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00609, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00604, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00586, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00583, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00576, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00565, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00562, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00558, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00554, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00537, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00534, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00534, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00534, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00525, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00520, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00519, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00510, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00499, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00497, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00489, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00482, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00468, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00460, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00453, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00439, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00436, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00436, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.00428, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00421, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00421, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00413, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00401, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00398, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00381, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00379, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00375, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00352, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00300, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00299, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00292, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00290, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00285, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00281, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00275, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00254, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00250, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00241, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00208, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00197, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00174, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00166, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00130, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00121, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00121, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00113, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00109, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00099, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00065, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00060, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00044, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00022, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 2:29:22  iter: 8000  loss: 2.4127 (4.7098)  obj_loss: 0.7671 (1.0901)  loss_cal: 0.0362 (0.0342)  rel_ce_loss: 0.7092 (1.3940)  1_CE_loss: 0.0039 (0.0599)  2_CE_loss: 0.0378 (0.1055)  2_DKS_loss: 0.0002 (0.0045)  3_CE_loss: 0.0339 (0.1969)  3_DKS_loss: 0.0016 (0.0116)  4_CE_loss: 0.3195 (0.7506)  4_DKS_loss: 0.0040 (0.0288)  5_CE_loss: 0.4157 (0.8793)  5_DKS_loss: 0.0234 (0.1545)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0790 (1.1203)  data: 0.0906 (0.1409)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:---Total norm 13.42101 clip coef 0.37255-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : 7.89658, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 4.74101, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 3.00915, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 2.61672, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.45995, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 2.31259, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.16316, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.15305, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : 2.11573, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.87844, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.78322, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.74260, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.73632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.73632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.73632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.73632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 1.73318, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.69021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.65381, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.62286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 1.61342, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.50567, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 1.26090, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 1.15133, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.06323, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.95442, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.93825, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.91009, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.77392, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.74317, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.64467, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.62884, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.59703, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.57150, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.56994, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.56887, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.56712, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.55933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.55479, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.53831, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.53762, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.53287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.51709, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.47742, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.45805, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.43204, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.43153, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.42220, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.41532, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.39564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.37643, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.35028, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.33448, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : 0.33190, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : 0.33190, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.31397, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.29565, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.29518, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.29096, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.29085, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.28838, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.26838, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26830, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25377, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.24829, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.24814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23592, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20792, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.20521, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19848, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18669, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17603, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17050, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16520, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16365, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16325, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16276, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16181, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16158, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.16141, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14933, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14632, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14582, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14048, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14012, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13567, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13546, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13412, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13293, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13250, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13123, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12938, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12931, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12909, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12880, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12612, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.12582, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.12576, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12549, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12506, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12428, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12384, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12269, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12106, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12097, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11900, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11890, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11803, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.11790, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11240, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11211, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11172, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11016, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10907, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.10850, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10813, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.10576, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.10466, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10083, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10033, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09897, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09868, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09786, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09716, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09567, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09563, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09417, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09268, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09118, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08936, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.08912, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.08902, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.08902, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08761, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.08686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08661, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08613, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08609, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08608, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08187, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07961, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.07948, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07198, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07073, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07027, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06929, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06838, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.06800, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06702, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06646, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06630, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06630, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06613, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06598, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06550, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06536, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06487, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06458, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.06367, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06335, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06282, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06239, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06214, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.06214, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06155, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06149, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.06081, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06078, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06039, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05953, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05736, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05498, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05455, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05416, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.05394, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05390, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05344, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05305, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05292, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05165, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.04924, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04685, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04476, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04364, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04236, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04203, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04127, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04017, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03938, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03813, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03750, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.03600, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03551, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.03529, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03480, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03478, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03478, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.03364, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03298, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03170, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03057, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03008, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.02980, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02923, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02923, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02837, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02763, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02719, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02711, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02698, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.02459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02451, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02425, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02193, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.02170, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01966, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01934, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01656, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01595, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01378, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01354, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01297, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01290, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01229, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01195, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01187, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01183, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01171, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01159, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01146, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01136, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01131, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01096, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01069, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01046, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01006, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00989, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00969, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00964, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00956, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00954, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00941, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00934, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00932, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00921, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00913, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00903, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00902, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00901, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00892, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00891, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00890, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00890, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00881, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00878, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00876, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00875, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00868, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00865, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00864, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00855, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00850, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00850, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00850, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00849, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00847, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00847, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00841, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00839, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00839, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00837, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00833, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00829, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00824, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00824, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00822, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00822, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00818, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00809, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00805, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00804, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00803, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00797, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00793, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00784, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00783, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00771, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00764, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00749, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00742, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00658, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00596, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00589, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00554, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00469, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00453, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00453, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00448, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00432, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00424, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00408, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00353, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00350, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00337, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00336, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00332, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00326, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00325, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00319, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00319, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00316, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00314, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00314, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00313, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00310, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00310, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00306, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00303, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00303, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00302, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00298, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00296, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00288, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00283, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00279, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00246, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00202, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00198, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00167, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00164, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00164, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00118, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00108, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00101, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00096, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00082, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00074, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00061, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00041, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00021, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 2:24:18  iter: 8000  loss: 4.9882 (15.9290)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9224 (7.7867)  loss_bg: 0.9840 (8.1423)  time: 1.0307 (1.0823)  data: 0.0897 (0.1409)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:27:27  iter: 8100  loss: 2.4504 (4.6811)  obj_loss: 0.8262 (1.0864)  loss_cal: 0.0352 (0.0343)  rel_ce_loss: 0.6622 (1.3855)  1_CE_loss: 0.0024 (0.0593)  2_CE_loss: 0.0391 (0.1047)  2_DKS_loss: 0.0003 (0.0044)  3_CE_loss: 0.0376 (0.1950)  3_DKS_loss: 0.0013 (0.0115)  4_CE_loss: 0.3017 (0.7451)  4_DKS_loss: 0.0030 (0.0285)  5_CE_loss: 0.4073 (0.8736)  5_DKS_loss: 0.0160 (0.1528)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0709 (1.1199)  data: 0.0867 (0.1405)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:22:28  iter: 8100  loss: 5.3107 (15.8024)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1003 (7.7426)  loss_bg: 0.8655 (8.0598)  time: 1.0352 (1.0821)  data: 0.0886 (0.1407)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:25:33  iter: 8200  loss: 2.3229 (4.6535)  obj_loss: 0.7456 (1.0827)  loss_cal: 0.0340 (0.0343)  rel_ce_loss: 0.7380 (1.3775)  1_CE_loss: 0.0047 (0.0588)  2_CE_loss: 0.0232 (0.1039)  2_DKS_loss: 0.0002 (0.0044)  3_CE_loss: 0.0288 (0.1931)  3_DKS_loss: 0.0011 (0.0114)  4_CE_loss: 0.2895 (0.7399)  4_DKS_loss: 0.0042 (0.0282)  5_CE_loss: 0.4066 (0.8681)  5_DKS_loss: 0.0233 (0.1513)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0615 (1.1197)  data: 0.0887 (0.1403)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:20:37  iter: 8200  loss: 5.0214 (15.6729)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9221 (7.6971)  loss_bg: 0.9589 (7.9758)  time: 1.0279 (1.0817)  data: 0.0890 (0.1404)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:23:39  iter: 8300  loss: 2.3854 (4.6260)  obj_loss: 0.7866 (1.0791)  loss_cal: 0.0359 (0.0343)  rel_ce_loss: 0.6975 (1.3694)  1_CE_loss: 0.0078 (0.0582)  2_CE_loss: 0.0392 (0.1032)  2_DKS_loss: 0.0001 (0.0043)  3_CE_loss: 0.0306 (0.1913)  3_DKS_loss: 0.0015 (0.0113)  4_CE_loss: 0.2966 (0.7347)  4_DKS_loss: 0.0044 (0.0280)  5_CE_loss: 0.3739 (0.8625)  5_DKS_loss: 0.0150 (0.1497)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0846 (1.1194)  data: 0.0879 (0.1400)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:18:47  iter: 8300  loss: 5.1856 (15.5504)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1605 (7.6540)  loss_bg: 0.7390 (7.8965)  time: 1.0460 (1.0815)  data: 0.0893 (0.1401)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:21:46  iter: 8400  loss: 2.2965 (4.5999)  obj_loss: 0.7207 (1.0754)  loss_cal: 0.0355 (0.0343)  rel_ce_loss: 0.6926 (1.3619)  1_CE_loss: 0.0012 (0.0577)  2_CE_loss: 0.0273 (0.1025)  2_DKS_loss: 0.0003 (0.0043)  3_CE_loss: 0.0358 (0.1895)  3_DKS_loss: 0.0010 (0.0111)  4_CE_loss: 0.2824 (0.7298)  4_DKS_loss: 0.0023 (0.0277)  5_CE_loss: 0.4080 (0.8574)  5_DKS_loss: 0.0312 (0.1483)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0804 (1.1193)  data: 0.0901 (0.1399)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:16:58  iter: 8400  loss: 5.0580 (15.4276)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9165 (7.6107)  loss_bg: 0.8103 (7.8169)  time: 1.0401 (1.0814)  data: 0.0876 (0.1400)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:19:52  iter: 8500  loss: 2.2544 (4.5735)  obj_loss: 0.7109 (1.0717)  loss_cal: 0.0350 (0.0343)  rel_ce_loss: 0.6724 (1.3542)  1_CE_loss: 0.0123 (0.0572)  2_CE_loss: 0.0403 (0.1018)  2_DKS_loss: 0.0002 (0.0042)  3_CE_loss: 0.0376 (0.1877)  3_DKS_loss: 0.0015 (0.0110)  4_CE_loss: 0.2776 (0.7249)  4_DKS_loss: 0.0034 (0.0274)  5_CE_loss: 0.3987 (0.8523)  5_DKS_loss: 0.0164 (0.1469)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0646 (1.1190)  data: 0.0880 (0.1396)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:15:08  iter: 8500  loss: 5.1245 (15.3111)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.6075 (7.5688)  loss_bg: 0.9566 (7.7423)  time: 1.0433 (1.0811)  data: 0.0886 (0.1398)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:17:59  iter: 8600  loss: 2.2646 (4.5471)  obj_loss: 0.7456 (1.0681)  loss_cal: 0.0345 (0.0343)  rel_ce_loss: 0.6884 (1.3464)  1_CE_loss: 0.0031 (0.0566)  2_CE_loss: 0.0348 (0.1011)  2_DKS_loss: 0.0003 (0.0042)  3_CE_loss: 0.0259 (0.1859)  3_DKS_loss: 0.0009 (0.0109)  4_CE_loss: 0.2969 (0.7200)  4_DKS_loss: 0.0035 (0.0271)  5_CE_loss: 0.3869 (0.8470)  5_DKS_loss: 0.0182 (0.1454)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.1188)  data: 0.0866 (0.1394)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:13:20  iter: 8600  loss: 5.4956 (15.1958)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8928 (7.5289)  loss_bg: 0.9147 (7.6669)  time: 1.0683 (1.0811)  data: 0.0919 (0.1398)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:16:05  iter: 8700  loss: 2.3908 (4.5220)  obj_loss: 0.8091 (1.0648)  loss_cal: 0.0347 (0.0344)  rel_ce_loss: 0.7239 (1.3391)  1_CE_loss: 0.0015 (0.0561)  2_CE_loss: 0.0348 (0.1004)  2_DKS_loss: 0.0003 (0.0041)  3_CE_loss: 0.0345 (0.1843)  3_DKS_loss: 0.0015 (0.0108)  4_CE_loss: 0.3044 (0.7154)  4_DKS_loss: 0.0027 (0.0269)  5_CE_loss: 0.3926 (0.8419)  5_DKS_loss: 0.0240 (0.1440)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0729 (1.1186)  data: 0.0891 (0.1393)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:11:30  iter: 8700  loss: 5.5293 (15.0826)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1085 (7.4894)  loss_bg: 0.9203 (7.5932)  time: 1.0403 (1.0809)  data: 0.0936 (0.1397)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:14:12  iter: 8800  loss: 2.3906 (4.4976)  obj_loss: 0.7905 (1.0615)  loss_cal: 0.0343 (0.0344)  rel_ce_loss: 0.7092 (1.3319)  1_CE_loss: 0.0017 (0.0556)  2_CE_loss: 0.0271 (0.0997)  2_DKS_loss: 0.0002 (0.0041)  3_CE_loss: 0.0305 (0.1827)  3_DKS_loss: 0.0013 (0.0107)  4_CE_loss: 0.3210 (0.7109)  4_DKS_loss: 0.0033 (0.0266)  5_CE_loss: 0.4213 (0.8371)  5_DKS_loss: 0.0160 (0.1426)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0567 (1.1185)  data: 0.0868 (0.1391)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:09:39  iter: 8800  loss: 5.2795 (14.9726)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8886 (7.4506)  loss_bg: 1.0829 (7.5220)  time: 1.0473 (1.0805)  data: 0.0937 (0.1392)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:12:19  iter: 8900  loss: 2.3922 (4.4742)  obj_loss: 0.7559 (1.0584)  loss_cal: 0.0349 (0.0344)  rel_ce_loss: 0.7092 (1.3251)  1_CE_loss: 0.0038 (0.0551)  2_CE_loss: 0.0350 (0.0990)  2_DKS_loss: 0.0002 (0.0041)  3_CE_loss: 0.0315 (0.1811)  3_DKS_loss: 0.0011 (0.0106)  4_CE_loss: 0.3219 (0.7063)  4_DKS_loss: 0.0023 (0.0263)  5_CE_loss: 0.4061 (0.8325)  5_DKS_loss: 0.0237 (0.1413)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0716 (1.1182)  data: 0.0889 (0.1389)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:07:51  iter: 8900  loss: 5.0819 (14.8648)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0665 (7.4129)  loss_bg: 0.6526 (7.4519)  time: 1.0260 (1.0806)  data: 0.0888 (0.1393)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:10:27  iter: 9000  loss: 2.2298 (4.4505)  obj_loss: 0.7651 (1.0554)  loss_cal: 0.0340 (0.0344)  rel_ce_loss: 0.6672 (1.3182)  1_CE_loss: 0.0087 (0.0546)  2_CE_loss: 0.0434 (0.0984)  2_DKS_loss: 0.0001 (0.0040)  3_CE_loss: 0.0382 (0.1795)  3_DKS_loss: 0.0006 (0.0105)  4_CE_loss: 0.2970 (0.7018)  4_DKS_loss: 0.0016 (0.0261)  5_CE_loss: 0.3551 (0.8278)  5_DKS_loss: 0.0116 (0.1399)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0707 (1.1183)  data: 0.0878 (0.1389)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:06:02  iter: 9000  loss: 5.2505 (14.7586)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1150 (7.3764)  loss_bg: 0.6715 (7.3821)  time: 1.0402 (1.0803)  data: 0.0901 (0.1391)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:08:34  iter: 9100  loss: 2.3691 (4.4272)  obj_loss: 0.7930 (1.0525)  loss_cal: 0.0353 (0.0344)  rel_ce_loss: 0.7480 (1.3114)  1_CE_loss: 0.0105 (0.0542)  2_CE_loss: 0.0260 (0.0978)  2_DKS_loss: 0.0002 (0.0040)  3_CE_loss: 0.0318 (0.1779)  3_DKS_loss: 0.0009 (0.0104)  4_CE_loss: 0.3243 (0.6974)  4_DKS_loss: 0.0041 (0.0258)  5_CE_loss: 0.3994 (0.8230)  5_DKS_loss: 0.0122 (0.1386)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0347 (1.1180)  data: 0.0883 (0.1386)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:04:12  iter: 9100  loss: 5.1820 (14.6541)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.3146 (7.3400)  loss_bg: 0.7505 (7.3141)  time: 1.0362 (1.0800)  data: 0.0896 (0.1389)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:06:41  iter: 9200  loss: 2.5309 (4.4055)  obj_loss: 0.8140 (1.0496)  loss_cal: 0.0343 (0.0344)  rel_ce_loss: 0.7912 (1.3051)  1_CE_loss: 0.0077 (0.0537)  2_CE_loss: 0.0492 (0.0972)  2_DKS_loss: 0.0001 (0.0039)  3_CE_loss: 0.0352 (0.1764)  3_DKS_loss: 0.0010 (0.0103)  4_CE_loss: 0.2951 (0.6932)  4_DKS_loss: 0.0044 (0.0256)  5_CE_loss: 0.4534 (0.8187)  5_DKS_loss: 0.0457 (0.1374)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0383 (1.1179)  data: 0.0887 (0.1385)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:02:22  iter: 9200  loss: 5.2632 (14.5513)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0438 (7.3044)  loss_bg: 0.8460 (7.2470)  time: 1.0402 (1.0798)  data: 0.0877 (0.1387)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:04:48  iter: 9300  loss: 2.3125 (4.3834)  obj_loss: 0.8062 (1.0470)  loss_cal: 0.0343 (0.0344)  rel_ce_loss: 0.6823 (1.2985)  1_CE_loss: 0.0151 (0.0533)  2_CE_loss: 0.0250 (0.0966)  2_DKS_loss: 0.0002 (0.0039)  3_CE_loss: 0.0282 (0.1750)  3_DKS_loss: 0.0008 (0.0102)  4_CE_loss: 0.2717 (0.6890)  4_DKS_loss: 0.0033 (0.0253)  5_CE_loss: 0.3698 (0.8142)  5_DKS_loss: 0.0164 (0.1362)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0781 (1.1176)  data: 0.0882 (0.1383)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 2:00:34  iter: 9300  loss: 5.1877 (14.4569)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8440 (7.2692)  loss_bg: 1.1182 (7.1876)  time: 1.0450 (1.0797)  data: 0.0932 (0.1385)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:02:56  iter: 9400  loss: 2.2821 (4.3614)  obj_loss: 0.8066 (1.0441)  loss_cal: 0.0339 (0.0344)  rel_ce_loss: 0.6987 (1.2919)  1_CE_loss: 0.0050 (0.0528)  2_CE_loss: 0.0330 (0.0960)  2_DKS_loss: 0.0001 (0.0039)  3_CE_loss: 0.0291 (0.1735)  3_DKS_loss: 0.0008 (0.0101)  4_CE_loss: 0.2881 (0.6850)  4_DKS_loss: 0.0018 (0.0251)  5_CE_loss: 0.3831 (0.8098)  5_DKS_loss: 0.0133 (0.1349)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0701 (1.1176)  data: 0.0891 (0.1382)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:58:44  iter: 9400  loss: 5.0728 (14.3583)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.1682 (7.2348)  loss_bg: 1.0544 (7.1235)  time: 1.0373 (1.0795)  data: 0.0907 (0.1384)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 2:01:02  iter: 9500  loss: 2.2039 (4.3403)  obj_loss: 0.7563 (1.0412)  loss_cal: 0.0358 (0.0345)  rel_ce_loss: 0.6133 (1.2857)  1_CE_loss: 0.0027 (0.0524)  2_CE_loss: 0.0318 (0.0954)  2_DKS_loss: 0.0002 (0.0038)  3_CE_loss: 0.0277 (0.1721)  3_DKS_loss: 0.0008 (0.0100)  4_CE_loss: 0.2947 (0.6810)  4_DKS_loss: 0.0019 (0.0249)  5_CE_loss: 0.3894 (0.8057)  5_DKS_loss: 0.0127 (0.1337)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0619 (1.1174)  data: 0.0873 (0.1380)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:56:55  iter: 9500  loss: 5.1475 (14.2621)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.7229 (7.2013)  loss_bg: 1.2199 (7.0608)  time: 1.0399 (1.0794)  data: 0.0901 (0.1382)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:59:10  iter: 9600  loss: 2.4405 (4.3197)  obj_loss: 0.8257 (1.0385)  loss_cal: 0.0356 (0.0345)  rel_ce_loss: 0.6841 (1.2796)  1_CE_loss: 0.0102 (0.0520)  2_CE_loss: 0.0331 (0.0949)  2_DKS_loss: 0.0003 (0.0038)  3_CE_loss: 0.0295 (0.1707)  3_DKS_loss: 0.0009 (0.0099)  4_CE_loss: 0.3107 (0.6771)  4_DKS_loss: 0.0028 (0.0246)  5_CE_loss: 0.3890 (0.8015)  5_DKS_loss: 0.0161 (0.1326)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0756 (1.1173)  data: 0.0887 (0.1380)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:55:07  iter: 9600  loss: 5.2985 (14.1679)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9394 (7.1679)  loss_bg: 0.8105 (7.0000)  time: 1.0506 (1.0792)  data: 0.0929 (0.1382)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:57:18  iter: 9700  loss: 2.2842 (4.2989)  obj_loss: 0.7651 (1.0359)  loss_cal: 0.0379 (0.0345)  rel_ce_loss: 0.6766 (1.2735)  1_CE_loss: 0.0089 (0.0516)  2_CE_loss: 0.0322 (0.0943)  2_DKS_loss: 0.0002 (0.0037)  3_CE_loss: 0.0355 (0.1693)  3_DKS_loss: 0.0007 (0.0098)  4_CE_loss: 0.2858 (0.6732)  4_DKS_loss: 0.0023 (0.0244)  5_CE_loss: 0.4189 (0.7973)  5_DKS_loss: 0.0086 (0.1314)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0718 (1.1172)  data: 0.0885 (0.1379)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:53:19  iter: 9700  loss: 5.2145 (14.0772)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8010 (7.1354)  loss_bg: 1.0927 (6.9418)  time: 1.0517 (1.0793)  data: 0.0909 (0.1383)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:55:25  iter: 9800  loss: 2.2488 (4.2783)  obj_loss: 0.7173 (1.0331)  loss_cal: 0.0344 (0.0345)  rel_ce_loss: 0.6635 (1.2675)  1_CE_loss: 0.0090 (0.0512)  2_CE_loss: 0.0348 (0.0938)  2_DKS_loss: 0.0001 (0.0037)  3_CE_loss: 0.0268 (0.1680)  3_DKS_loss: 0.0011 (0.0098)  4_CE_loss: 0.3206 (0.6694)  4_DKS_loss: 0.0021 (0.0242)  5_CE_loss: 0.4049 (0.7931)  5_DKS_loss: 0.0113 (0.1301)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0677 (1.1170)  data: 0.0884 (0.1377)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:51:30  iter: 9800  loss: 4.8857 (13.9870)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.8888 (7.1043)  loss_bg: 1.0070 (6.8828)  time: 1.0376 (1.0791)  data: 0.0879 (0.1380)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:53:33  iter: 9900  loss: 2.2295 (4.2585)  obj_loss: 0.7578 (1.0305)  loss_cal: 0.0351 (0.0345)  rel_ce_loss: 0.6590 (1.2616)  1_CE_loss: 0.0081 (0.0508)  2_CE_loss: 0.0456 (0.0933)  2_DKS_loss: 0.0003 (0.0037)  3_CE_loss: 0.0392 (0.1668)  3_DKS_loss: 0.0012 (0.0097)  4_CE_loss: 0.3073 (0.6657)  4_DKS_loss: 0.0025 (0.0240)  5_CE_loss: 0.3733 (0.7891)  5_DKS_loss: 0.0178 (0.1290)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0622 (1.1170)  data: 0.0891 (0.1377)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:49:43  iter: 9900  loss: 5.2408 (13.8989)  obj_loss: 0.0000 (0.0000)  loss_fg: 4.0541 (7.0728)  loss_bg: 1.2001 (6.8261)  time: 1.0333 (1.0792)  data: 0.0886 (0.1381)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:51:40  iter: 10000  loss: 2.4393 (4.2391)  obj_loss: 0.7769 (1.0280)  loss_cal: 0.0338 (0.0345)  rel_ce_loss: 0.7082 (1.2559)  1_CE_loss: 0.0101 (0.0504)  2_CE_loss: 0.0349 (0.0928)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0473 (0.1655)  3_DKS_loss: 0.0009 (0.0096)  4_CE_loss: 0.2761 (0.6621)  4_DKS_loss: 0.0024 (0.0238)  5_CE_loss: 0.3998 (0.7852)  5_DKS_loss: 0.0151 (0.1279)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0386 (1.1168)  data: 0.0884 (0.1376)  lr: 0.016000  max mem: 10734
INFO:maskrcnn_benchmark:Start validating
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
INFO:maskrcnn_benchmark:eta: 1:47:55  iter: 10000  loss: 5.0446 (13.8126)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.5069 (7.0423)  loss_bg: 0.9725 (6.7703)  time: 1.1282 (1.0792)  data: 0.1534 (0.1380)  lr: 0.016000  max mem: 10548
INFO:maskrcnn_benchmark:Start validating
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
INFO:maskrcnn_benchmark:Total run time: 0:04:42.376974 (0.056475394773483276 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:03:58.610885 (0.047722176933288576 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Total run time: 0:04:31.805297 (0.054361059427261355 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:03:56.731455 (0.04734629092216492 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.5927
====================================================================================================
SGG eval:     R @ 20: 0.2601;     R @ 50: 0.3123;     R @ 100: 0.3333;  for mode=sgcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2855;  ng-R @ 50: 0.3716;  ng-R @ 100: 0.4328;  for mode=sgcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.1068;    zR @ 50: 0.1325;    zR @ 100: 0.1538;  for mode=sgcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.1261; ng-zR @ 50: 0.1581; ng-zR @ 100: 0.2083;  for mode=sgcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0443;    mR @ 50: 0.0610;    mR @ 100: 0.0696;  for mode=sgcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0228) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0202) (attached to:0.0036) (behind:0.0752) (belonging to:0.0000) (between:0.0000) (carrying:0.3092) (covered in:0.0000) (covering:0.0000) (eating:0.1905) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0588) (has:0.4556) (holding:0.0940) (in:0.1399) (in front of:0.0347) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1233) (of:0.3666) (on:0.4384) (on back of:0.0000) (over:0.0244) (painted on:0.0000) (parked on:0.0000) (part of:0.0139) (playing:0.0000) (riding:0.2217) (says:0.0000) (sitting on:0.0571) (standing on:0.0000) (to:0.0000) (under:0.0838) (using:0.0000) (walking in:0.0000) (walking on:0.0856) (watching:0.1176) (wearing:0.5010) (wears:0.0044) (with:0.0356) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0439;    mR @ 50: 0.0642;    mR @ 100: 0.0732;  for mode=sgcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0174) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0233) (attached to:0.0075) (behind:0.0704) (belonging to:0.0000) (between:0.0000) (carrying:0.2946) (covered in:0.0000) (covering:0.0000) (eating:0.0870) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0488) (has:0.5295) (holding:0.1120) (in:0.1485) (in front of:0.0457) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1189) (of:0.3980) (on:0.4823) (on back of:0.0000) (over:0.0194) (painted on:0.0000) (parked on:0.0000) (part of:0.0426) (playing:0.0000) (riding:0.2929) (says:0.0000) (sitting on:0.0718) (standing on:0.0000) (to:0.0000) (under:0.0880) (using:0.0000) (walking in:0.0000) (walking on:0.1180) (watching:0.0714) (wearing:0.5270) (wears:0.0048) (with:0.0421) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0526; ng-mR @ 50: 0.0856; ng-mR @ 100: 0.1239;  for mode=sgcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1076) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0783) (attached to:0.0404) (behind:0.1506) (belonging to:0.0000) (between:0.0000) (carrying:0.3399) (covered in:0.1429) (covering:0.0286) (eating:0.1905) (flying in:0.0000) (for:0.0000) (from:0.1000) (growing on:0.0000) (hanging from:0.3199) (has:0.5034) (holding:0.2320) (in:0.2461) (in front of:0.1287) (laying on:0.0476) (looking at:0.0870) (lying on:0.1111) (made of:0.0000) (mounted on:0.0000) (near:0.2322) (of:0.4818) (on:0.5235) (on back of:0.0455) (over:0.0549) (painted on:0.0000) (parked on:0.0143) (part of:0.0139) (playing:0.0000) (riding:0.3110) (says:0.0000) (sitting on:0.1714) (standing on:0.0815) (to:0.0000) (under:0.1577) (using:0.0769) (walking in:0.0000) (walking on:0.1883) (watching:0.2353) (wearing:0.5025) (wears:0.1073) (with:0.1405) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0262; zs-mR @ 50: 0.0373; zs-mR @ 100: 0.0439;  for mode=sgcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.2222) (behind:0.3333) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.2000) (holding:0.0000) (in:0.1111) (in front of:0.2727) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1429) (of:0.0000) (on:0.2500) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.2000) (standing on:0.0000) (to:0.0000) (under:0.2500) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2143) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.2302;     A @ 50: 0.2306;     A @ 100: 0.2306;  for mode=sgcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Validation Result: 0.3333
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:     R @ 20: 0.5607;     R @ 50: 0.6180;     R @ 100: 0.6358;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.6362;  ng-R @ 50: 0.7654;  ng-R @ 100: 0.8340;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2442;    zR @ 50: 0.3187;    zR @ 100: 0.3379;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.2891; ng-zR @ 50: 0.4744; ng-zR @ 100: 0.6143;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.1227;    mR @ 50: 0.1580;    mR @ 100: 0.1745;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0850) (across:0.0000) (against:0.0000) (along:0.0385) (and:0.0000) (at:0.4965) (attached to:0.0115) (behind:0.2446) (belonging to:0.0000) (between:0.0192) (carrying:0.4605) (covered in:0.0357) (covering:0.1102) (eating:0.5238) (flying in:0.0000) (for:0.0370) (from:0.0000) (growing on:0.0000) (hanging from:0.0551) (has:0.7762) (holding:0.5432) (in:0.4126) (in front of:0.2471) (laying on:0.0000) (looking at:0.3261) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1912) (of:0.3806) (on:0.8935) (on back of:0.0000) (over:0.0244) (painted on:0.0000) (parked on:0.3056) (part of:0.0000) (playing:0.0000) (riding:0.3199) (says:0.0000) (sitting on:0.2025) (standing on:0.0000) (to:0.0000) (under:0.1743) (using:0.1923) (walking in:0.0000) (walking on:0.2257) (watching:0.3725) (wearing:0.9030) (wears:0.0221) (with:0.0941) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.1221;    mR @ 50: 0.1570;    mR @ 100: 0.1750;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0461) (across:0.0000) (against:0.0000) (along:0.0286) (and:0.0000) (at:0.5291) (attached to:0.0075) (behind:0.2401) (belonging to:0.0000) (between:0.0263) (carrying:0.4643) (covered in:0.0417) (covering:0.1644) (eating:0.4348) (flying in:0.0000) (for:0.0270) (from:0.0000) (growing on:0.0000) (hanging from:0.0610) (has:0.8208) (holding:0.5680) (in:0.4469) (in front of:0.2590) (laying on:0.0000) (looking at:0.2571) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1869) (of:0.3631) (on:0.9111) (on back of:0.0000) (over:0.0194) (painted on:0.0000) (parked on:0.2751) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.1753) (standing on:0.0000) (to:0.0000) (under:0.1840) (using:0.2381) (walking in:0.0000) (walking on:0.2493) (watching:0.3571) (wearing:0.9176) (wears:0.0144) (with:0.1029) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.1849; ng-mR @ 50: 0.2889; ng-mR @ 100: 0.4111;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.4046) (across:0.1111) (against:0.0526) (along:0.2308) (and:0.0323) (at:0.7210) (attached to:0.1655) (behind:0.3731) (belonging to:0.0286) (between:0.0385) (carrying:0.8202) (covered in:0.2500) (covering:0.3578) (eating:0.7143) (flying in:0.0000) (for:0.1111) (from:0.2000) (growing on:0.0000) (hanging from:0.3346) (has:0.8542) (holding:0.7744) (in:0.8245) (in front of:0.4807) (laying on:0.2619) (looking at:0.3913) (lying on:0.2222) (made of:0.0000) (mounted on:0.0435) (near:0.5037) (of:0.7963) (on:0.9508) (on back of:0.1591) (over:0.1646) (painted on:0.1429) (parked on:0.9136) (part of:0.0920) (playing:0.0000) (riding:0.9137) (says:1.0000) (sitting on:0.6309) (standing on:0.2924) (to:0.2222) (under:0.4320) (using:0.5192) (walking in:0.3077) (walking on:0.8228) (watching:0.5294) (wearing:0.9618) (wears:0.8296) (with:0.5722) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0580; zs-mR @ 50: 0.0898; zs-mR @ 100: 0.1003;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.1111) (behind:0.4444) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.5000) (has:0.4000) (holding:0.5000) (in:0.4286) (in front of:0.0909) (laying on:0.0000) (looking at:0.3333) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.2143) (of:0.4000) (on:0.8333) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.4000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.3571) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.5250;     A @ 50: 0.5260;     A @ 100: 0.5260;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Validation Result: 0.6358
INFO:maskrcnn_benchmark:eta: 1:54:15  iter: 10100  loss: 2.1682 (4.2193)  obj_loss: 0.7520 (1.0253)  loss_cal: 0.0348 (0.0345)  rel_ce_loss: 0.6459 (1.2500)  1_CE_loss: 0.0091 (0.0500)  2_CE_loss: 0.0265 (0.0923)  2_DKS_loss: 0.0001 (0.0036)  3_CE_loss: 0.0290 (0.1643)  3_DKS_loss: 0.0004 (0.0095)  4_CE_loss: 0.2763 (0.6585)  4_DKS_loss: 0.0011 (0.0235)  5_CE_loss: 0.3676 (0.7812)  5_DKS_loss: 0.0040 (0.1267)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0845 (1.1619)  data: 0.0927 (0.1827)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:50:36  iter: 10100  loss: 4.6655 (13.7227)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.9113 (7.0101)  loss_bg: 0.7807 (6.7127)  time: 1.0414 (1.1248)  data: 0.0910 (0.1835)  lr: 0.001600  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:52:16  iter: 10200  loss: 2.2276 (4.1995)  obj_loss: 0.7588 (1.0225)  loss_cal: 0.0345 (0.0345)  rel_ce_loss: 0.6448 (1.2441)  1_CE_loss: 0.0058 (0.0496)  2_CE_loss: 0.0373 (0.0918)  2_DKS_loss: 0.0001 (0.0036)  3_CE_loss: 0.0311 (0.1630)  3_DKS_loss: 0.0004 (0.0094)  4_CE_loss: 0.2974 (0.6549)  4_DKS_loss: 0.0011 (0.0233)  5_CE_loss: 0.3985 (0.7773)  5_DKS_loss: 0.0038 (0.1255)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0700 (1.1615)  data: 0.0924 (0.1823)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:48:40  iter: 10200  loss: 4.4191 (13.6331)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4910 (6.9771)  loss_bg: 0.8906 (6.6560)  time: 1.0299 (1.1243)  data: 0.0910 (0.1830)  lr: 0.001600  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:50:18  iter: 10300  loss: 2.1071 (4.1798)  obj_loss: 0.7158 (1.0199)  loss_cal: 0.0360 (0.0345)  rel_ce_loss: 0.6547 (1.2382)  1_CE_loss: 0.0077 (0.0492)  2_CE_loss: 0.0239 (0.0912)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0245 (0.1618)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2808 (0.6514)  4_DKS_loss: 0.0009 (0.0231)  5_CE_loss: 0.3610 (0.7734)  5_DKS_loss: 0.0031 (0.1243)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0870 (1.1611)  data: 0.0929 (0.1819)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:46:45  iter: 10300  loss: 4.5954 (13.5455)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.6031 (6.9452)  loss_bg: 0.8212 (6.6003)  time: 1.0376 (1.1238)  data: 0.0940 (0.1826)  lr: 0.001600  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:48:19  iter: 10400  loss: 2.1159 (4.1595)  obj_loss: 0.7739 (1.0170)  loss_cal: 0.0366 (0.0346)  rel_ce_loss: 0.6229 (1.2321)  1_CE_loss: 0.0099 (0.0489)  2_CE_loss: 0.0365 (0.0907)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0278 (0.1606)  3_DKS_loss: 0.0003 (0.0092)  4_CE_loss: 0.2726 (0.6477)  4_DKS_loss: 0.0009 (0.0229)  5_CE_loss: 0.3382 (0.7693)  5_DKS_loss: 0.0026 (0.1231)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0666 (1.1606)  data: 0.0899 (0.1814)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:44:50  iter: 10400  loss: 4.4404 (13.4587)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.6593 (6.9131)  loss_bg: 0.8118 (6.5457)  time: 1.0414 (1.1233)  data: 0.0913 (0.1821)  lr: 0.001600  max mem: 10548
INFO:maskrcnn_benchmark:eta: 1:46:21  iter: 10500  loss: 2.0740 (4.1402)  obj_loss: 0.7261 (1.0143)  loss_cal: 0.0369 (0.0346)  rel_ce_loss: 0.6092 (1.2264)  1_CE_loss: 0.0066 (0.0485)  2_CE_loss: 0.0352 (0.0902)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0303 (0.1594)  3_DKS_loss: 0.0003 (0.0091)  4_CE_loss: 0.2638 (0.6442)  4_DKS_loss: 0.0008 (0.0227)  5_CE_loss: 0.3476 (0.7655)  5_DKS_loss: 0.0030 (0.1220)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0862 (1.1602)  data: 0.0927 (0.1810)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:42:55  iter: 10500  loss: 4.2375 (13.3730)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4064 (6.8809)  loss_bg: 0.8471 (6.4921)  time: 1.0391 (1.1228)  data: 0.0901 (0.1816)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:44:23  iter: 10600  loss: 2.0760 (4.1213)  obj_loss: 0.6816 (1.0115)  loss_cal: 0.0358 (0.0346)  rel_ce_loss: 0.6488 (1.2208)  1_CE_loss: 0.0026 (0.0481)  2_CE_loss: 0.0360 (0.0897)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0362 (0.1582)  3_DKS_loss: 0.0003 (0.0091)  4_CE_loss: 0.2496 (0.6408)  4_DKS_loss: 0.0010 (0.0225)  5_CE_loss: 0.3509 (0.7618)  5_DKS_loss: 0.0026 (0.1208)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0818 (1.1599)  data: 0.0936 (0.1807)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:41:01  iter: 10600  loss: 3.9430 (13.2886)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1181 (6.8491)  loss_bg: 0.8030 (6.4395)  time: 1.0356 (1.1225)  data: 0.0929 (0.1814)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:42:23  iter: 10700  loss: 2.0326 (4.1028)  obj_loss: 0.7559 (1.0088)  loss_cal: 0.0356 (0.0346)  rel_ce_loss: 0.5866 (1.2154)  1_CE_loss: 0.0037 (0.0478)  2_CE_loss: 0.0434 (0.0893)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0351 (0.1571)  3_DKS_loss: 0.0004 (0.0090)  4_CE_loss: 0.2675 (0.6375)  4_DKS_loss: 0.0010 (0.0223)  5_CE_loss: 0.3496 (0.7580)  5_DKS_loss: 0.0026 (0.1197)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0694 (1.1591)  data: 0.0934 (0.1799)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:39:05  iter: 10700  loss: 4.4443 (13.2073)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.5433 (6.8193)  loss_bg: 0.8503 (6.3880)  time: 1.0239 (1.1218)  data: 0.0923 (0.1806)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:37:11  iter: 10800  loss: 4.3947 (13.1263)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4028 (6.7889)  loss_bg: 0.8601 (6.3374)  time: 1.0382 (1.1215)  data: 0.0915 (0.1804)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:40:26  iter: 10800  loss: 2.0576 (4.0846)  obj_loss: 0.7588 (1.0062)  loss_cal: 0.0359 (0.0346)  rel_ce_loss: 0.6008 (1.2100)  1_CE_loss: 0.0064 (0.0475)  2_CE_loss: 0.0348 (0.0889)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0318 (0.1560)  3_DKS_loss: 0.0003 (0.0089)  4_CE_loss: 0.2723 (0.6342)  4_DKS_loss: 0.0010 (0.0221)  5_CE_loss: 0.3383 (0.7544)  5_DKS_loss: 0.0029 (0.1187)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0713 (1.1589)  data: 0.0918 (0.1797)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:35:16  iter: 10900  loss: 4.3410 (13.0456)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4347 (6.7580)  loss_bg: 0.8826 (6.2876)  time: 1.0350 (1.1209)  data: 0.0905 (0.1798)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:38:26  iter: 10900  loss: 1.9421 (4.0657)  obj_loss: 0.6250 (1.0031)  loss_cal: 0.0359 (0.0346)  rel_ce_loss: 0.5939 (1.2044)  1_CE_loss: 0.0089 (0.0472)  2_CE_loss: 0.0314 (0.0884)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0328 (0.1550)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2676 (0.6309)  4_DKS_loss: 0.0009 (0.0219)  5_CE_loss: 0.3667 (0.7507)  5_DKS_loss: 0.0026 (0.1176)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0627 (1.1582)  data: 0.0909 (0.1792)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:33:22  iter: 11000  loss: 4.4214 (12.9662)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.5110 (6.7276)  loss_bg: 0.8482 (6.2386)  time: 1.0485 (1.1205)  data: 0.0943 (0.1794)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:36:29  iter: 11000  loss: 2.0075 (4.0463)  obj_loss: 0.6416 (0.9995)  loss_cal: 0.0366 (0.0347)  rel_ce_loss: 0.5819 (1.1988)  1_CE_loss: 0.0059 (0.0468)  2_CE_loss: 0.0334 (0.0879)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0293 (0.1539)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2724 (0.6276)  4_DKS_loss: 0.0010 (0.0217)  5_CE_loss: 0.3440 (0.7469)  5_DKS_loss: 0.0028 (0.1165)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0794 (1.1579)  data: 0.0970 (0.1788)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:31:28  iter: 11100  loss: 4.3155 (12.8880)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3012 (6.6976)  loss_bg: 0.8166 (6.1904)  time: 1.0391 (1.1201)  data: 0.0940 (0.1791)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:34:31  iter: 11100  loss: 1.9732 (4.0274)  obj_loss: 0.6104 (0.9962)  loss_cal: 0.0381 (0.0347)  rel_ce_loss: 0.6034 (1.1933)  1_CE_loss: 0.0084 (0.0465)  2_CE_loss: 0.0356 (0.0875)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0299 (0.1528)  3_DKS_loss: 0.0003 (0.0087)  4_CE_loss: 0.2613 (0.6242)  4_DKS_loss: 0.0009 (0.0215)  5_CE_loss: 0.3196 (0.7432)  5_DKS_loss: 0.0026 (0.1155)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0761 (1.1575)  data: 0.0930 (0.1784)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:29:34  iter: 11200  loss: 4.0373 (12.8105)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0786 (6.6674)  loss_bg: 0.8480 (6.1431)  time: 1.0334 (1.1197)  data: 0.0926 (0.1788)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:32:34  iter: 11200  loss: 1.9825 (4.0092)  obj_loss: 0.6143 (0.9931)  loss_cal: 0.0373 (0.0347)  rel_ce_loss: 0.6290 (1.1880)  1_CE_loss: 0.0044 (0.0462)  2_CE_loss: 0.0260 (0.0870)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0270 (0.1518)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2809 (0.6211)  4_DKS_loss: 0.0009 (0.0213)  5_CE_loss: 0.3468 (0.7396)  5_DKS_loss: 0.0028 (0.1145)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0754 (1.1573)  data: 0.0919 (0.1782)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:27:40  iter: 11300  loss: 4.1058 (12.7340)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0184 (6.6373)  loss_bg: 0.8775 (6.0967)  time: 1.0318 (1.1192)  data: 0.0913 (0.1782)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:30:36  iter: 11300  loss: 1.9623 (3.9906)  obj_loss: 0.5884 (0.9899)  loss_cal: 0.0381 (0.0347)  rel_ce_loss: 0.5716 (1.1825)  1_CE_loss: 0.0074 (0.0459)  2_CE_loss: 0.0436 (0.0866)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0392 (0.1508)  3_DKS_loss: 0.0004 (0.0085)  4_CE_loss: 0.2507 (0.6178)  4_DKS_loss: 0.0010 (0.0211)  5_CE_loss: 0.3252 (0.7360)  5_DKS_loss: 0.0029 (0.1135)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0585 (1.1567)  data: 0.0898 (0.1776)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:25:47  iter: 11400  loss: 3.9188 (12.6594)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1101 (6.6086)  loss_bg: 0.8189 (6.0509)  time: 1.0403 (1.1189)  data: 0.0955 (0.1779)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:28:39  iter: 11400  loss: 1.8592 (3.9726)  obj_loss: 0.5903 (0.9866)  loss_cal: 0.0380 (0.0348)  rel_ce_loss: 0.5496 (1.1773)  1_CE_loss: 0.0078 (0.0456)  2_CE_loss: 0.0289 (0.0862)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0235 (0.1497)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2555 (0.6148)  4_DKS_loss: 0.0008 (0.0210)  5_CE_loss: 0.3270 (0.7326)  5_DKS_loss: 0.0026 (0.1126)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0821 (1.1564)  data: 0.0925 (0.1773)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:23:55  iter: 11500  loss: 3.9679 (12.5862)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0982 (6.5801)  loss_bg: 0.8555 (6.0061)  time: 1.0604 (1.1189)  data: 0.0937 (0.1779)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:26:43  iter: 11500  loss: 1.8878 (3.9548)  obj_loss: 0.6138 (0.9835)  loss_cal: 0.0369 (0.0348)  rel_ce_loss: 0.5650 (1.1721)  1_CE_loss: 0.0116 (0.0453)  2_CE_loss: 0.0380 (0.0857)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0448 (0.1487)  3_DKS_loss: 0.0003 (0.0084)  4_CE_loss: 0.2757 (0.6116)  4_DKS_loss: 0.0009 (0.0208)  5_CE_loss: 0.3462 (0.7292)  5_DKS_loss: 0.0028 (0.1116)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.1563)  data: 0.0931 (0.1772)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:22:01  iter: 11600  loss: 4.2713 (12.5137)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3634 (6.5518)  loss_bg: 0.8087 (5.9619)  time: 1.0494 (1.1185)  data: 0.0958 (0.1775)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:24:46  iter: 11600  loss: 1.9552 (3.9374)  obj_loss: 0.6133 (0.9803)  loss_cal: 0.0369 (0.0348)  rel_ce_loss: 0.5976 (1.1671)  1_CE_loss: 0.0036 (0.0450)  2_CE_loss: 0.0390 (0.0853)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0308 (0.1477)  3_DKS_loss: 0.0004 (0.0083)  4_CE_loss: 0.2606 (0.6087)  4_DKS_loss: 0.0009 (0.0206)  5_CE_loss: 0.3428 (0.7259)  5_DKS_loss: 0.0030 (0.1107)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0907 (1.1560)  data: 0.0927 (0.1770)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:20:08  iter: 11700  loss: 4.3071 (12.4425)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4103 (6.5240)  loss_bg: 0.8604 (5.9185)  time: 1.0646 (1.1184)  data: 0.0909 (0.1773)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:22:49  iter: 11700  loss: 1.9064 (3.9205)  obj_loss: 0.6025 (0.9773)  loss_cal: 0.0366 (0.0348)  rel_ce_loss: 0.5748 (1.1622)  1_CE_loss: 0.0040 (0.0447)  2_CE_loss: 0.0383 (0.0849)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0285 (0.1468)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2728 (0.6057)  4_DKS_loss: 0.0009 (0.0204)  5_CE_loss: 0.3480 (0.7226)  5_DKS_loss: 0.0024 (0.1097)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0771 (1.1556)  data: 0.0921 (0.1766)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:18:15  iter: 11800  loss: 4.2711 (12.3731)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1499 (6.4970)  loss_bg: 0.9165 (5.8761)  time: 1.0426 (1.1180)  data: 0.0934 (0.1769)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:20:52  iter: 11800  loss: 1.8360 (3.9034)  obj_loss: 0.6289 (0.9742)  loss_cal: 0.0375 (0.0348)  rel_ce_loss: 0.5650 (1.1573)  1_CE_loss: 0.0022 (0.0444)  2_CE_loss: 0.0213 (0.0845)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0217 (0.1458)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2374 (0.6027)  4_DKS_loss: 0.0009 (0.0203)  5_CE_loss: 0.3316 (0.7193)  5_DKS_loss: 0.0026 (0.1088)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0942 (1.1554)  data: 0.0960 (0.1763)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:16:23  iter: 11900  loss: 4.1999 (12.3046)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3022 (6.4705)  loss_bg: 0.8344 (5.8341)  time: 1.0617 (1.1178)  data: 0.1008 (0.1768)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:18:56  iter: 11900  loss: 1.9405 (3.8869)  obj_loss: 0.6064 (0.9713)  loss_cal: 0.0371 (0.0349)  rel_ce_loss: 0.5846 (1.1526)  1_CE_loss: 0.0027 (0.0441)  2_CE_loss: 0.0395 (0.0841)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0278 (0.1449)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2734 (0.5998)  4_DKS_loss: 0.0008 (0.0201)  5_CE_loss: 0.3617 (0.7161)  5_DKS_loss: 0.0023 (0.1079)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0812 (1.1553)  data: 0.0917 (0.1762)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:---Total norm 8.82392 clip coef 0.56664-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 3.86980, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : 3.66322, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 2.32764, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 2.25219, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 1.97108, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.82141, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 1.59217, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : 1.51446, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 1.39908, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.31884, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 1.12128, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.09450, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.09450, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.09450, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.09450, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.09148, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.98493, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.97490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.97120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.93175, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.86315, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.86232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.76070, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.76002, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.72466, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.68356, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.61236, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.59099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.56531, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.52772, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.48456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.47252, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.45743, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.43474, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.43034, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.41951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.37612, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.36680, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.36406, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.36165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.35837, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.34729, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.34099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.33952, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.33556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.33488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.32571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.32251, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.31387, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.31097, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.30235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.30201, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.29850, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29530, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29492, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29180, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29144, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29114, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29058, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.28983, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28928, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.28585, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.28149, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27553, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.27231, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26500, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.26273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25676, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : 0.25533, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : 0.25533, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25432, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25350, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.24469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24381, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23871, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23830, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23772, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23647, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23593, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23161, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23136, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23132, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22774, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22726, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22328, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22102, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21867, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21852, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21621, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.21361, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21320, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21134, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21100, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20852, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20744, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20700, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20675, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.20596, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20493, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20262, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19980, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19559, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19326, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18926, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18920, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17815, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17619, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17552, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16924, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16252, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16206, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16134, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16114, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15793, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15617, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15098, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14971, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14571, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14533, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.14375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14156, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14140, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.14014, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.12471, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.12149, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12074, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12013, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11891, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11860, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11572, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11510, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11455, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11122, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10927, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10839, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10731, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10679, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10630, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10579, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10515, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10433, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10304, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10256, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10000, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09710, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09666, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09647, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09491, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09450, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.09371, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09316, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09266, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08642, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.08514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08440, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.08289, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.08279, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08207, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08147, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.08038, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07814, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07781, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07772, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07772, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07514, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07170, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06991, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06554, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06519, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06495, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06246, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05849, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05849, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05814, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05806, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.05781, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.05738, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05070, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.04883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.04702, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.04656, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04529, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04462, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03978, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03916, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03872, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03856, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03769, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03640, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03470, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03403, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03327, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03279, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03274, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03032, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02989, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02886, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.02769, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02706, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02706, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02662, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.02629, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02623, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02603, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02505, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02478, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02452, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02326, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02321, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02313, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02197, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02190, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.02178, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02168, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02166, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02104, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02104, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02102, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.02051, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02040, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02035, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02026, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02024, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.01999, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01995, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01981, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01972, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01959, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01954, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01949, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01949, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01917, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01909, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01909, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01897, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01894, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01892, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01889, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01872, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01867, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01864, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01855, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01842, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01836, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01831, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01826, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01817, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01801, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01788, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01787, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01782, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01766, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01764, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01740, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01728, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01726, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01717, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01712, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01702, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01643, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01630, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01621, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01594, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01586, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01548, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01523, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01497, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01424, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01411, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01336, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01276, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01229, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01069, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01007, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00982, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00958, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00956, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00878, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00865, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00842, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00830, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00827, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00821, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00817, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00816, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00816, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00799, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00794, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00792, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00771, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00768, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00759, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00744, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00735, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00715, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00715, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00684, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00681, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00680, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00674, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00656, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00652, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00638, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00634, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00624, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00619, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00619, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00618, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00592, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00587, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00585, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00568, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00559, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00554, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00534, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00524, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00479, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00479, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00438, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00436, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00429, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00419, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00415, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00372, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00371, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00356, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00266, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00259, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00244, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00243, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00194, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00158, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00106, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00090, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00032, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1:14:30  iter: 12000  loss: 4.3312 (12.2374)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4512 (6.4446)  loss_bg: 0.8677 (5.7928)  time: 1.0679 (1.1176)  data: 0.0968 (0.1765)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:---Total norm 4.02474 clip coef 1.24232-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.92913, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.62648, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.19104, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.01507, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 0.98505, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.82167, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.64628, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.64354, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.56529, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.40437, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.39309, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.39051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.39051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.39051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.39051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.37980, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.37474, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.35971, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.35015, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.32796, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.32320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.30261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.29982, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29659, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.26934, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25743, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.25058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24934, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23516, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23511, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.23184, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23069, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22940, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22478, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22220, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22123, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21543, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.21442, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21182, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21096, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20735, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20720, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20472, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.20354, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20348, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.20133, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19328, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19237, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18653, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18634, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.17785, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17591, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17330, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17274, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16941, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16383, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16380, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16285, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15785, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15541, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15387, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14735, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14654, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14573, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14493, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14099, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13045, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13042, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12979, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12811, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.12785, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12728, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12579, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12361, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12243, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12221, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12201, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12177, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.12173, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12151, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11951, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11763, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11708, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11699, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11672, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11617, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11564, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.11539, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11539, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11535, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11408, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11217, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11189, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.11163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11111, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11066, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.10976, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10945, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10929, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10877, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10725, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.10514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10298, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10175, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09811, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09775, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09739, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.09690, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09431, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09357, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08999, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08992, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08957, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08892, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08792, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.08382, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08193, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08181, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08164, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07916, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07746, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07732, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07717, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.07403, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07309, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07207, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07169, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07139, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.06922, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06875, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.06795, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06752, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06745, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06603, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06570, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06566, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06511, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06431, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06199, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06116, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06027, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06022, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06006, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05845, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.05686, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.05686, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05545, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05347, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05259, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05257, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05236, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.05028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04999, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04969, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04835, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04790, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.04728, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.04687, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04682, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04608, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04333, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04135, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04035, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03939, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03937, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03832, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03832, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03832, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03832, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.03828, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03722, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03640, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03582, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03532, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.03293, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.03202, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.03202, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03185, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03176, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03068, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03020, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02999, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02995, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.02980, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02954, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02909, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02870, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02837, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02807, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02643, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02570, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02477, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02377, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02200, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.02194, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02183, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02103, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02064, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02064, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02035, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02022, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02006, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01996, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01995, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01987, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01983, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01981, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01981, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01968, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01966, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01951, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01944, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01900, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01890, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01889, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01885, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01873, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01869, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01862, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01845, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01844, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01825, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01805, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01780, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01759, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01757, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01747, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01732, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01691, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01643, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01559, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01481, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01439, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01291, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01263, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01227, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01216, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01201, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01197, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01170, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01170, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01166, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01144, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01130, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01130, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01053, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01005, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01004, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00995, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00995, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00983, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00980, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00980, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00927, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00927, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00927, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00917, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00914, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00912, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00910, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00910, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00905, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00903, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00887, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00884, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00884, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00882, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00882, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00878, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00831, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00828, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00799, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00797, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00790, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00780, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00763, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00759, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00758, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00752, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00750, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00738, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00735, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00690, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00683, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00679, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00674, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00668, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00665, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00658, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00657, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00642, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00629, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00628, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00623, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00619, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00617, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00589, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00574, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00568, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00567, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00566, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00566, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00559, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00559, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00547, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00545, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00537, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00534, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00517, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00505, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00495, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00480, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00479, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00467, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00442, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00420, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00391, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00361, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00357, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00344, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00335, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00325, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00323, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00314, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00311, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00309, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00305, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00305, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00297, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00282, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00281, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00274, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00271, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00269, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00260, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00246, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00227, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00212, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00205, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00190, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00184, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00183, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00183, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00167, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00147, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00115, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00111, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00055, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00050, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00037, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00021, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1:16:59  iter: 12000  loss: 1.9405 (3.8707)  obj_loss: 0.6353 (0.9684)  loss_cal: 0.0387 (0.0349)  rel_ce_loss: 0.5584 (1.1478)  1_CE_loss: 0.0052 (0.0439)  2_CE_loss: 0.0309 (0.0837)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0355 (0.1440)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2692 (0.5970)  4_DKS_loss: 0.0008 (0.0200)  5_CE_loss: 0.3407 (0.7130)  5_DKS_loss: 0.0023 (0.1071)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0992 (1.1549)  data: 0.0928 (0.1758)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:12:37  iter: 12100  loss: 4.0216 (12.1708)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0978 (6.4188)  loss_bg: 0.8059 (5.7520)  time: 1.0429 (1.1172)  data: 0.0932 (0.1761)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:15:02  iter: 12100  loss: 1.9273 (3.8547)  obj_loss: 0.5747 (0.9655)  loss_cal: 0.0377 (0.0349)  rel_ce_loss: 0.5781 (1.1432)  1_CE_loss: 0.0065 (0.0436)  2_CE_loss: 0.0428 (0.0834)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0335 (0.1431)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2355 (0.5942)  4_DKS_loss: 0.0009 (0.0198)  5_CE_loss: 0.3207 (0.7099)  5_DKS_loss: 0.0023 (0.1062)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0954 (1.1545)  data: 0.0924 (0.1754)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:10:43  iter: 12200  loss: 4.1943 (12.1058)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2161 (6.3935)  loss_bg: 0.9244 (5.7122)  time: 1.0318 (1.1168)  data: 0.0938 (0.1757)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:13:05  iter: 12200  loss: 1.9952 (3.8392)  obj_loss: 0.5732 (0.9628)  loss_cal: 0.0375 (0.0349)  rel_ce_loss: 0.6212 (1.1386)  1_CE_loss: 0.0096 (0.0433)  2_CE_loss: 0.0338 (0.0830)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0378 (0.1422)  3_DKS_loss: 0.0003 (0.0079)  4_CE_loss: 0.2678 (0.5916)  4_DKS_loss: 0.0009 (0.0196)  5_CE_loss: 0.3572 (0.7069)  5_DKS_loss: 0.0028 (0.1053)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0632 (1.1541)  data: 0.0938 (0.1750)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:08:51  iter: 12300  loss: 4.4253 (12.0422)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.4656 (6.3692)  loss_bg: 0.8796 (5.6730)  time: 1.0339 (1.1165)  data: 0.0920 (0.1754)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:11:08  iter: 12300  loss: 1.8662 (3.8235)  obj_loss: 0.6094 (0.9600)  loss_cal: 0.0369 (0.0350)  rel_ce_loss: 0.5641 (1.1341)  1_CE_loss: 0.0053 (0.0431)  2_CE_loss: 0.0283 (0.0826)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0324 (0.1413)  3_DKS_loss: 0.0003 (0.0079)  4_CE_loss: 0.2472 (0.5888)  4_DKS_loss: 0.0009 (0.0195)  5_CE_loss: 0.3255 (0.7038)  5_DKS_loss: 0.0024 (0.1045)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0514 (1.1537)  data: 0.0918 (0.1747)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:06:57  iter: 12400  loss: 4.0566 (11.9780)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1919 (6.3438)  loss_bg: 0.8289 (5.6342)  time: 1.0528 (1.1160)  data: 0.0933 (0.1749)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:09:12  iter: 12400  loss: 1.9486 (3.8082)  obj_loss: 0.5781 (0.9572)  loss_cal: 0.0369 (0.0350)  rel_ce_loss: 0.5991 (1.1296)  1_CE_loss: 0.0161 (0.0428)  2_CE_loss: 0.0369 (0.0822)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0376 (0.1405)  3_DKS_loss: 0.0003 (0.0078)  4_CE_loss: 0.2661 (0.5863)  4_DKS_loss: 0.0010 (0.0193)  5_CE_loss: 0.3573 (0.7009)  5_DKS_loss: 0.0028 (0.1037)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1062 (1.1535)  data: 0.0980 (0.1744)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:05:05  iter: 12500  loss: 4.0475 (11.9154)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3019 (6.3192)  loss_bg: 0.8218 (5.5962)  time: 1.0418 (1.1158)  data: 0.0950 (0.1747)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:07:16  iter: 12500  loss: 1.9633 (3.7933)  obj_loss: 0.6865 (0.9545)  loss_cal: 0.0369 (0.0350)  rel_ce_loss: 0.5968 (1.1253)  1_CE_loss: 0.0023 (0.0425)  2_CE_loss: 0.0339 (0.0819)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0298 (0.1396)  3_DKS_loss: 0.0003 (0.0077)  4_CE_loss: 0.2692 (0.5837)  4_DKS_loss: 0.0010 (0.0192)  5_CE_loss: 0.3328 (0.6981)  5_DKS_loss: 0.0027 (0.1029)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0784 (1.1532)  data: 0.0928 (0.1742)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:03:12  iter: 12600  loss: 4.0236 (11.8534)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0269 (6.2945)  loss_bg: 0.8641 (5.5589)  time: 1.0553 (1.1155)  data: 0.0926 (0.1745)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:05:19  iter: 12600  loss: 2.0071 (3.7786)  obj_loss: 0.6167 (0.9519)  loss_cal: 0.0367 (0.0350)  rel_ce_loss: 0.5790 (1.1210)  1_CE_loss: 0.0027 (0.0423)  2_CE_loss: 0.0227 (0.0815)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0269 (0.1388)  3_DKS_loss: 0.0003 (0.0077)  4_CE_loss: 0.2545 (0.5812)  4_DKS_loss: 0.0009 (0.0191)  5_CE_loss: 0.3470 (0.6952)  5_DKS_loss: 0.0025 (0.1021)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0665 (1.1529)  data: 0.0953 (0.1740)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 1:01:21  iter: 12700  loss: 4.1801 (11.7935)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2836 (6.2715)  loss_bg: 0.8522 (5.5219)  time: 1.0525 (1.1156)  data: 0.0970 (0.1745)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:03:24  iter: 12700  loss: 1.8420 (3.7637)  obj_loss: 0.5645 (0.9491)  loss_cal: 0.0379 (0.0350)  rel_ce_loss: 0.5791 (1.1168)  1_CE_loss: 0.0103 (0.0421)  2_CE_loss: 0.0375 (0.0811)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0230 (0.1380)  3_DKS_loss: 0.0003 (0.0076)  4_CE_loss: 0.2468 (0.5786)  4_DKS_loss: 0.0009 (0.0189)  5_CE_loss: 0.3182 (0.6923)  5_DKS_loss: 0.0027 (0.1013)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0657 (1.1528)  data: 0.0919 (0.1739)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:59:29  iter: 12800  loss: 4.0150 (11.7336)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1972 (6.2479)  loss_bg: 0.8155 (5.4857)  time: 1.0519 (1.1156)  data: 0.0931 (0.1744)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 1:01:28  iter: 12800  loss: 2.0093 (3.7494)  obj_loss: 0.6162 (0.9466)  loss_cal: 0.0371 (0.0351)  rel_ce_loss: 0.5724 (1.1126)  1_CE_loss: 0.0064 (0.0418)  2_CE_loss: 0.0309 (0.0808)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0299 (0.1372)  3_DKS_loss: 0.0003 (0.0076)  4_CE_loss: 0.2683 (0.5761)  4_DKS_loss: 0.0010 (0.0188)  5_CE_loss: 0.3621 (0.6896)  5_DKS_loss: 0.0026 (0.1005)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1009 (1.1527)  data: 0.0937 (0.1737)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:57:37  iter: 12900  loss: 4.1086 (11.6747)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3351 (6.2247)  loss_bg: 0.8799 (5.4500)  time: 1.0511 (1.1153)  data: 0.0958 (0.1742)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:59:33  iter: 12900  loss: 1.8686 (3.7349)  obj_loss: 0.5825 (0.9440)  loss_cal: 0.0373 (0.0351)  rel_ce_loss: 0.5845 (1.1084)  1_CE_loss: 0.0037 (0.0416)  2_CE_loss: 0.0310 (0.0804)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0350 (0.1364)  3_DKS_loss: 0.0003 (0.0075)  4_CE_loss: 0.2298 (0.5736)  4_DKS_loss: 0.0008 (0.0186)  5_CE_loss: 0.3120 (0.6868)  5_DKS_loss: 0.0024 (0.0998)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0437 (1.1527)  data: 0.0948 (0.1737)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:55:45  iter: 13000  loss: 4.0948 (11.6169)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2893 (6.2021)  loss_bg: 0.8458 (5.4149)  time: 1.0361 (1.1151)  data: 0.0927 (0.1740)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:57:36  iter: 13000  loss: 1.8056 (3.7207)  obj_loss: 0.6079 (0.9414)  loss_cal: 0.0382 (0.0351)  rel_ce_loss: 0.5573 (1.1043)  1_CE_loss: 0.0068 (0.0413)  2_CE_loss: 0.0242 (0.0801)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0257 (0.1356)  3_DKS_loss: 0.0003 (0.0075)  4_CE_loss: 0.2325 (0.5712)  4_DKS_loss: 0.0008 (0.0185)  5_CE_loss: 0.3124 (0.6841)  5_DKS_loss: 0.0024 (0.0990)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0648 (1.1523)  data: 0.0924 (0.1733)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:53:52  iter: 13100  loss: 3.9688 (11.5600)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0980 (6.1799)  loss_bg: 0.7861 (5.3802)  time: 1.0462 (1.1147)  data: 0.0938 (0.1737)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:55:41  iter: 13100  loss: 2.0335 (3.7073)  obj_loss: 0.6104 (0.9390)  loss_cal: 0.0369 (0.0351)  rel_ce_loss: 0.6281 (1.1004)  1_CE_loss: 0.0096 (0.0411)  2_CE_loss: 0.0330 (0.0797)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0326 (0.1348)  3_DKS_loss: 0.0003 (0.0074)  4_CE_loss: 0.2599 (0.5688)  4_DKS_loss: 0.0008 (0.0184)  5_CE_loss: 0.3292 (0.6815)  5_DKS_loss: 0.0024 (0.0983)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0962 (1.1521)  data: 0.0932 (0.1731)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:52:00  iter: 13200  loss: 4.2061 (11.5040)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2071 (6.1579)  loss_bg: 0.8107 (5.3460)  time: 1.0477 (1.1146)  data: 0.0943 (0.1735)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:53:45  iter: 13200  loss: 1.9404 (3.6936)  obj_loss: 0.5933 (0.9365)  loss_cal: 0.0374 (0.0351)  rel_ce_loss: 0.5786 (1.0964)  1_CE_loss: 0.0075 (0.0409)  2_CE_loss: 0.0373 (0.0794)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0306 (0.1341)  3_DKS_loss: 0.0004 (0.0073)  4_CE_loss: 0.2612 (0.5665)  4_DKS_loss: 0.0010 (0.0182)  5_CE_loss: 0.3492 (0.6789)  5_DKS_loss: 0.0023 (0.0976)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0539 (1.1519)  data: 0.0913 (0.1729)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:50:08  iter: 13300  loss: 4.1754 (11.4486)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2501 (6.1361)  loss_bg: 0.8639 (5.3124)  time: 1.0268 (1.1143)  data: 0.0899 (0.1732)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:51:49  iter: 13300  loss: 1.8971 (3.6803)  obj_loss: 0.5674 (0.9341)  loss_cal: 0.0370 (0.0351)  rel_ce_loss: 0.6021 (1.0926)  1_CE_loss: 0.0042 (0.0407)  2_CE_loss: 0.0305 (0.0791)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0304 (0.1333)  3_DKS_loss: 0.0003 (0.0073)  4_CE_loss: 0.2426 (0.5641)  4_DKS_loss: 0.0009 (0.0181)  5_CE_loss: 0.3122 (0.6763)  5_DKS_loss: 0.0025 (0.0968)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0860 (1.1515)  data: 0.0932 (0.1724)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:48:16  iter: 13400  loss: 4.1032 (11.3938)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3238 (6.1145)  loss_bg: 0.8754 (5.2792)  time: 1.0492 (1.1139)  data: 0.0907 (0.1728)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:49:53  iter: 13400  loss: 1.8943 (3.6671)  obj_loss: 0.6855 (0.9318)  loss_cal: 0.0374 (0.0352)  rel_ce_loss: 0.5551 (1.0887)  1_CE_loss: 0.0031 (0.0405)  2_CE_loss: 0.0296 (0.0787)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0371 (0.1326)  3_DKS_loss: 0.0003 (0.0072)  4_CE_loss: 0.2245 (0.5619)  4_DKS_loss: 0.0009 (0.0180)  5_CE_loss: 0.2930 (0.6737)  5_DKS_loss: 0.0024 (0.0961)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0955 (1.1512)  data: 0.0942 (0.1722)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:46:23  iter: 13500  loss: 3.9941 (11.3399)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0461 (6.0932)  loss_bg: 0.8316 (5.2466)  time: 1.0216 (1.1135)  data: 0.0946 (0.1725)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:47:56  iter: 13500  loss: 2.0213 (3.6542)  obj_loss: 0.6270 (0.9294)  loss_cal: 0.0376 (0.0352)  rel_ce_loss: 0.6032 (1.0850)  1_CE_loss: 0.0050 (0.0402)  2_CE_loss: 0.0375 (0.0784)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0374 (0.1319)  3_DKS_loss: 0.0003 (0.0072)  4_CE_loss: 0.2674 (0.5596)  4_DKS_loss: 0.0008 (0.0178)  5_CE_loss: 0.3480 (0.6712)  5_DKS_loss: 0.0024 (0.0954)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0742 (1.1508)  data: 0.0932 (0.1717)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:44:31  iter: 13600  loss: 3.8978 (11.2871)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1712 (6.0726)  loss_bg: 0.8056 (5.2145)  time: 1.0312 (1.1131)  data: 0.0911 (0.1720)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:46:00  iter: 13600  loss: 1.8516 (3.6416)  obj_loss: 0.5659 (0.9271)  loss_cal: 0.0376 (0.0352)  rel_ce_loss: 0.5723 (1.0813)  1_CE_loss: 0.0091 (0.0400)  2_CE_loss: 0.0307 (0.0782)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0281 (0.1312)  3_DKS_loss: 0.0003 (0.0071)  4_CE_loss: 0.2414 (0.5575)  4_DKS_loss: 0.0008 (0.0177)  5_CE_loss: 0.3369 (0.6688)  5_DKS_loss: 0.0025 (0.0948)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0849 (1.1503)  data: 0.0908 (0.1713)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:42:39  iter: 13700  loss: 4.0114 (11.2351)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0295 (6.0522)  loss_bg: 0.8921 (5.1828)  time: 1.0359 (1.1126)  data: 0.0912 (0.1716)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:40:47  iter: 13800  loss: 4.2970 (11.1831)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.3788 (6.0315)  loss_bg: 0.8555 (5.1516)  time: 1.0187 (1.1124)  data: 0.0920 (0.1714)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:44:05  iter: 13700  loss: 1.9320 (3.6290)  obj_loss: 0.5933 (0.9248)  loss_cal: 0.0381 (0.0352)  rel_ce_loss: 0.6121 (1.0776)  1_CE_loss: 0.0097 (0.0398)  2_CE_loss: 0.0329 (0.0779)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0299 (0.1305)  3_DKS_loss: 0.0003 (0.0071)  4_CE_loss: 0.2576 (0.5554)  4_DKS_loss: 0.0009 (0.0176)  5_CE_loss: 0.3384 (0.6664)  5_DKS_loss: 0.0025 (0.0941)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0785 (1.1501)  data: 0.0922 (0.1711)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:38:55  iter: 13900  loss: 4.2143 (11.1324)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2546 (6.0116)  loss_bg: 0.8026 (5.1208)  time: 1.0541 (1.1122)  data: 0.0894 (0.1711)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:42:09  iter: 13800  loss: 1.8418 (3.6166)  obj_loss: 0.5938 (0.9226)  loss_cal: 0.0380 (0.0352)  rel_ce_loss: 0.5662 (1.0740)  1_CE_loss: 0.0056 (0.0396)  2_CE_loss: 0.0347 (0.0776)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0299 (0.1298)  3_DKS_loss: 0.0003 (0.0070)  4_CE_loss: 0.2272 (0.5532)  4_DKS_loss: 0.0009 (0.0175)  5_CE_loss: 0.2911 (0.6640)  5_DKS_loss: 0.0023 (0.0934)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0867 (1.1497)  data: 0.0916 (0.1708)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:37:03  iter: 14000  loss: 4.0328 (11.0818)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0201 (5.9913)  loss_bg: 0.8067 (5.0905)  time: 1.0366 (1.1119)  data: 0.0946 (0.1708)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:40:14  iter: 13900  loss: 1.9276 (3.6042)  obj_loss: 0.5874 (0.9204)  loss_cal: 0.0382 (0.0353)  rel_ce_loss: 0.5818 (1.0704)  1_CE_loss: 0.0081 (0.0394)  2_CE_loss: 0.0325 (0.0773)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0375 (0.1291)  3_DKS_loss: 0.0003 (0.0070)  4_CE_loss: 0.2317 (0.5510)  4_DKS_loss: 0.0009 (0.0174)  5_CE_loss: 0.3391 (0.6616)  5_DKS_loss: 0.0024 (0.0928)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0801 (1.1496)  data: 0.0920 (0.1706)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:35:11  iter: 14100  loss: 4.0061 (11.0322)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1620 (5.9716)  loss_bg: 0.8050 (5.0605)  time: 1.0532 (1.1116)  data: 0.0896 (0.1704)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:38:18  iter: 14000  loss: 1.8895 (3.5919)  obj_loss: 0.6030 (0.9182)  loss_cal: 0.0369 (0.0353)  rel_ce_loss: 0.5660 (1.0669)  1_CE_loss: 0.0069 (0.0392)  2_CE_loss: 0.0375 (0.0769)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0312 (0.1284)  3_DKS_loss: 0.0003 (0.0069)  4_CE_loss: 0.2629 (0.5489)  4_DKS_loss: 0.0009 (0.0172)  5_CE_loss: 0.3283 (0.6594)  5_DKS_loss: 0.0022 (0.0921)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0876 (1.1493)  data: 0.0978 (0.1703)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:33:20  iter: 14200  loss: 4.2983 (10.9838)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2331 (5.9526)  loss_bg: 0.8890 (5.0312)  time: 1.0444 (1.1114)  data: 0.0908 (0.1703)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:36:23  iter: 14100  loss: 1.9343 (3.5798)  obj_loss: 0.6133 (0.9160)  loss_cal: 0.0382 (0.0353)  rel_ce_loss: 0.5806 (1.0633)  1_CE_loss: 0.0021 (0.0390)  2_CE_loss: 0.0344 (0.0766)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0288 (0.1277)  3_DKS_loss: 0.0003 (0.0069)  4_CE_loss: 0.2669 (0.5468)  4_DKS_loss: 0.0009 (0.0171)  5_CE_loss: 0.3279 (0.6570)  5_DKS_loss: 0.0024 (0.0915)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0554 (1.1491)  data: 0.0938 (0.1701)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:31:28  iter: 14300  loss: 3.8331 (10.9360)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0216 (5.9340)  loss_bg: 0.8592 (5.0020)  time: 1.0463 (1.1112)  data: 0.0908 (0.1701)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:34:27  iter: 14200  loss: 1.8125 (3.5680)  obj_loss: 0.5811 (0.9137)  loss_cal: 0.0382 (0.0353)  rel_ce_loss: 0.5501 (1.0599)  1_CE_loss: 0.0097 (0.0388)  2_CE_loss: 0.0242 (0.0764)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0236 (0.1271)  3_DKS_loss: 0.0003 (0.0069)  4_CE_loss: 0.2318 (0.5448)  4_DKS_loss: 0.0008 (0.0170)  5_CE_loss: 0.3094 (0.6548)  5_DKS_loss: 0.0023 (0.0909)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0809 (1.1489)  data: 0.0926 (0.1699)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:29:37  iter: 14400  loss: 4.0471 (10.8884)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2443 (5.9151)  loss_bg: 0.8246 (4.9734)  time: 1.0406 (1.1111)  data: 0.0936 (0.1700)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:32:32  iter: 14300  loss: 1.8814 (3.5562)  obj_loss: 0.5786 (0.9115)  loss_cal: 0.0371 (0.0353)  rel_ce_loss: 0.5355 (1.0565)  1_CE_loss: 0.0061 (0.0386)  2_CE_loss: 0.0306 (0.0761)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0273 (0.1264)  3_DKS_loss: 0.0003 (0.0068)  4_CE_loss: 0.2570 (0.5428)  4_DKS_loss: 0.0009 (0.0169)  5_CE_loss: 0.3396 (0.6526)  5_DKS_loss: 0.0024 (0.0902)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0488 (1.1486)  data: 0.0921 (0.1697)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:27:46  iter: 14500  loss: 3.8919 (10.8411)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9895 (5.8961)  loss_bg: 0.7809 (4.9451)  time: 1.0420 (1.1108)  data: 0.0911 (0.1698)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:30:37  iter: 14400  loss: 1.8351 (3.5450)  obj_loss: 0.5879 (0.9094)  loss_cal: 0.0365 (0.0353)  rel_ce_loss: 0.5678 (1.0533)  1_CE_loss: 0.0106 (0.0384)  2_CE_loss: 0.0274 (0.0758)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0325 (0.1258)  3_DKS_loss: 0.0003 (0.0068)  4_CE_loss: 0.2420 (0.5408)  4_DKS_loss: 0.0009 (0.0168)  5_CE_loss: 0.3217 (0.6505)  5_DKS_loss: 0.0022 (0.0896)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.1484)  data: 0.0907 (0.1695)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:25:54  iter: 14600  loss: 3.8639 (10.7938)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9018 (5.8767)  loss_bg: 0.8673 (4.9171)  time: 1.0454 (1.1104)  data: 0.0951 (0.1694)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:28:41  iter: 14500  loss: 1.8141 (3.5332)  obj_loss: 0.5869 (0.9072)  loss_cal: 0.0375 (0.0354)  rel_ce_loss: 0.5698 (1.0499)  1_CE_loss: 0.0075 (0.0382)  2_CE_loss: 0.0318 (0.0755)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0304 (0.1252)  3_DKS_loss: 0.0003 (0.0067)  4_CE_loss: 0.2179 (0.5388)  4_DKS_loss: 0.0009 (0.0167)  5_CE_loss: 0.3300 (0.6482)  5_DKS_loss: 0.0025 (0.0890)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0767 (1.1480)  data: 0.0941 (0.1691)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:24:03  iter: 14700  loss: 3.7822 (10.7474)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9760 (5.8580)  loss_bg: 0.8062 (4.8894)  time: 1.0392 (1.1101)  data: 0.0924 (0.1691)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:26:46  iter: 14600  loss: 1.7878 (3.5213)  obj_loss: 0.5269 (0.9048)  loss_cal: 0.0380 (0.0354)  rel_ce_loss: 0.5625 (1.0465)  1_CE_loss: 0.0036 (0.0380)  2_CE_loss: 0.0261 (0.0752)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0206 (0.1245)  3_DKS_loss: 0.0003 (0.0067)  4_CE_loss: 0.2556 (0.5368)  4_DKS_loss: 0.0009 (0.0166)  5_CE_loss: 0.3370 (0.6460)  5_DKS_loss: 0.0025 (0.0884)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0623 (1.1478)  data: 0.0930 (0.1689)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:22:11  iter: 14800  loss: 3.9863 (10.7020)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0519 (5.8398)  loss_bg: 0.8793 (4.8622)  time: 1.0516 (1.1100)  data: 0.1004 (0.1690)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:24:51  iter: 14700  loss: 1.7219 (3.5099)  obj_loss: 0.5913 (0.9025)  loss_cal: 0.0374 (0.0354)  rel_ce_loss: 0.5584 (1.0432)  1_CE_loss: 0.0032 (0.0378)  2_CE_loss: 0.0251 (0.0750)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0238 (0.1239)  3_DKS_loss: 0.0003 (0.0066)  4_CE_loss: 0.2428 (0.5349)  4_DKS_loss: 0.0009 (0.0165)  5_CE_loss: 0.3171 (0.6439)  5_DKS_loss: 0.0022 (0.0879)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0938 (1.1476)  data: 0.0958 (0.1687)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:20:20  iter: 14900  loss: 3.8279 (10.6569)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0511 (5.8217)  loss_bg: 0.8096 (4.8353)  time: 1.0316 (1.1097)  data: 0.0924 (0.1687)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:22:56  iter: 14800  loss: 1.8142 (3.4987)  obj_loss: 0.5889 (0.9004)  loss_cal: 0.0376 (0.0354)  rel_ce_loss: 0.5342 (1.0400)  1_CE_loss: 0.0073 (0.0376)  2_CE_loss: 0.0332 (0.0747)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0316 (0.1233)  3_DKS_loss: 0.0003 (0.0066)  4_CE_loss: 0.2325 (0.5330)  4_DKS_loss: 0.0009 (0.0164)  5_CE_loss: 0.3237 (0.6417)  5_DKS_loss: 0.0027 (0.0873)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0916 (1.1473)  data: 0.0958 (0.1684)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:18:29  iter: 15000  loss: 3.7784 (10.6123)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9039 (5.8035)  loss_bg: 0.7656 (4.8088)  time: 1.0252 (1.1093)  data: 0.0930 (0.1684)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:21:01  iter: 14900  loss: 1.8009 (3.4874)  obj_loss: 0.5112 (0.8981)  loss_cal: 0.0383 (0.0354)  rel_ce_loss: 0.5827 (1.0367)  1_CE_loss: 0.0023 (0.0374)  2_CE_loss: 0.0339 (0.0745)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0244 (0.1227)  3_DKS_loss: 0.0003 (0.0065)  4_CE_loss: 0.2411 (0.5311)  4_DKS_loss: 0.0009 (0.0162)  5_CE_loss: 0.3082 (0.6396)  5_DKS_loss: 0.0026 (0.0867)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0619 (1.1470)  data: 0.0947 (0.1681)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:16:38  iter: 15100  loss: 4.0138 (10.5682)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1776 (5.7856)  loss_bg: 0.8737 (4.7826)  time: 1.0484 (1.1092)  data: 0.0905 (0.1682)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:19:06  iter: 15000  loss: 1.7463 (3.4762)  obj_loss: 0.5229 (0.8959)  loss_cal: 0.0380 (0.0354)  rel_ce_loss: 0.5369 (1.0335)  1_CE_loss: 0.0070 (0.0373)  2_CE_loss: 0.0304 (0.0742)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0245 (0.1221)  3_DKS_loss: 0.0003 (0.0065)  4_CE_loss: 0.2339 (0.5292)  4_DKS_loss: 0.0008 (0.0161)  5_CE_loss: 0.3108 (0.6375)  5_DKS_loss: 0.0022 (0.0861)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0651 (1.1467)  data: 0.0940 (0.1679)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:14:47  iter: 15200  loss: 3.8533 (10.5247)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0010 (5.7680)  loss_bg: 0.8295 (4.7567)  time: 1.0684 (1.1088)  data: 0.0963 (0.1678)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:17:11  iter: 15100  loss: 1.8254 (3.4654)  obj_loss: 0.5273 (0.8937)  loss_cal: 0.0385 (0.0355)  rel_ce_loss: 0.5409 (1.0304)  1_CE_loss: 0.0099 (0.0371)  2_CE_loss: 0.0338 (0.0739)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0323 (0.1215)  3_DKS_loss: 0.0003 (0.0065)  4_CE_loss: 0.2477 (0.5274)  4_DKS_loss: 0.0009 (0.0160)  5_CE_loss: 0.3039 (0.6354)  5_DKS_loss: 0.0024 (0.0856)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0770 (1.1463)  data: 0.0916 (0.1675)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:12:55  iter: 15300  loss: 3.9755 (10.4812)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.1491 (5.7501)  loss_bg: 0.7780 (4.7311)  time: 1.0301 (1.1085)  data: 0.0902 (0.1675)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:15:16  iter: 15200  loss: 1.7645 (3.4547)  obj_loss: 0.6035 (0.8917)  loss_cal: 0.0380 (0.0355)  rel_ce_loss: 0.5596 (1.0273)  1_CE_loss: 0.0053 (0.0369)  2_CE_loss: 0.0233 (0.0737)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0292 (0.1209)  3_DKS_loss: 0.0003 (0.0064)  4_CE_loss: 0.2140 (0.5255)  4_DKS_loss: 0.0009 (0.0159)  5_CE_loss: 0.3195 (0.6334)  5_DKS_loss: 0.0027 (0.0850)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0712 (1.1460)  data: 0.0919 (0.1672)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:11:04  iter: 15400  loss: 3.7304 (10.4387)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9969 (5.7329)  loss_bg: 0.7993 (4.7058)  time: 1.0281 (1.1083)  data: 0.0926 (0.1674)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:13:22  iter: 15300  loss: 1.8302 (3.4442)  obj_loss: 0.5752 (0.8896)  loss_cal: 0.0381 (0.0355)  rel_ce_loss: 0.5369 (1.0243)  1_CE_loss: 0.0104 (0.0367)  2_CE_loss: 0.0322 (0.0734)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0282 (0.1204)  3_DKS_loss: 0.0003 (0.0064)  4_CE_loss: 0.2678 (0.5237)  4_DKS_loss: 0.0009 (0.0158)  5_CE_loss: 0.3231 (0.6314)  5_DKS_loss: 0.0022 (0.0845)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0762 (1.1460)  data: 0.0955 (0.1671)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:09:14  iter: 15500  loss: 4.0993 (10.3975)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2025 (5.7165)  loss_bg: 0.8145 (4.6810)  time: 1.0568 (1.1081)  data: 0.0917 (0.1672)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:11:27  iter: 15400  loss: 1.8645 (3.4339)  obj_loss: 0.5903 (0.8876)  loss_cal: 0.0372 (0.0355)  rel_ce_loss: 0.5479 (1.0213)  1_CE_loss: 0.0096 (0.0366)  2_CE_loss: 0.0312 (0.0732)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0308 (0.1198)  3_DKS_loss: 0.0004 (0.0063)  4_CE_loss: 0.2431 (0.5220)  4_DKS_loss: 0.0010 (0.0157)  5_CE_loss: 0.3153 (0.6295)  5_DKS_loss: 0.0024 (0.0840)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0794 (1.1457)  data: 0.0912 (0.1669)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:07:23  iter: 15600  loss: 3.8445 (10.3560)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.0369 (5.6995)  loss_bg: 0.7794 (4.6565)  time: 1.0302 (1.1079)  data: 0.0954 (0.1670)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:09:32  iter: 15500  loss: 1.7142 (3.4235)  obj_loss: 0.5327 (0.8856)  loss_cal: 0.0390 (0.0355)  rel_ce_loss: 0.5094 (1.0183)  1_CE_loss: 0.0056 (0.0364)  2_CE_loss: 0.0217 (0.0730)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0245 (0.1192)  3_DKS_loss: 0.0003 (0.0063)  4_CE_loss: 0.2129 (0.5202)  4_DKS_loss: 0.0009 (0.0157)  5_CE_loss: 0.2877 (0.6275)  5_DKS_loss: 0.0023 (0.0834)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0760 (1.1456)  data: 0.0950 (0.1667)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:05:32  iter: 15700  loss: 3.9418 (10.3150)  obj_loss: 0.0000 (0.0000)  loss_fg: 3.2405 (5.6828)  loss_bg: 0.7897 (4.6322)  time: 1.0270 (1.1076)  data: 0.0913 (0.1667)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:07:38  iter: 15600  loss: 1.8334 (3.4132)  obj_loss: 0.5664 (0.8836)  loss_cal: 0.0380 (0.0355)  rel_ce_loss: 0.5497 (1.0153)  1_CE_loss: 0.0035 (0.0362)  2_CE_loss: 0.0255 (0.0727)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0247 (0.1187)  3_DKS_loss: 0.0003 (0.0063)  4_CE_loss: 0.2534 (0.5185)  4_DKS_loss: 0.0009 (0.0156)  5_CE_loss: 0.3282 (0.6255)  5_DKS_loss: 0.0025 (0.0829)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0865 (1.1453)  data: 0.0913 (0.1665)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:03:41  iter: 15800  loss: 3.7112 (10.2748)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.8026 (5.6666)  loss_bg: 0.7684 (4.6082)  time: 1.0383 (1.1073)  data: 0.0913 (0.1664)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:05:43  iter: 15700  loss: 1.8228 (3.4030)  obj_loss: 0.5615 (0.8816)  loss_cal: 0.0384 (0.0356)  rel_ce_loss: 0.5452 (1.0124)  1_CE_loss: 0.0066 (0.0361)  2_CE_loss: 0.0281 (0.0724)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0241 (0.1181)  3_DKS_loss: 0.0003 (0.0062)  4_CE_loss: 0.2388 (0.5167)  4_DKS_loss: 0.0009 (0.0155)  5_CE_loss: 0.3334 (0.6236)  5_DKS_loss: 0.0025 (0.0824)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.1450)  data: 0.0935 (0.1661)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:eta: 0:01:50  iter: 15900  loss: 3.8081 (10.2351)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9556 (5.6505)  loss_bg: 0.8391 (4.5846)  time: 1.0428 (1.1070)  data: 0.0898 (0.1661)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark:eta: 0:03:48  iter: 15800  loss: 1.7728 (3.3929)  obj_loss: 0.5762 (0.8797)  loss_cal: 0.0372 (0.0356)  rel_ce_loss: 0.5310 (1.0095)  1_CE_loss: 0.0031 (0.0359)  2_CE_loss: 0.0225 (0.0722)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0261 (0.1176)  3_DKS_loss: 0.0003 (0.0062)  4_CE_loss: 0.2374 (0.5150)  4_DKS_loss: 0.0009 (0.0154)  5_CE_loss: 0.3325 (0.6217)  5_DKS_loss: 0.0025 (0.0819)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0547 (1.1447)  data: 0.0913 (0.1659)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:---Total norm 13.67121 clip coef 0.36573-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : 5.21302, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 5.19162, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 3.75680, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 3.58579, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 3.45463, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 3.38049, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 2.61577, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 2.61307, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : 2.36326, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 2.22707, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 2.03758, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.61317, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.61317, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.61317, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.61317, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.60463, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.57323, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.42803, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.36584, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 1.26805, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.14765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.13481, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 1.08187, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.07348, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 1.04512, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.03080, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.01973, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.96831, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.89624, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.86230, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.84633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.74911, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.73713, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.73511, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.70752, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.69282, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.67008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.66287, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.66272, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.65676, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.65511, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.64921, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.63953, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.62277, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.61274, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.59222, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.56599, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.54701, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.54375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.52934, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.51104, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.49831, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.49768, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.49404, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.49105, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.49098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.49011, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.48492, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.47533, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.47483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.46981, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.46495, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.46459, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.45788, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.45646, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.45532, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44761, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44721, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.44649, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44520, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44117, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.43750, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.43612, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.43202, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.43150, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.43070, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.42518, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.42446, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.42270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.42235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41911, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41822, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41701, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41465, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.41285, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.40834, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.40565, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.39967, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.39440, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.38190, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.38178, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.37647, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : 0.37221, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : 0.37221, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.37041, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.36669, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.36644, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.36496, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.36437, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.36069, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.35616, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.35520, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.35034, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.34236, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.34164, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33987, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33878, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.33636, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.33574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33368, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33243, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.33139, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.32881, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.31972, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.31600, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29408, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.29403, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.29104, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.28335, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.28320, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.27882, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.26989, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.26094, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25842, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.25805, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.25639, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.25238, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.24390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21282, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20999, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20250, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20117, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20109, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20103, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19946, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19920, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19803, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19798, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19742, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.19725, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19526, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19481, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.19435, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.19322, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19313, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.19295, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19076, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.18833, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.18698, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.18296, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.17617, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.17169, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17154, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.16668, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.16327, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.16200, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.16091, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15108, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15029, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14419, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.13549, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.13445, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.13190, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.13105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13026, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12929, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.12885, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12881, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.12839, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.12739, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12567, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.12407, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.12224, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.11897, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.11897, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11680, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11528, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.11156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.10693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10538, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.10124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.10115, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09423, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.08461, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.08294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07895, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.07895, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.07855, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.07581, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07297, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07254, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.07244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.07241, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.07151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.07021, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06763, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06545, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06509, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.06482, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06478, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.06238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06235, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06126, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06018, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05989, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05819, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.05737, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.05605, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05573, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05539, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05485, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05432, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05337, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05250, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.05073, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05063, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.05051, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04912, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04793, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04791, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.04625, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04530, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04340, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.04155, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04130, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03978, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03885, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03872, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03842, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03830, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.03814, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03748, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03669, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03665, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03656, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03655, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03644, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03631, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03585, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03549, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03533, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03422, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03422, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03354, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03279, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03264, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03248, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03236, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03191, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03190, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03158, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03127, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03057, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03024, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02999, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02978, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02967, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02961, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02896, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02824, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02778, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02723, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02702, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02592, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02559, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02552, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.02508, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02497, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.02387, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02345, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.02240, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02113, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.02112, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02109, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.02107, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02046, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.02043, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02038, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01994, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01955, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01930, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01661, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01637, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01608, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01581, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01575, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01552, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01550, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01489, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01340, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01310, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01305, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01278, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01252, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01241, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01215, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01205, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01185, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01176, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01176, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01164, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01164, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01133, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01111, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01110, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01060, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00983, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00952, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00907, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00877, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00864, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00784, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00734, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00710, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00709, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00682, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00647, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00591, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00585, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00581, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00435, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00434, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00419, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00270, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00236, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00221, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00187, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 0:00:00  iter: 16000  loss: 3.6963 (10.1962)  obj_loss: 0.0000 (0.0000)  loss_fg: 2.9585 (5.6348)  loss_bg: 0.8542 (4.5613)  time: 1.0387 (1.1067)  data: 0.0894 (0.1658)  lr: 0.001600  max mem: 11608
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/NPredictor101-predcls-loss/model_0016000.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/NPredictor101-predcls-loss/model_final.pth
INFO:maskrcnn_benchmark:Total training time: 4:55:11.531667 (1.1070 s / it)
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_test dataset(26446 images).
INFO:maskrcnn_benchmark:eta: 0:01:54  iter: 15900  loss: 1.8178 (3.3830)  obj_loss: 0.5122 (0.8778)  loss_cal: 0.0375 (0.0356)  rel_ce_loss: 0.5739 (1.0066)  1_CE_loss: 0.0119 (0.0357)  2_CE_loss: 0.0374 (0.0720)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0390 (0.1171)  3_DKS_loss: 0.0004 (0.0062)  4_CE_loss: 0.2474 (0.5133)  4_DKS_loss: 0.0010 (0.0153)  5_CE_loss: 0.3208 (0.6198)  5_DKS_loss: 0.0026 (0.0814)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1626 (1.1446)  data: 0.1749 (0.1657)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark:---Total norm 4.43685 clip coef 1.12693-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 2.59451, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 2.04576, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.21504, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.15611, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 0.95108, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.91816, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.64031, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.57693, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.51135, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.40934, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.34881, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.31905, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.28666, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.27737, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27513, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26218, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26044, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24394, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23730, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23712, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23096, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22730, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22724, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.22289, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22169, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21734, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21632, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21203, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20698, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20293, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19904, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19802, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.19751, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19744, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.19110, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18867, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18720, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18270, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.18242, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.17876, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17836, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17671, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.17560, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17307, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17156, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.16247, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16165, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15652, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15559, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15455, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14910, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.14901, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.14848, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14551, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14214, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14000, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13934, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13917, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.13906, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.13786, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13610, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13473, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13425, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13252, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12871, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12728, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12676, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.12465, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11808, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11351, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.11220, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11211, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11172, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10945, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10704, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10512, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10500, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.10491, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10381, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10379, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10355, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10281, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10240, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09955, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09703, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09591, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09553, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09520, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09386, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09079, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09064, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09000, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08951, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08877, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08803, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08456, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08255, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08239, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08228, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07763, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.07584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07504, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07419, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07341, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.07237, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07166, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07036, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06995, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06988, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06816, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.06731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06554, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06471, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06458, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06418, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06070, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06031, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05980, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05862, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05838, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05769, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05730, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05639, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05629, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05567, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05502, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05491, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05415, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05398, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05377, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05368, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05304, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.05212, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.05212, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05195, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05112, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05057, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05017, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.05005, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.04920, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04845, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04810, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.04701, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04574, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04301, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.04276, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04261, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.03908, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03799, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03767, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03650, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03637, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03600, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03413, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03265, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.03232, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.03148, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03074, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02984, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02967, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.02962, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02925, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02870, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02820, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02733, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02701, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.02637, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02599, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02538, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02507, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02408, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02324, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.02303, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.02303, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02229, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02128, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02057, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02016, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01999, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.01979, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01961, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01949, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01947, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01913, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01901, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01893, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01879, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01875, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.01855, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01830, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01830, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01812, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01781, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01763, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.01759, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01737, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01729, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01711, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01678, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01657, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01645, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01629, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01629, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01611, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01599, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01570, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01513, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01508, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01464, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01403, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01390, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01357, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01355, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01299, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01289, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01267, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01241, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01227, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01221, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01201, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01192, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01192, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01189, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01148, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01113, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01098, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01073, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01036, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01022, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01013, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01012, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00976, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00916, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00915, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00915, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00914, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00896, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00846, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.00845, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00827, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00826, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00824, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00819, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00811, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00809, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00797, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00789, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00783, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00781, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00765, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00762, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00760, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00747, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00729, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00716, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00711, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00702, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00698, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00698, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00676, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00675, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00674, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00649, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00647, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00646, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00642, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00636, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00629, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00625, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00605, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00605, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00600, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00595, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00569, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00564, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00542, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00528, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00511, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00510, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00506, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00492, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00472, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00472, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00472, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00462, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00447, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00446, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00441, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00427, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00423, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.00418, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00409, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00406, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00373, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00369, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00326, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00321, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00320, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00315, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00308, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00304, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00301, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00301, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00289, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00283, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00280, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00278, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00278, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00274, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00269, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00254, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00250, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00243, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00232, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00232, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00227, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00222, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00221, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00218, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00200, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00193, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00168, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00151, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00144, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00144, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00139, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00103, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00086, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00059, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00055, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00036, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00022, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00011, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 0:00:00  iter: 16000  loss: 1.8443 (3.3736)  obj_loss: 0.5342 (0.8759)  loss_cal: 0.0371 (0.0356)  rel_ce_loss: 0.5826 (1.0039)  1_CE_loss: 0.0063 (0.0356)  2_CE_loss: 0.0273 (0.0718)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0321 (0.1166)  3_DKS_loss: 0.0004 (0.0061)  4_CE_loss: 0.2419 (0.5117)  4_DKS_loss: 0.0010 (0.0152)  5_CE_loss: 0.3308 (0.6180)  5_DKS_loss: 0.0025 (0.0809)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1184 (1.1448)  data: 0.1711 (0.1659)  lr: 0.001600  max mem: 10734
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-sgcls0.001/model_0016000.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-sgcls0.001/model_final.pth
INFO:maskrcnn_benchmark:Total training time: 5:05:20.124514 (1.1450 s / it)
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_test dataset(26446 images).
INFO:maskrcnn_benchmark:Total run time: 0:26:08.978085 (0.0593276141784848 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:21:51.289042 (0.04958364370865783 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Total run time: 0:25:30.527998 (0.05787370482236113 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:22:00.065006 (0.049915488392107875 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9995
====================================================================================================
SGG eval:     R @ 20: 0.5724;     R @ 50: 0.6425;     R @ 100: 0.6634;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.6488;  ng-R @ 50: 0.7928;  ng-R @ 100: 0.8650;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2183;    zR @ 50: 0.2892;    zR @ 100: 0.3294;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.2578; ng-zR @ 50: 0.3944; ng-zR @ 100: 0.5120;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.1536;    mR @ 50: 0.2007;    mR @ 100: 0.2228;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1768) (across:0.0952) (against:0.0161) (along:0.0902) (and:0.0266) (at:0.4673) (attached to:0.0460) (behind:0.5780) (belonging to:0.0354) (between:0.0139) (carrying:0.3470) (covered in:0.2652) (covering:0.0745) (eating:0.4677) (flying in:0.0000) (for:0.0824) (from:0.0000) (growing on:0.0000) (hanging from:0.0951) (has:0.7855) (holding:0.7185) (in:0.4139) (in front of:0.1940) (laying on:0.1048) (looking at:0.0993) (lying on:0.0000) (made of:0.0312) (mounted on:0.0139) (near:0.3927) (of:0.6892) (on:0.7604) (on back of:0.0142) (over:0.0981) (painted on:0.0172) (parked on:0.2517) (part of:0.0000) (playing:0.0152) (riding:0.5347) (says:0.0000) (sitting on:0.4004) (standing on:0.2344) (to:0.1250) (under:0.3050) (using:0.2300) (walking in:0.0141) (walking on:0.3672) (watching:0.2014) (wearing:0.9346) (wears:0.1370) (with:0.1794) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.1585;    mR @ 50: 0.2092;    mR @ 100: 0.2337;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.1508) (across:0.0854) (against:0.0145) (along:0.0839) (and:0.0231) (at:0.4913) (attached to:0.0513) (behind:0.5807) (belonging to:0.0390) (between:0.0139) (carrying:0.3523) (covered in:0.3010) (covering:0.1317) (eating:0.4142) (flying in:0.0000) (for:0.0819) (from:0.0000) (growing on:0.0000) (hanging from:0.0909) (has:0.8275) (holding:0.7597) (in:0.4831) (in front of:0.1959) (laying on:0.1347) (looking at:0.1089) (lying on:0.0000) (made of:0.0270) (mounted on:0.0118) (near:0.3994) (of:0.7452) (on:0.7917) (on back of:0.0112) (over:0.0937) (painted on:0.0098) (parked on:0.3014) (part of:0.0000) (playing:0.0345) (riding:0.5789) (says:0.0000) (sitting on:0.4278) (standing on:0.2881) (to:0.1176) (under:0.2979) (using:0.2896) (walking in:0.0088) (walking on:0.3704) (watching:0.2270) (wearing:0.9396) (wears:0.1201) (with:0.1775) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.2174; ng-mR @ 50: 0.3518; ng-mR @ 100: 0.4626;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.5506) (across:0.2976) (against:0.1613) (along:0.2905) (and:0.1105) (at:0.7203) (attached to:0.3557) (behind:0.6780) (belonging to:0.3083) (between:0.1389) (carrying:0.7272) (covered in:0.4450) (covering:0.3002) (eating:0.6556) (flying in:0.0000) (for:0.2358) (from:0.0732) (growing on:0.0747) (hanging from:0.4201) (has:0.9017) (holding:0.8415) (in:0.7713) (in front of:0.4449) (laying on:0.6369) (looking at:0.4091) (lying on:0.4158) (made of:0.0000) (mounted on:0.2253) (near:0.6531) (of:0.8705) (on:0.9122) (on back of:0.1879) (over:0.2611) (painted on:0.1638) (parked on:0.7842) (part of:0.1173) (playing:0.0644) (riding:0.9210) (says:0.0833) (sitting on:0.7768) (standing on:0.7195) (to:0.4085) (under:0.5588) (using:0.4871) (walking in:0.2606) (walking on:0.8435) (watching:0.3501) (wearing:0.9710) (wears:0.9155) (with:0.6313) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0924; zs-mR @ 50: 0.1266; zs-mR @ 100: 0.1511;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.2222) (across:0.0556) (against:0.0286) (along:0.0370) (and:0.0703) (at:0.1509) (attached to:0.0279) (behind:0.4608) (belonging to:0.1759) (between:0.0000) (carrying:0.2000) (covered in:0.2500) (covering:0.0000) (eating:0.4265) (flying in:0.0000) (for:0.0375) (from:0.0000) (growing on:0.0000) (hanging from:0.1116) (has:0.5282) (holding:0.3584) (in:0.3906) (in front of:0.1962) (laying on:0.1146) (looking at:0.1500) (lying on:0.0000) (made of:0.0526) (mounted on:0.0000) (near:0.4924) (of:0.3039) (on:0.6503) (on back of:0.0500) (over:0.0575) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0256) (riding:0.0625) (says:0.0000) (sitting on:0.1918) (standing on:0.2184) (to:0.0263) (under:0.3171) (using:0.0000) (walking in:0.0625) (walking on:0.2647) (watching:0.2195) (wearing:0.1748) (wears:0.0891) (with:0.3014) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.5015;     A @ 50: 0.5020;     A @ 100: 0.5020;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.5929
====================================================================================================
SGG eval:     R @ 20: 0.2657;     R @ 50: 0.3251;     R @ 100: 0.3478;  for mode=sgcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2904;  ng-R @ 50: 0.3884;  ng-R @ 100: 0.4491;  for mode=sgcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0778;    zR @ 50: 0.1069;    zR @ 100: 0.1233;  for mode=sgcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0838; ng-zR @ 50: 0.1286; ng-zR @ 100: 0.1746;  for mode=sgcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0498;    mR @ 50: 0.0759;    mR @ 100: 0.0915;  for mode=sgcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0610) (across:0.0000) (against:0.0000) (along:0.0092) (and:0.0000) (at:0.0586) (attached to:0.0094) (behind:0.1790) (belonging to:0.0142) (between:0.0000) (carrying:0.1457) (covered in:0.0411) (covering:0.0194) (eating:0.0572) (flying in:0.0000) (for:0.0096) (from:0.0000) (growing on:0.0000) (hanging from:0.0339) (has:0.4787) (holding:0.2533) (in:0.1514) (in front of:0.0706) (laying on:0.0574) (looking at:0.0552) (lying on:0.0306) (made of:0.0000) (mounted on:0.0278) (near:0.1825) (of:0.3652) (on:0.4011) (on back of:0.0035) (over:0.0718) (painted on:0.0345) (parked on:0.1039) (part of:0.0144) (playing:0.0000) (riding:0.2432) (says:0.0833) (sitting on:0.0878) (standing on:0.0343) (to:0.0082) (under:0.1769) (using:0.1021) (walking in:0.0282) (walking on:0.2002) (watching:0.0645) (wearing:0.5001) (wears:0.0308) (with:0.0763) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0515;    mR @ 50: 0.0794;    mR @ 100: 0.0966;  for mode=sgcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0519) (across:0.0000) (against:0.0000) (along:0.0065) (and:0.0000) (at:0.0628) (attached to:0.0060) (behind:0.1790) (belonging to:0.0135) (between:0.0000) (carrying:0.1485) (covered in:0.0510) (covering:0.0165) (eating:0.1213) (flying in:0.0000) (for:0.0085) (from:0.0000) (growing on:0.0000) (hanging from:0.0246) (has:0.5253) (holding:0.2776) (in:0.1833) (in front of:0.0784) (laying on:0.0599) (looking at:0.0516) (lying on:0.0412) (made of:0.0000) (mounted on:0.0296) (near:0.1858) (of:0.4005) (on:0.4311) (on back of:0.0056) (over:0.0713) (painted on:0.0196) (parked on:0.1027) (part of:0.0141) (playing:0.0000) (riding:0.2672) (says:0.0833) (sitting on:0.0961) (standing on:0.0298) (to:0.0065) (under:0.1754) (using:0.1131) (walking in:0.0177) (walking on:0.1944) (watching:0.0714) (wearing:0.5093) (wears:0.0258) (with:0.0732) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0584; ng-mR @ 50: 0.1077; ng-mR @ 100: 0.1584;  for mode=sgcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1666) (across:0.0317) (against:0.0000) (along:0.0367) (and:0.0296) (at:0.1212) (attached to:0.0376) (behind:0.2706) (belonging to:0.0537) (between:0.0208) (carrying:0.2428) (covered in:0.1185) (covering:0.0484) (eating:0.1047) (flying in:0.0000) (for:0.1093) (from:0.0000) (growing on:0.0000) (hanging from:0.0900) (has:0.5278) (holding:0.3293) (in:0.2331) (in front of:0.1781) (laying on:0.2050) (looking at:0.1452) (lying on:0.1763) (made of:0.0000) (mounted on:0.0573) (near:0.2911) (of:0.4857) (on:0.5013) (on back of:0.0355) (over:0.1459) (painted on:0.0924) (parked on:0.3043) (part of:0.1070) (playing:0.0000) (riding:0.3776) (says:0.0833) (sitting on:0.1929) (standing on:0.1264) (to:0.1066) (under:0.2846) (using:0.1552) (walking in:0.1056) (walking on:0.2743) (watching:0.0630) (wearing:0.5059) (wears:0.1371) (with:0.2097) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0369; zs-mR @ 50: 0.0560; zs-mR @ 100: 0.0690;  for mode=sgcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.1255) (across:0.0000) (against:0.0000) (along:0.0370) (and:0.0000) (at:0.0189) (attached to:0.0164) (behind:0.1374) (belonging to:0.1481) (between:0.0000) (carrying:0.0333) (covered in:0.0000) (covering:0.0333) (eating:0.0000) (flying in:0.0000) (for:0.0250) (from:0.0000) (growing on:0.0000) (hanging from:0.0764) (has:0.1450) (holding:0.0817) (in:0.1157) (in front of:0.1410) (laying on:0.1042) (looking at:0.1139) (lying on:0.0811) (made of:0.0000) (mounted on:0.0400) (near:0.2185) (of:0.0971) (on:0.1592) (on back of:0.0125) (over:0.0840) (painted on:0.0714) (parked on:0.0000) (part of:0.0606) (playing:0.0000) (riding:0.0417) (says:0.1429) (sitting on:0.1089) (standing on:0.1281) (to:0.0000) (under:0.2153) (using:0.0000) (walking in:0.0625) (walking on:0.2353) (watching:0.0976) (wearing:0.0854) (wears:0.0233) (with:0.1325) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.2147;     A @ 50: 0.2148;     A @ 100: 0.2148;  for mode=sgcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.1', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'True', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'LxyPredictor1', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/LAMBDA/Lxy1-predcls0.1'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.1
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: LxyPredictor1
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/LAMBDA/Lxy1-predcls0.1
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/LAMBDA/Lxy1-predcls0.1/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/LAMBDA/Lxy1-predcls0.1/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['GLOBAL_SETTING.BASIC_ENCODER', 'Hybrid-Attention', 'SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.001', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'False', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'LxyPredictor1', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/LAMBDA/Lxy1-sgcls0.001'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/Lxy_e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    # VG_NUM_CLASSES: 151
    NUM_CLASSES: 151
    #GQA_200_NUM_CLASSES: 201                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    # VG_NUM_CLASSES: 51
    NUM_CLASSES: 51
    #GQA_200_NUM_CLASSES: 101                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000    ####SHA 5000
  CHECKPOINT_PERIOD: 2000   ####SHA 5000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True  ####SHA False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3  ####SHA 没有
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  
  IMS_PER_BATCH: 1

GLOBAL_SETTING:
  ############### Parameters for Basic Encoder in Predictor ##############
  #RELATION_PREDICTOR: "MotifsLikePredictor"
  #RELATION_PREDICTOR: "VCTreePredictor"
  #RELATION_PREDICTOR: "TransLikePredictor"
  #RELATION_PREDICTOR: "MotifsLike_GCL"
  #RELATION_PREDICTOR: "VCTree_GCL"
  RELATION_PREDICTOR: "TransLike_GCL"
  BASIC_ENCODER: 'Hybrid-Attention'
  # ['Self-Attention', 'Cross-Attention', 'Hybrid-Attention'] for Transformer-Based Model, and ['Motifs', 'VTransE'] for DNN-Based Model
  ############### Parameters for Global Settings of Experiment ##############
  DATASET_CHOICE: 'VG'
  USE_BIAS: True                                      # If use the relation statistics to serve as the priori knowledge
  CHOOSE_BEST_MODEL_BY_METRIC: '_mean_recall'         # ['_recall', '_mean_recall'] To control which metric is the main concern
  PRINT_INTERVAL: 100
  ############### Parameters for GCL Loss Setting ##############
  GCL_SETTING:
    GROUP_SPLIT_MODE: 'divide4'                       # To control the number of groups ['divide4', ''divide3', 'divide5', 'average']
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0                   # To control the loss of Knowledge Transfer
    KNOWLEDGE_TRANSFER_MODE: 'KL_logit_TopDown'       # To control how to transfer the knowledge between different auxiliary classifiers
    # ['None', 'KL_logit_Neighbor', 'KL_logit_None', 'KL_logit_TopDown', 'KL_logit_BottomUp', 'KL_logit_BiDirection']
    ############### The Following Parameters would not affect the performance much, is nearly useless ##############
    NO_RELATION_RESTRAIN: True              # If two object do not have a relation, then limit their contribution to the final loss
    ZERO_LABEL_PADDING_MODE: 'rand_insert'  # ['rand_insert', 'rand_choose', 'all_include'], to control how to insert into the relation which is ZERO
    NO_RELATION_PENALTY: 0.1

LOSS: 'dnorm'    
GAMMA: 1.0
ALPHA: 1.0
BETA: 1.0
INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.001
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: LxyPredictor1
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/LAMBDA/Lxy1-sgcls0.001
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/LAMBDA/Lxy1-predcls0.1/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.weight of shape (5, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.weight of shape (11, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.weight of shape (20, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.weight of shape (39, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.weight of shape (5, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.weight of shape (11, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.weight of shape (20, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.weight of shape (39, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/LAMBDA/Lxy1-predcls0.1/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: inf, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: inf, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: inf, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: inf, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 4077365.50000, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 241770.40625, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 161523.45312, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 72243.06250, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 66601.88281, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 66259.77344, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 51525.37109, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 109.96768, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 1.98260, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2246762.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2231365.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2205761.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2134887.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2110164.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1488522.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1534027.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1437406.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2134887.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2089392.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1528155.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1386236.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1379124.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : inf, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : inf, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: inf, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: inf, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: inf, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: inf, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: inf, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: inf, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2261266.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2256625.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2256625.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2256625.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2256625.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2252094.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2246762.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2229230.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2186815.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1649963.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1576986.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1538579.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1524363.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1521476.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1480877.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1408115.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1382198.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1380401.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1361292.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 232425.39062, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 161072.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 160090.42188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 148673.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 142732.04688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 71478.64844, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 71478.64844, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 69262.91406, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 69262.91406, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 61200.00781, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 38780.16797, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 16564.76172, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 43.98341, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 41.05225, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 40.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 37.29463, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: nan, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 16564.76172, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: nan, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 61200.00781, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 5:01:05  iter: 1  loss: 377.9860 (377.9860)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.1375 (0.1375)  rel_ce_loss: 70.5824 (70.5824)  1_CE_loss: 4.2409 (4.2409)  2_CE_loss: 42.5517 (42.5517)  2_DKS_loss: 1.1660 (1.1660)  3_CE_loss: 85.2294 (85.2294)  3_DKS_loss: 2.9896 (2.9896)  4_CE_loss: 87.4357 (87.4357)  4_DKS_loss: 4.6862 (4.6862)  5_CE_loss: 71.2346 (71.2346)  5_DKS_loss: 7.7319 (7.7319)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1291 (1.1291)  data: 0.1016 (0.1016)  lr: 0.001600  max mem: 7921
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/LAMBDA/Lxy1-sgcls0.001/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_1.weight of shape (5, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_2.weight of shape (11, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_3.weight of shape (20, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_4.weight of shape (39, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_classifer_5.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.bias of shape (5,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_1.weight of shape (5, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.bias of shape (11,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_2.weight of shape (11, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.bias of shape (20,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_3.weight of shape (20, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.bias of shape (39,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_4.weight of shape (39, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress_5.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/LAMBDA/Lxy1-sgcls0.001/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: inf, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: inf, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: inf, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: inf, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: inf, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 3972510.75000, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 257713.20312, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 164229.46875, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 144249.65625, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 67514.92969, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 64221.58984, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 60574.50000, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 54500.85547, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: nan, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: nan, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: nan, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2753002.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2753002.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2744091.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2687499.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2602401.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2602401.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2602401.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2565989.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2540894.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2538878.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2534876.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2519599.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1886777.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1852183.12500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1975060.37500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1967170.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2538878.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2531653.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1659553.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1649152.62500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: inf, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: inf, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: inf, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: inf, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : inf, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : inf, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: inf, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: inf, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: inf, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: inf, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : inf, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 2602401.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 2556238.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2062558.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1992844.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1764684.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1728150.50000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1723735.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1709363.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1703988.87500, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1680169.25000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1652611.75000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 1587824.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 259561.26562, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 251617.04688, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 243830.18750, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 218434.89062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 164745.90625, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 71428.21094, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 62542.47656, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 49245.87500, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 58.13492, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 53.53938, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 44.17344, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 42.39526, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: nan, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 72215.75781, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 69690.01562, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: nan, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: nan, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 72215.75781, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 71428.21094, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 69690.01562, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: nan, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 62542.47656, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 5:16:36  iter: 1  loss: 410.0055 (410.0055)  obj_loss: 5.8164 (5.8164)  loss_cal: 0.0017 (0.0017)  rel_ce_loss: 51.8934 (51.8934)  1_CE_loss: 56.2916 (56.2916)  2_CE_loss: 56.1298 (56.1298)  2_DKS_loss: 1.2496 (1.2496)  3_CE_loss: 82.2121 (82.2121)  3_DKS_loss: 2.7720 (2.7720)  4_CE_loss: 93.0964 (93.0964)  4_DKS_loss: 4.6913 (4.6913)  5_CE_loss: 48.6180 (48.6180)  5_DKS_loss: 7.2332 (7.2332)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1873 (1.1873)  data: 0.1060 (0.1060)  lr: 0.001600  max mem: 7921
INFO:maskrcnn_benchmark:eta: 4:43:10  iter: 100  loss: 18.6924 (68.8980)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2541 (0.2827)  rel_ce_loss: 7.1929 (17.7310)  1_CE_loss: 0.0975 (0.5954)  2_CE_loss: 0.5405 (5.7243)  2_DKS_loss: 0.0000 (0.3257)  3_CE_loss: 0.5326 (10.3789)  3_DKS_loss: 0.0203 (0.8601)  4_CE_loss: 3.8432 (14.7054)  4_DKS_loss: 0.0555 (1.4418)  5_CE_loss: 3.9697 (13.3087)  5_DKS_loss: 1.7010 (3.5440)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0622 (1.0686)  data: 0.0892 (0.0921)  lr: 0.004451  max mem: 9662
INFO:maskrcnn_benchmark:eta: 4:42:59  iter: 100  loss: 26.3796 (73.9410)  obj_loss: 5.0234 (5.4592)  loss_cal: 0.0031 (0.0030)  rel_ce_loss: 7.9579 (15.7455)  1_CE_loss: 0.1461 (4.5646)  2_CE_loss: 0.7440 (5.9540)  2_DKS_loss: 0.0007 (0.2636)  3_CE_loss: 0.6886 (8.2677)  3_DKS_loss: 0.0097 (0.6979)  4_CE_loss: 6.0638 (17.7912)  4_DKS_loss: 0.0455 (1.1162)  5_CE_loss: 3.8692 (10.8119)  5_DKS_loss: 1.7673 (3.2663)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0661 (1.0679)  data: 0.0885 (0.0916)  lr: 0.004451  max mem: 9666
INFO:maskrcnn_benchmark:eta: 4:42:52  iter: 200  loss: 15.8397 (43.3680)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2865 (0.2950)  rel_ce_loss: 5.6998 (12.2851)  1_CE_loss: 0.0168 (0.3608)  2_CE_loss: 0.3030 (3.1662)  2_DKS_loss: 0.0000 (0.1634)  3_CE_loss: 0.3938 (5.4688)  3_DKS_loss: 0.0087 (0.4389)  4_CE_loss: 3.3716 (9.2227)  4_DKS_loss: 0.0319 (0.7469)  5_CE_loss: 2.8879 (8.6922)  5_DKS_loss: 1.4812 (2.5280)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0901 (1.0742)  data: 0.0913 (0.0936)  lr: 0.007331  max mem: 9662
INFO:maskrcnn_benchmark:eta: 4:42:07  iter: 200  loss: 20.1940 (47.8357)  obj_loss: 4.3945 (5.0328)  loss_cal: 0.0028 (0.0031)  rel_ce_loss: 5.2884 (11.2034)  1_CE_loss: 0.1187 (2.3722)  2_CE_loss: 0.4942 (3.2791)  2_DKS_loss: 0.0009 (0.1337)  3_CE_loss: 0.3950 (4.3759)  3_DKS_loss: 0.0103 (0.3599)  4_CE_loss: 3.3819 (10.7993)  4_DKS_loss: 0.0422 (0.5829)  5_CE_loss: 3.0041 (7.3275)  5_DKS_loss: 1.0448 (2.3661)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0620 (1.0714)  data: 0.0928 (0.0941)  lr: 0.007331  max mem: 9666
INFO:maskrcnn_benchmark:eta: 4:40:57  iter: 300  loss: 10.4928 (33.1591)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2913 (0.2973)  rel_ce_loss: 3.8427 (9.7470)  1_CE_loss: 0.0035 (0.2723)  2_CE_loss: 0.4584 (2.2637)  2_DKS_loss: 0.0000 (0.1098)  3_CE_loss: 0.4234 (3.8209)  3_DKS_loss: 0.0034 (0.2986)  4_CE_loss: 2.2197 (7.1535)  4_DKS_loss: 0.0458 (0.5195)  5_CE_loss: 1.9401 (6.6937)  5_DKS_loss: 0.5722 (1.9828)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0601 (1.0737)  data: 0.0966 (0.0958)  lr: 0.010211  max mem: 9662
INFO:maskrcnn_benchmark:eta: 4:41:07  iter: 300  loss: 14.7199 (37.7196)  obj_loss: 3.5078 (4.6635)  loss_cal: 0.0031 (0.0032)  rel_ce_loss: 3.9639 (9.1976)  1_CE_loss: 0.1972 (1.6323)  2_CE_loss: 0.4565 (2.3762)  2_DKS_loss: 0.0002 (0.0904)  3_CE_loss: 0.3997 (3.0676)  3_DKS_loss: 0.0108 (0.2475)  4_CE_loss: 2.3241 (8.2342)  4_DKS_loss: 0.0452 (0.4096)  5_CE_loss: 2.2868 (5.8586)  5_DKS_loss: 0.6472 (1.9388)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0706 (1.0744)  data: 0.0994 (0.0953)  lr: 0.010211  max mem: 9666
INFO:maskrcnn_benchmark:eta: 4:40:05  iter: 400  loss: 8.6543 (27.1967)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2425 (0.2988)  rel_ce_loss: 3.5253 (8.1914)  1_CE_loss: 0.0586 (0.2235)  2_CE_loss: 0.3576 (1.7874)  2_DKS_loss: 0.0000 (0.0828)  3_CE_loss: 0.3361 (2.9584)  3_DKS_loss: 0.0071 (0.2282)  4_CE_loss: 1.6946 (5.8343)  4_DKS_loss: 0.0248 (0.4036)  5_CE_loss: 1.7646 (5.5574)  5_DKS_loss: 0.4536 (1.6309)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0840 (1.0773)  data: 0.0985 (0.0984)  lr: 0.013091  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:40:20  iter: 400  loss: 9.7849 (31.3039)  obj_loss: 2.3105 (4.1925)  loss_cal: 0.0034 (0.0032)  rel_ce_loss: 2.8420 (7.8071)  1_CE_loss: 0.0662 (1.2578)  2_CE_loss: 0.3095 (1.8778)  2_DKS_loss: 0.0000 (0.0688)  3_CE_loss: 0.3381 (2.3993)  3_DKS_loss: 0.0103 (0.1913)  4_CE_loss: 1.4551 (6.6699)  4_DKS_loss: 0.0344 (0.3220)  5_CE_loss: 1.5496 (4.9080)  5_DKS_loss: 0.4098 (1.6062)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0821 (1.0782)  data: 0.0967 (0.0978)  lr: 0.013091  max mem: 9666
INFO:maskrcnn_benchmark:eta: 4:38:37  iter: 500  loss: 6.5626 (23.3833)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2741 (0.3005)  rel_ce_loss: 2.7140 (7.1789)  1_CE_loss: 0.0500 (0.1905)  2_CE_loss: 0.1926 (1.4881)  2_DKS_loss: 0.0003 (0.0667)  3_CE_loss: 0.3026 (2.4337)  3_DKS_loss: 0.0043 (0.1856)  4_CE_loss: 0.9936 (4.9487)  4_DKS_loss: 0.0258 (0.3338)  5_CE_loss: 1.4678 (4.8672)  5_DKS_loss: 0.2983 (1.3897)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0964 (1.0786)  data: 0.1007 (0.1000)  lr: 0.015971  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:38:44  iter: 500  loss: 8.1191 (26.9330)  obj_loss: 1.7119 (3.7486)  loss_cal: 0.0028 (0.0033)  rel_ce_loss: 2.8079 (6.8463)  1_CE_loss: 0.0414 (1.0267)  2_CE_loss: 0.2464 (1.5754)  2_DKS_loss: 0.0021 (0.0559)  3_CE_loss: 0.2962 (1.9934)  3_DKS_loss: 0.0111 (0.1574)  4_CE_loss: 1.1782 (5.6104)  4_DKS_loss: 0.0529 (0.2699)  5_CE_loss: 1.4875 (4.2746)  5_DKS_loss: 0.3314 (1.3712)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0881 (1.0790)  data: 0.0958 (0.0986)  lr: 0.015971  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:36:58  iter: 600  loss: 5.5454 (20.5413)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2518 (0.3019)  rel_ce_loss: 2.1971 (6.3836)  1_CE_loss: 0.0413 (0.1697)  2_CE_loss: 0.1935 (1.2854)  2_DKS_loss: 0.0000 (0.0558)  3_CE_loss: 0.1964 (2.0695)  3_DKS_loss: 0.0028 (0.1572)  4_CE_loss: 1.0233 (4.3192)  4_DKS_loss: 0.0188 (0.2852)  5_CE_loss: 1.1986 (4.3012)  5_DKS_loss: 0.3246 (1.2127)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0818 (1.0791)  data: 0.0995 (0.1007)  lr: 0.016000  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:37:09  iter: 600  loss: 5.7850 (23.6516)  obj_loss: 1.4277 (3.3863)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.8020 (6.0909)  1_CE_loss: 0.0369 (0.8690)  2_CE_loss: 0.1393 (1.3543)  2_DKS_loss: 0.0001 (0.0469)  3_CE_loss: 0.1187 (1.7014)  3_DKS_loss: 0.0072 (0.1333)  4_CE_loss: 0.8141 (4.8502)  4_DKS_loss: 0.0162 (0.2314)  5_CE_loss: 0.9641 (3.7885)  5_DKS_loss: 0.2128 (1.1961)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0711 (1.0798)  data: 0.0950 (0.0991)  lr: 0.016000  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:35:15  iter: 700  loss: 5.6999 (18.4494)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2888 (0.3042)  rel_ce_loss: 2.0724 (5.8004)  1_CE_loss: 0.0259 (0.1517)  2_CE_loss: 0.1711 (1.1337)  2_DKS_loss: 0.0000 (0.0480)  3_CE_loss: 0.1170 (1.8065)  3_DKS_loss: 0.0054 (0.1366)  4_CE_loss: 0.7572 (3.8371)  4_DKS_loss: 0.0157 (0.2496)  5_CE_loss: 1.2157 (3.9015)  5_DKS_loss: 0.2561 (1.0802)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0690 (1.0795)  data: 0.0928 (0.1006)  lr: 0.016000  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:35:27  iter: 700  loss: 5.3869 (21.1240)  obj_loss: 1.2773 (3.0965)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.8339 (5.4981)  1_CE_loss: 0.0598 (0.7566)  2_CE_loss: 0.1894 (1.1879)  2_DKS_loss: 0.0001 (0.0404)  3_CE_loss: 0.1834 (1.4854)  3_DKS_loss: 0.0090 (0.1160)  4_CE_loss: 0.7712 (4.2775)  4_DKS_loss: 0.0217 (0.2026)  5_CE_loss: 0.9250 (3.4018)  5_DKS_loss: 0.1996 (1.0580)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0952 (1.0802)  data: 0.0975 (0.0990)  lr: 0.016000  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:33:29  iter: 800  loss: 4.5602 (16.7741)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2894 (0.3049)  rel_ce_loss: 1.7805 (5.3256)  1_CE_loss: 0.0384 (0.1381)  2_CE_loss: 0.1269 (1.0125)  2_DKS_loss: 0.0000 (0.0421)  3_CE_loss: 0.1521 (1.6039)  3_DKS_loss: 0.0026 (0.1206)  4_CE_loss: 0.8129 (3.4576)  4_DKS_loss: 0.0156 (0.2216)  5_CE_loss: 1.1092 (3.5699)  5_DKS_loss: 0.2141 (0.9773)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0652 (1.0796)  data: 0.0935 (0.1008)  lr: 0.016000  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:33:41  iter: 800  loss: 6.3974 (19.2394)  obj_loss: 1.1904 (2.8601)  loss_cal: 0.0031 (0.0033)  rel_ce_loss: 1.9435 (5.0612)  1_CE_loss: 0.0457 (0.6707)  2_CE_loss: 0.1205 (1.0616)  2_DKS_loss: 0.0001 (0.0356)  3_CE_loss: 0.1487 (1.3237)  3_DKS_loss: 0.0052 (0.1027)  4_CE_loss: 0.9512 (3.8530)  4_DKS_loss: 0.0133 (0.1809)  5_CE_loss: 0.9933 (3.1247)  5_DKS_loss: 0.2176 (0.9619)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0665 (1.0804)  data: 0.0964 (0.0995)  lr: 0.016000  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:31:48  iter: 900  loss: 4.3927 (15.4323)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2839 (0.3054)  rel_ce_loss: 1.8357 (4.9434)  1_CE_loss: 0.0099 (0.1261)  2_CE_loss: 0.0908 (0.9142)  2_DKS_loss: 0.0001 (0.0375)  3_CE_loss: 0.0856 (1.4415)  3_DKS_loss: 0.0039 (0.1080)  4_CE_loss: 0.7085 (3.1572)  4_DKS_loss: 0.0162 (0.1996)  5_CE_loss: 0.9005 (3.3029)  5_DKS_loss: 0.1814 (0.8966)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0983 (1.0801)  data: 0.1001 (0.1010)  lr: 0.016000  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:31:51  iter: 900  loss: 5.2343 (17.7025)  obj_loss: 1.2021 (2.6733)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 1.6231 (4.6915)  1_CE_loss: 0.0320 (0.6025)  2_CE_loss: 0.0947 (0.9620)  2_DKS_loss: 0.0003 (0.0318)  3_CE_loss: 0.1311 (1.1941)  3_DKS_loss: 0.0063 (0.0924)  4_CE_loss: 0.8304 (3.5143)  4_DKS_loss: 0.0191 (0.1636)  5_CE_loss: 0.8869 (2.8944)  5_DKS_loss: 0.1856 (0.8795)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0693 (1.0803)  data: 0.0948 (0.0998)  lr: 0.016000  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:30:11  iter: 1000  loss: 3.8575 (14.3442)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2707 (0.3061)  rel_ce_loss: 1.5572 (4.6292)  1_CE_loss: 0.0202 (0.1173)  2_CE_loss: 0.1284 (0.8354)  2_DKS_loss: 0.0001 (0.0338)  3_CE_loss: 0.1060 (1.3108)  3_DKS_loss: 0.0033 (0.0979)  4_CE_loss: 0.6420 (2.9160)  4_DKS_loss: 0.0108 (0.1821)  5_CE_loss: 0.9304 (3.0836)  5_DKS_loss: 0.1775 (0.8319)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1005 (1.0808)  data: 0.0993 (0.1014)  lr: 0.016000  max mem: 9663
INFO:maskrcnn_benchmark:eta: 4:30:14  iter: 1000  loss: 4.7540 (16.4292)  obj_loss: 1.0732 (2.5173)  loss_cal: 0.0029 (0.0033)  rel_ce_loss: 1.5402 (4.3880)  1_CE_loss: 0.0103 (0.5480)  2_CE_loss: 0.0836 (0.8793)  2_DKS_loss: 0.0000 (0.0288)  3_CE_loss: 0.0916 (1.0887)  3_DKS_loss: 0.0052 (0.0838)  4_CE_loss: 0.6408 (3.2298)  4_DKS_loss: 0.0193 (0.1496)  5_CE_loss: 0.8706 (2.7005)  5_DKS_loss: 0.1661 (0.8122)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0860 (1.0810)  data: 0.1008 (0.1005)  lr: 0.016000  max mem: 9667
INFO:maskrcnn_benchmark:eta: 4:28:33  iter: 1100  loss: 3.9288 (13.4282)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2830 (0.3061)  rel_ce_loss: 1.5414 (4.3607)  1_CE_loss: 0.0237 (0.1096)  2_CE_loss: 0.0866 (0.7697)  2_DKS_loss: 0.0000 (0.0308)  3_CE_loss: 0.1245 (1.2034)  3_DKS_loss: 0.0028 (0.0896)  4_CE_loss: 0.5709 (2.7147)  4_DKS_loss: 0.0143 (0.1673)  5_CE_loss: 0.9587 (2.9017)  5_DKS_loss: 0.1535 (0.7746)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0819 (1.0815)  data: 0.0960 (0.1016)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:28:22  iter: 1100  loss: 4.7388 (15.3753)  obj_loss: 0.9868 (2.3884)  loss_cal: 0.0028 (0.0033)  rel_ce_loss: 1.6882 (4.1371)  1_CE_loss: 0.0225 (0.5021)  2_CE_loss: 0.1015 (0.8111)  2_DKS_loss: 0.0003 (0.0262)  3_CE_loss: 0.1127 (1.0015)  3_DKS_loss: 0.0051 (0.0769)  4_CE_loss: 0.6275 (2.9951)  4_DKS_loss: 0.0158 (0.1380)  5_CE_loss: 0.8750 (2.5406)  5_DKS_loss: 0.1779 (0.7549)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0944 (1.0807)  data: 0.0973 (0.1007)  lr: 0.016000  max mem: 9727
INFO:maskrcnn_benchmark:eta: 4:26:42  iter: 1200  loss: 3.5800 (12.6527)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2683 (0.3059)  rel_ce_loss: 1.3993 (4.1343)  1_CE_loss: 0.0018 (0.1028)  2_CE_loss: 0.0718 (0.7140)  2_DKS_loss: 0.0000 (0.0283)  3_CE_loss: 0.0977 (1.1122)  3_DKS_loss: 0.0030 (0.0826)  4_CE_loss: 0.5170 (2.5429)  4_DKS_loss: 0.0134 (0.1551)  5_CE_loss: 0.8590 (2.7484)  5_DKS_loss: 0.1528 (0.7262)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0638 (1.0813)  data: 0.0943 (0.1014)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:26:35  iter: 1200  loss: 4.3995 (14.4743)  obj_loss: 1.0557 (2.2801)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.5395 (3.9200)  1_CE_loss: 0.0337 (0.4642)  2_CE_loss: 0.0654 (0.7519)  2_DKS_loss: 0.0004 (0.0241)  3_CE_loss: 0.0777 (0.9275)  3_DKS_loss: 0.0045 (0.0710)  4_CE_loss: 0.5723 (2.7954)  4_DKS_loss: 0.0101 (0.1283)  5_CE_loss: 0.8665 (2.4027)  5_DKS_loss: 0.1681 (0.7058)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0834 (1.0808)  data: 0.0935 (0.1007)  lr: 0.016000  max mem: 9727
INFO:maskrcnn_benchmark:eta: 4:25:04  iter: 1300  loss: 3.2856 (11.9708)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2873 (0.3058)  rel_ce_loss: 1.3448 (3.9357)  1_CE_loss: 0.0023 (0.0969)  2_CE_loss: 0.0458 (0.6664)  2_DKS_loss: 0.0000 (0.0261)  3_CE_loss: 0.0675 (1.0339)  3_DKS_loss: 0.0024 (0.0766)  4_CE_loss: 0.5075 (2.3906)  4_DKS_loss: 0.0125 (0.1444)  5_CE_loss: 0.8048 (2.6126)  5_DKS_loss: 0.1374 (0.6817)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0823 (1.0819)  data: 0.0948 (0.1014)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:24:53  iter: 1300  loss: 4.5728 (13.7198)  obj_loss: 1.0850 (2.1867)  loss_cal: 0.0035 (0.0033)  rel_ce_loss: 1.5662 (3.7404)  1_CE_loss: 0.0056 (0.4308)  2_CE_loss: 0.0727 (0.7026)  2_DKS_loss: 0.0002 (0.0224)  3_CE_loss: 0.1216 (0.8650)  3_DKS_loss: 0.0043 (0.0661)  4_CE_loss: 0.5631 (2.6298)  4_DKS_loss: 0.0137 (0.1201)  5_CE_loss: 0.9257 (2.2890)  5_DKS_loss: 0.1329 (0.6637)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0813 (1.0812)  data: 0.0986 (0.1008)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:23:10  iter: 1400  loss: 3.5523 (11.3941)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2786 (0.3050)  rel_ce_loss: 1.4620 (3.7650)  1_CE_loss: 0.0068 (0.0914)  2_CE_loss: 0.0701 (0.6256)  2_DKS_loss: 0.0000 (0.0243)  3_CE_loss: 0.0788 (0.9666)  3_DKS_loss: 0.0039 (0.0716)  4_CE_loss: 0.6144 (2.2650)  4_DKS_loss: 0.0168 (0.1352)  5_CE_loss: 0.8734 (2.4980)  5_DKS_loss: 0.1463 (0.6464)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0766 (1.0815)  data: 0.0913 (0.1010)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:23:08  iter: 1400  loss: 4.4057 (13.0503)  obj_loss: 0.9805 (2.1029)  loss_cal: 0.0030 (0.0033)  rel_ce_loss: 1.4155 (3.5790)  1_CE_loss: 0.0213 (0.4030)  2_CE_loss: 0.0821 (0.6599)  2_DKS_loss: 0.0007 (0.0208)  3_CE_loss: 0.1098 (0.8106)  3_DKS_loss: 0.0045 (0.0617)  4_CE_loss: 0.5297 (2.4828)  4_DKS_loss: 0.0155 (0.1127)  5_CE_loss: 0.8701 (2.1870)  5_DKS_loss: 0.1342 (0.6266)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.0814)  data: 0.0947 (0.1013)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:21:42  iter: 1500  loss: 3.4373 (10.8753)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2714 (0.3048)  rel_ce_loss: 1.3877 (3.6112)  1_CE_loss: 0.0077 (0.0866)  2_CE_loss: 0.1043 (0.5901)  2_DKS_loss: 0.0000 (0.0227)  3_CE_loss: 0.0747 (0.9092)  3_DKS_loss: 0.0043 (0.0671)  4_CE_loss: 0.5182 (2.1515)  4_DKS_loss: 0.0185 (0.1271)  5_CE_loss: 0.7815 (2.3925)  5_DKS_loss: 0.1335 (0.6125)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0835 (1.0829)  data: 0.0940 (0.1016)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:21:26  iter: 1500  loss: 4.1108 (12.4614)  obj_loss: 0.9941 (2.0318)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.2919 (3.4330)  1_CE_loss: 0.0330 (0.3781)  2_CE_loss: 0.0531 (0.6220)  2_DKS_loss: 0.0007 (0.0195)  3_CE_loss: 0.0433 (0.7638)  3_DKS_loss: 0.0032 (0.0579)  4_CE_loss: 0.6146 (2.3564)  4_DKS_loss: 0.0148 (0.1065)  5_CE_loss: 0.7996 (2.0956)  5_DKS_loss: 0.1307 (0.5936)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0818 (1.0819)  data: 0.0948 (0.1016)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:20:18  iter: 1600  loss: 3.3052 (10.4125)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2720 (0.3048)  rel_ce_loss: 1.2675 (3.4722)  1_CE_loss: 0.0088 (0.0823)  2_CE_loss: 0.0524 (0.5588)  2_DKS_loss: 0.0000 (0.0213)  3_CE_loss: 0.0638 (0.8584)  3_DKS_loss: 0.0020 (0.0631)  4_CE_loss: 0.5457 (2.0520)  4_DKS_loss: 0.0122 (0.1201)  5_CE_loss: 0.7711 (2.2970)  5_DKS_loss: 0.1457 (0.5826)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1048 (1.0846)  data: 0.1019 (0.1028)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:20:04  iter: 1600  loss: 4.1371 (11.9462)  obj_loss: 1.0127 (1.9666)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.3611 (3.3072)  1_CE_loss: 0.0242 (0.3566)  2_CE_loss: 0.0847 (0.5895)  2_DKS_loss: 0.0004 (0.0183)  3_CE_loss: 0.0990 (0.7226)  3_DKS_loss: 0.0036 (0.0546)  4_CE_loss: 0.5500 (2.2451)  4_DKS_loss: 0.0117 (0.1011)  5_CE_loss: 0.7530 (2.0160)  5_DKS_loss: 0.1432 (0.5652)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0621 (1.0837)  data: 0.0927 (0.1034)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:18:46  iter: 1700  loss: 3.1773 (10.0032)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2900 (0.3047)  rel_ce_loss: 1.1348 (3.3487)  1_CE_loss: 0.0080 (0.0787)  2_CE_loss: 0.0639 (0.5313)  2_DKS_loss: 0.0000 (0.0200)  3_CE_loss: 0.0529 (0.8133)  3_DKS_loss: 0.0028 (0.0597)  4_CE_loss: 0.5502 (1.9625)  4_DKS_loss: 0.0087 (0.1138)  5_CE_loss: 0.7224 (2.2147)  5_DKS_loss: 0.1115 (0.5558)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1015 (1.0858)  data: 0.0920 (0.1039)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:18:07  iter: 1700  loss: 3.9873 (11.4837)  obj_loss: 0.9355 (1.9085)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.2889 (3.1937)  1_CE_loss: 0.0200 (0.3372)  2_CE_loss: 0.0523 (0.5602)  2_DKS_loss: 0.0003 (0.0173)  3_CE_loss: 0.0681 (0.6857)  3_DKS_loss: 0.0045 (0.0518)  4_CE_loss: 0.4839 (2.1442)  4_DKS_loss: 0.0123 (0.0961)  5_CE_loss: 0.8442 (1.9468)  5_DKS_loss: 0.1069 (0.5389)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0756 (1.0830)  data: 0.0891 (0.1028)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:16:51  iter: 1800  loss: 2.9105 (9.6295)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2950 (0.3049)  rel_ce_loss: 1.2049 (3.2360)  1_CE_loss: 0.0059 (0.0754)  2_CE_loss: 0.0609 (0.5058)  2_DKS_loss: 0.0001 (0.0189)  3_CE_loss: 0.0661 (0.7728)  3_DKS_loss: 0.0033 (0.0566)  4_CE_loss: 0.4369 (1.8821)  4_DKS_loss: 0.0117 (0.1083)  5_CE_loss: 0.6875 (2.1376)  5_DKS_loss: 0.0979 (0.5312)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0705 (1.0853)  data: 0.0907 (0.1034)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:16:20  iter: 1800  loss: 3.5921 (11.0564)  obj_loss: 0.9414 (1.8573)  loss_cal: 0.0035 (0.0033)  rel_ce_loss: 1.2378 (3.0861)  1_CE_loss: 0.0042 (0.3199)  2_CE_loss: 0.0453 (0.5332)  2_DKS_loss: 0.0003 (0.0164)  3_CE_loss: 0.0514 (0.6518)  3_DKS_loss: 0.0028 (0.0491)  4_CE_loss: 0.4502 (2.0529)  4_DKS_loss: 0.0136 (0.0915)  5_CE_loss: 0.7252 (1.8801)  5_DKS_loss: 0.0812 (0.5147)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0896 (1.0831)  data: 0.0902 (0.1023)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:15:00  iter: 1900  loss: 2.9145 (9.2834)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3070 (0.3047)  rel_ce_loss: 1.1517 (3.1299)  1_CE_loss: 0.0028 (0.0723)  2_CE_loss: 0.0498 (0.4834)  2_DKS_loss: 0.0001 (0.0179)  3_CE_loss: 0.0461 (0.7363)  3_DKS_loss: 0.0025 (0.0538)  4_CE_loss: 0.4128 (1.8082)  4_DKS_loss: 0.0085 (0.1032)  5_CE_loss: 0.6739 (2.0652)  5_DKS_loss: 0.0831 (0.5084)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0782 (1.0852)  data: 0.0905 (0.1029)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:14:38  iter: 1900  loss: 3.8392 (10.6757)  obj_loss: 0.9521 (1.8106)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.2575 (2.9913)  1_CE_loss: 0.0032 (0.3045)  2_CE_loss: 0.0596 (0.5092)  2_DKS_loss: 0.0003 (0.0155)  3_CE_loss: 0.0730 (0.6220)  3_DKS_loss: 0.0030 (0.0467)  4_CE_loss: 0.4669 (1.9705)  4_DKS_loss: 0.0105 (0.0875)  5_CE_loss: 0.6654 (1.8213)  5_DKS_loss: 0.0999 (0.4933)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0658 (1.0836)  data: 0.0898 (0.1026)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:13:21  iter: 2000  loss: 3.0488 (8.9732)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3177 (0.3047)  rel_ce_loss: 1.2223 (3.0355)  1_CE_loss: 0.0084 (0.0696)  2_CE_loss: 0.0458 (0.4626)  2_DKS_loss: 0.0001 (0.0171)  3_CE_loss: 0.0309 (0.7031)  3_DKS_loss: 0.0023 (0.0513)  4_CE_loss: 0.5824 (1.7417)  4_DKS_loss: 0.0088 (0.0986)  5_CE_loss: 0.7246 (2.0014)  5_DKS_loss: 0.0862 (0.4877)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0584 (1.0858)  data: 0.0900 (0.1034)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:12:44  iter: 2000  loss: 3.6164 (10.3258)  obj_loss: 0.9321 (1.7689)  loss_cal: 0.0032 (0.0033)  rel_ce_loss: 1.2429 (2.9024)  1_CE_loss: 0.0219 (0.2908)  2_CE_loss: 0.0835 (0.4877)  2_DKS_loss: 0.0005 (0.0148)  3_CE_loss: 0.0601 (0.5947)  3_DKS_loss: 0.0030 (0.0446)  4_CE_loss: 0.4536 (1.8958)  4_DKS_loss: 0.0101 (0.0837)  5_CE_loss: 0.6252 (1.7659)  5_DKS_loss: 0.0774 (0.4732)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0841 (1.0832)  data: 0.0873 (0.1021)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:11:32  iter: 2100  loss: 3.0364 (8.6880)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2808 (0.3047)  rel_ce_loss: 1.3168 (2.9466)  1_CE_loss: 0.0113 (0.0671)  2_CE_loss: 0.0727 (0.4437)  2_DKS_loss: 0.0000 (0.0163)  3_CE_loss: 0.0662 (0.6733)  3_DKS_loss: 0.0017 (0.0490)  4_CE_loss: 0.4438 (1.6822)  4_DKS_loss: 0.0062 (0.0945)  5_CE_loss: 0.7068 (1.9412)  5_DKS_loss: 0.0879 (0.4695)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1067 (1.0858)  data: 0.0929 (0.1031)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:10:49  iter: 2100  loss: 3.5284 (10.0107)  obj_loss: 0.9565 (1.7315)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 1.1671 (2.8231)  1_CE_loss: 0.0149 (0.2782)  2_CE_loss: 0.0586 (0.4680)  2_DKS_loss: 0.0006 (0.0141)  3_CE_loss: 0.0651 (0.5700)  3_DKS_loss: 0.0027 (0.0427)  4_CE_loss: 0.3954 (1.8281)  4_DKS_loss: 0.0123 (0.0803)  5_CE_loss: 0.6211 (1.7157)  5_DKS_loss: 0.0770 (0.4554)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0834 (1.0827)  data: 0.0875 (0.1017)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:09:35  iter: 2200  loss: 2.7371 (8.4185)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2790 (0.3049)  rel_ce_loss: 1.0327 (2.8618)  1_CE_loss: 0.0074 (0.0649)  2_CE_loss: 0.0608 (0.4270)  2_DKS_loss: 0.0001 (0.0155)  3_CE_loss: 0.0581 (0.6459)  3_DKS_loss: 0.0027 (0.0469)  4_CE_loss: 0.4258 (1.6253)  4_DKS_loss: 0.0081 (0.0907)  5_CE_loss: 0.6691 (1.8837)  5_DKS_loss: 0.0697 (0.4519)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0690 (1.0852)  data: 0.0907 (0.1026)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:08:54  iter: 2200  loss: 3.3231 (9.7117)  obj_loss: 0.9316 (1.6958)  loss_cal: 0.0033 (0.0033)  rel_ce_loss: 1.0831 (2.7461)  1_CE_loss: 0.0142 (0.2668)  2_CE_loss: 0.0625 (0.4496)  2_DKS_loss: 0.0002 (0.0135)  3_CE_loss: 0.0460 (0.5467)  3_DKS_loss: 0.0026 (0.0409)  4_CE_loss: 0.4622 (1.7652)  4_DKS_loss: 0.0076 (0.0772)  5_CE_loss: 0.6113 (1.6681)  5_DKS_loss: 0.0661 (0.4383)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0764 (1.0822)  data: 0.0952 (0.1014)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:07:43  iter: 2300  loss: 2.5593 (8.1702)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2808 (0.3045)  rel_ce_loss: 0.9374 (2.7828)  1_CE_loss: 0.0022 (0.0627)  2_CE_loss: 0.0531 (0.4113)  2_DKS_loss: 0.0000 (0.0149)  3_CE_loss: 0.0505 (0.6207)  3_DKS_loss: 0.0018 (0.0450)  4_CE_loss: 0.4451 (1.5742)  4_DKS_loss: 0.0064 (0.0872)  5_CE_loss: 0.6228 (1.8306)  5_DKS_loss: 0.0897 (0.4361)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0894 (1.0849)  data: 0.0995 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:07:04  iter: 2300  loss: 3.3501 (9.4362)  obj_loss: 0.9233 (1.6633)  loss_cal: 0.0034 (0.0033)  rel_ce_loss: 1.0362 (2.6739)  1_CE_loss: 0.0294 (0.2564)  2_CE_loss: 0.0453 (0.4327)  2_DKS_loss: 0.0006 (0.0129)  3_CE_loss: 0.0543 (0.5256)  3_DKS_loss: 0.0029 (0.0393)  4_CE_loss: 0.4102 (1.7080)  4_DKS_loss: 0.0088 (0.0743)  5_CE_loss: 0.5826 (1.6236)  5_DKS_loss: 0.0717 (0.4228)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0811 (1.0821)  data: 0.0926 (0.1013)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:05:53  iter: 2400  loss: 2.5352 (7.9402)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2618 (0.3045)  rel_ce_loss: 1.0054 (2.7097)  1_CE_loss: 0.0033 (0.0608)  2_CE_loss: 0.0578 (0.3970)  2_DKS_loss: 0.0001 (0.0143)  3_CE_loss: 0.0635 (0.5979)  3_DKS_loss: 0.0010 (0.0432)  4_CE_loss: 0.3344 (1.5269)  4_DKS_loss: 0.0064 (0.0840)  5_CE_loss: 0.5755 (1.7812)  5_DKS_loss: 0.0547 (0.4208)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0979 (1.0848)  data: 0.0949 (0.1023)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:05:17  iter: 2400  loss: 3.0336 (9.1790)  obj_loss: 0.9634 (1.6340)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.8871 (2.6061)  1_CE_loss: 0.0029 (0.2465)  2_CE_loss: 0.0465 (0.4173)  2_DKS_loss: 0.0004 (0.0124)  3_CE_loss: 0.0519 (0.5063)  3_DKS_loss: 0.0028 (0.0378)  4_CE_loss: 0.4165 (1.6546)  4_DKS_loss: 0.0079 (0.0716)  5_CE_loss: 0.5471 (1.5810)  5_DKS_loss: 0.0564 (0.4080)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0673 (1.0822)  data: 0.0913 (0.1014)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:04:07  iter: 2500  loss: 2.5279 (7.7202)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3108 (0.3043)  rel_ce_loss: 0.9856 (2.6396)  1_CE_loss: 0.0118 (0.0591)  2_CE_loss: 0.0424 (0.3836)  2_DKS_loss: 0.0001 (0.0137)  3_CE_loss: 0.0418 (0.5764)  3_DKS_loss: 0.0018 (0.0416)  4_CE_loss: 0.4013 (1.4812)  4_DKS_loss: 0.0082 (0.0811)  5_CE_loss: 0.5604 (1.7336)  5_DKS_loss: 0.0663 (0.4063)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0765 (1.0850)  data: 0.0932 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:03:24  iter: 2500  loss: 3.2172 (8.9416)  obj_loss: 0.9775 (1.6069)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.9501 (2.5430)  1_CE_loss: 0.0105 (0.2374)  2_CE_loss: 0.0717 (0.4033)  2_DKS_loss: 0.0004 (0.0120)  3_CE_loss: 0.0567 (0.4888)  3_DKS_loss: 0.0031 (0.0364)  4_CE_loss: 0.4703 (1.6053)  4_DKS_loss: 0.0082 (0.0693)  5_CE_loss: 0.5560 (1.5415)  5_DKS_loss: 0.0612 (0.3943)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0604 (1.0818)  data: 0.0916 (0.1011)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:02:20  iter: 2600  loss: 2.3205 (7.5190)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2774 (0.3040)  rel_ce_loss: 0.9340 (2.5752)  1_CE_loss: 0.0096 (0.0574)  2_CE_loss: 0.0470 (0.3711)  2_DKS_loss: 0.0001 (0.0132)  3_CE_loss: 0.0416 (0.5564)  3_DKS_loss: 0.0019 (0.0400)  4_CE_loss: 0.3444 (1.4397)  4_DKS_loss: 0.0074 (0.0783)  5_CE_loss: 0.5526 (1.6906)  5_DKS_loss: 0.0392 (0.3930)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0884 (1.0851)  data: 0.0945 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 4:01:42  iter: 2600  loss: 3.0104 (8.7210)  obj_loss: 0.8984 (1.5812)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.9750 (2.4844)  1_CE_loss: 0.0097 (0.2291)  2_CE_loss: 0.0441 (0.3901)  2_DKS_loss: 0.0004 (0.0115)  3_CE_loss: 0.0531 (0.4724)  3_DKS_loss: 0.0026 (0.0352)  4_CE_loss: 0.4066 (1.5599)  4_DKS_loss: 0.0074 (0.0670)  5_CE_loss: 0.5029 (1.5054)  5_DKS_loss: 0.0574 (0.3815)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0760 (1.0822)  data: 0.0883 (0.1015)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 4:00:36  iter: 2700  loss: 2.2316 (7.3316)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2855 (0.3037)  rel_ce_loss: 0.9186 (2.5149)  1_CE_loss: 0.0104 (0.0559)  2_CE_loss: 0.0446 (0.3595)  2_DKS_loss: 0.0001 (0.0127)  3_CE_loss: 0.0515 (0.5377)  3_DKS_loss: 0.0018 (0.0387)  4_CE_loss: 0.3753 (1.4014)  4_DKS_loss: 0.0098 (0.0758)  5_CE_loss: 0.4611 (1.6505)  5_DKS_loss: 0.0549 (0.3808)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0867 (1.0854)  data: 0.0911 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:59:54  iter: 2700  loss: 2.9377 (8.5124)  obj_loss: 0.9243 (1.5574)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.9131 (2.4285)  1_CE_loss: 0.0119 (0.2214)  2_CE_loss: 0.0453 (0.3778)  2_DKS_loss: 0.0004 (0.0111)  3_CE_loss: 0.0417 (0.4570)  3_DKS_loss: 0.0040 (0.0340)  4_CE_loss: 0.3474 (1.5168)  4_DKS_loss: 0.0098 (0.0650)  5_CE_loss: 0.5174 (1.4708)  5_DKS_loss: 0.0451 (0.3693)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0966 (1.0823)  data: 0.0917 (0.1013)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:58:46  iter: 2800  loss: 2.2756 (7.1575)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2999 (0.3038)  rel_ce_loss: 0.8812 (2.4589)  1_CE_loss: 0.0033 (0.0546)  2_CE_loss: 0.0380 (0.3486)  2_DKS_loss: 0.0002 (0.0123)  3_CE_loss: 0.0443 (0.5204)  3_DKS_loss: 0.0029 (0.0374)  4_CE_loss: 0.3780 (1.3653)  4_DKS_loss: 0.0088 (0.0734)  5_CE_loss: 0.4975 (1.6132)  5_DKS_loss: 0.0780 (0.3697)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0938 (1.0853)  data: 0.0973 (0.1023)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:58:09  iter: 2800  loss: 2.8974 (8.3150)  obj_loss: 1.0273 (1.5361)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.9120 (2.3747)  1_CE_loss: 0.0042 (0.2141)  2_CE_loss: 0.0476 (0.3663)  2_DKS_loss: 0.0004 (0.0108)  3_CE_loss: 0.0384 (0.4427)  3_DKS_loss: 0.0022 (0.0329)  4_CE_loss: 0.3390 (1.4761)  4_DKS_loss: 0.0094 (0.0630)  5_CE_loss: 0.5001 (1.4372)  5_DKS_loss: 0.0419 (0.3579)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1070 (1.0825)  data: 0.0952 (0.1013)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:57:00  iter: 2900  loss: 2.3659 (6.9939)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2769 (0.3035)  rel_ce_loss: 0.9062 (2.4063)  1_CE_loss: 0.0079 (0.0532)  2_CE_loss: 0.0493 (0.3385)  2_DKS_loss: 0.0002 (0.0119)  3_CE_loss: 0.0459 (0.5044)  3_DKS_loss: 0.0065 (0.0363)  4_CE_loss: 0.3758 (1.3317)  4_DKS_loss: 0.0146 (0.0712)  5_CE_loss: 0.5308 (1.5780)  5_DKS_loss: 0.0686 (0.3590)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0661 (1.0855)  data: 0.0932 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:56:20  iter: 2900  loss: 2.7074 (8.1313)  obj_loss: 0.9126 (1.5155)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.8096 (2.3244)  1_CE_loss: 0.0040 (0.2074)  2_CE_loss: 0.0374 (0.3555)  2_DKS_loss: 0.0004 (0.0104)  3_CE_loss: 0.0380 (0.4292)  3_DKS_loss: 0.0017 (0.0319)  4_CE_loss: 0.3742 (1.4388)  4_DKS_loss: 0.0090 (0.0612)  5_CE_loss: 0.4567 (1.4062)  5_DKS_loss: 0.0419 (0.3475)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0790 (1.0825)  data: 0.0900 (0.1012)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:55:11  iter: 3000  loss: 2.1744 (6.8384)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2875 (0.3033)  rel_ce_loss: 0.8094 (2.3560)  1_CE_loss: 0.0135 (0.0520)  2_CE_loss: 0.0436 (0.3289)  2_DKS_loss: 0.0002 (0.0115)  3_CE_loss: 0.0472 (0.4892)  3_DKS_loss: 0.0022 (0.0352)  4_CE_loss: 0.3555 (1.2997)  4_DKS_loss: 0.0079 (0.0692)  5_CE_loss: 0.4681 (1.5444)  5_DKS_loss: 0.0351 (0.3491)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0771 (1.0855)  data: 0.0924 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:54:31  iter: 3000  loss: 2.9373 (7.9614)  obj_loss: 0.9224 (1.4965)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.8550 (2.2780)  1_CE_loss: 0.0030 (0.2012)  2_CE_loss: 0.0560 (0.3455)  2_DKS_loss: 0.0008 (0.0101)  3_CE_loss: 0.0482 (0.4167)  3_DKS_loss: 0.0034 (0.0310)  4_CE_loss: 0.3797 (1.4042)  4_DKS_loss: 0.0118 (0.0596)  5_CE_loss: 0.4766 (1.3776)  5_DKS_loss: 0.0490 (0.3379)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0603 (1.0824)  data: 0.0926 (0.1015)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:53:31  iter: 3100  loss: 2.3126 (6.6915)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3063 (0.3033)  rel_ce_loss: 0.9106 (2.3085)  1_CE_loss: 0.0077 (0.0508)  2_CE_loss: 0.0477 (0.3200)  2_DKS_loss: 0.0002 (0.0111)  3_CE_loss: 0.0423 (0.4751)  3_DKS_loss: 0.0026 (0.0341)  4_CE_loss: 0.3828 (1.2699)  4_DKS_loss: 0.0083 (0.0673)  5_CE_loss: 0.5261 (1.5118)  5_DKS_loss: 0.0808 (0.3396)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0937 (1.0861)  data: 0.0962 (0.1029)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:52:51  iter: 3100  loss: 2.9971 (7.7980)  obj_loss: 0.9829 (1.4780)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.8963 (2.2331)  1_CE_loss: 0.0156 (0.1953)  2_CE_loss: 0.0553 (0.3361)  2_DKS_loss: 0.0008 (0.0098)  3_CE_loss: 0.0590 (0.4049)  3_DKS_loss: 0.0028 (0.0301)  4_CE_loss: 0.3897 (1.3715)  4_DKS_loss: 0.0101 (0.0580)  5_CE_loss: 0.4814 (1.3497)  5_DKS_loss: 0.0301 (0.3283)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0661 (1.0831)  data: 0.0902 (0.1020)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:51:48  iter: 3200  loss: 2.0151 (6.5534)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3165 (0.3032)  rel_ce_loss: 0.8007 (2.2638)  1_CE_loss: 0.0058 (0.0496)  2_CE_loss: 0.0389 (0.3116)  2_DKS_loss: 0.0001 (0.0108)  3_CE_loss: 0.0362 (0.4617)  3_DKS_loss: 0.0015 (0.0332)  4_CE_loss: 0.3013 (1.2413)  4_DKS_loss: 0.0057 (0.0654)  5_CE_loss: 0.4432 (1.4811)  5_DKS_loss: 0.0485 (0.3316)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0916 (1.0866)  data: 0.0961 (0.1032)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:51:04  iter: 3200  loss: 2.7283 (7.6428)  obj_loss: 0.9175 (1.4611)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.8244 (2.1903)  1_CE_loss: 0.0095 (0.1898)  2_CE_loss: 0.0352 (0.3272)  2_DKS_loss: 0.0005 (0.0095)  3_CE_loss: 0.0344 (0.3939)  3_DKS_loss: 0.0020 (0.0292)  4_CE_loss: 0.3482 (1.3398)  4_DKS_loss: 0.0074 (0.0565)  5_CE_loss: 0.4217 (1.3230)  5_DKS_loss: 0.0335 (0.3193)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0801 (1.0832)  data: 0.0892 (0.1018)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:49:56  iter: 3300  loss: 2.1363 (6.4264)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2755 (0.3030)  rel_ce_loss: 0.8075 (2.2220)  1_CE_loss: 0.0065 (0.0485)  2_CE_loss: 0.0389 (0.3038)  2_DKS_loss: 0.0001 (0.0104)  3_CE_loss: 0.0392 (0.4493)  3_DKS_loss: 0.0024 (0.0323)  4_CE_loss: 0.3334 (1.2160)  4_DKS_loss: 0.0093 (0.0638)  5_CE_loss: 0.4544 (1.4536)  5_DKS_loss: 0.0386 (0.3237)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0805 (1.0863)  data: 0.0952 (0.1030)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:49:14  iter: 3300  loss: 2.7065 (7.4945)  obj_loss: 0.9165 (1.4451)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7937 (2.1491)  1_CE_loss: 0.0133 (0.1844)  2_CE_loss: 0.0300 (0.3186)  2_DKS_loss: 0.0005 (0.0093)  3_CE_loss: 0.0384 (0.3833)  3_DKS_loss: 0.0021 (0.0284)  4_CE_loss: 0.3482 (1.3099)  4_DKS_loss: 0.0068 (0.0550)  5_CE_loss: 0.4810 (1.2972)  5_DKS_loss: 0.0322 (0.3109)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0740 (1.0830)  data: 0.0883 (0.1016)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:48:01  iter: 3400  loss: 2.2020 (6.3052)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2945 (0.3029)  rel_ce_loss: 0.8378 (2.1819)  1_CE_loss: 0.0113 (0.0475)  2_CE_loss: 0.0505 (0.2965)  2_DKS_loss: 0.0001 (0.0101)  3_CE_loss: 0.0424 (0.4376)  3_DKS_loss: 0.0032 (0.0315)  4_CE_loss: 0.3479 (1.1921)  4_DKS_loss: 0.0073 (0.0622)  5_CE_loss: 0.4723 (1.4271)  5_DKS_loss: 0.0497 (0.3159)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0735 (1.0858)  data: 0.0920 (0.1027)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:47:18  iter: 3400  loss: 2.6841 (7.3543)  obj_loss: 0.9214 (1.4301)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7970 (2.1099)  1_CE_loss: 0.0111 (0.1794)  2_CE_loss: 0.0397 (0.3106)  2_DKS_loss: 0.0009 (0.0090)  3_CE_loss: 0.0378 (0.3733)  3_DKS_loss: 0.0047 (0.0277)  4_CE_loss: 0.3632 (1.2816)  4_DKS_loss: 0.0105 (0.0537)  5_CE_loss: 0.4667 (1.2729)  5_DKS_loss: 0.0301 (0.3027)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0662 (1.0824)  data: 0.0882 (0.1012)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:46:07  iter: 3500  loss: 2.2028 (6.1907)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2725 (0.3025)  rel_ce_loss: 0.8596 (2.1444)  1_CE_loss: 0.0105 (0.0465)  2_CE_loss: 0.0463 (0.2895)  2_DKS_loss: 0.0001 (0.0099)  3_CE_loss: 0.0432 (0.4265)  3_DKS_loss: 0.0021 (0.0307)  4_CE_loss: 0.3927 (1.1693)  4_DKS_loss: 0.0087 (0.0607)  5_CE_loss: 0.4800 (1.4023)  5_DKS_loss: 0.0467 (0.3086)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0839 (1.0854)  data: 0.0893 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:45:31  iter: 3500  loss: 2.6999 (7.2239)  obj_loss: 0.8857 (1.4161)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.8172 (2.0733)  1_CE_loss: 0.0052 (0.1748)  2_CE_loss: 0.0398 (0.3032)  2_DKS_loss: 0.0005 (0.0088)  3_CE_loss: 0.0389 (0.3641)  3_DKS_loss: 0.0031 (0.0270)  4_CE_loss: 0.3068 (1.2552)  4_DKS_loss: 0.0069 (0.0524)  5_CE_loss: 0.5101 (1.2505)  5_DKS_loss: 0.0286 (0.2952)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0821 (1.0825)  data: 0.0890 (0.1013)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:44:21  iter: 3600  loss: 2.1247 (6.0812)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3032 (0.3025)  rel_ce_loss: 0.8281 (2.1083)  1_CE_loss: 0.0037 (0.0455)  2_CE_loss: 0.0335 (0.2827)  2_DKS_loss: 0.0002 (0.0096)  3_CE_loss: 0.0315 (0.4159)  3_DKS_loss: 0.0027 (0.0300)  4_CE_loss: 0.3281 (1.1468)  4_DKS_loss: 0.0082 (0.0593)  5_CE_loss: 0.4933 (1.3780)  5_DKS_loss: 0.0345 (0.3025)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0812 (1.0856)  data: 0.0911 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:43:42  iter: 3600  loss: 2.6184 (7.1007)  obj_loss: 0.8701 (1.4028)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.8035 (2.0391)  1_CE_loss: 0.0092 (0.1703)  2_CE_loss: 0.0359 (0.2961)  2_DKS_loss: 0.0006 (0.0086)  3_CE_loss: 0.0450 (0.3554)  3_DKS_loss: 0.0027 (0.0263)  4_CE_loss: 0.3633 (1.2300)  4_DKS_loss: 0.0082 (0.0512)  5_CE_loss: 0.4349 (1.2295)  5_DKS_loss: 0.0313 (0.2882)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0685 (1.0825)  data: 0.0873 (0.1014)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:42:42  iter: 3700  loss: 2.0001 (5.9752)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2894 (0.3026)  rel_ce_loss: 0.7553 (2.0736)  1_CE_loss: 0.0085 (0.0446)  2_CE_loss: 0.0269 (0.2764)  2_DKS_loss: 0.0001 (0.0093)  3_CE_loss: 0.0367 (0.4059)  3_DKS_loss: 0.0024 (0.0293)  4_CE_loss: 0.2928 (1.1251)  4_DKS_loss: 0.0105 (0.0581)  5_CE_loss: 0.4659 (1.3548)  5_DKS_loss: 0.0411 (0.2954)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0814 (1.0864)  data: 0.0905 (0.1031)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:41:54  iter: 3700  loss: 2.5387 (6.9792)  obj_loss: 0.8979 (1.3884)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7742 (2.0055)  1_CE_loss: 0.0063 (0.1660)  2_CE_loss: 0.0405 (0.2894)  2_DKS_loss: 0.0004 (0.0083)  3_CE_loss: 0.0315 (0.3470)  3_DKS_loss: 0.0022 (0.0257)  4_CE_loss: 0.3229 (1.2058)  4_DKS_loss: 0.0055 (0.0500)  5_CE_loss: 0.4161 (1.2085)  5_DKS_loss: 0.0260 (0.2813)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0653 (1.0825)  data: 0.0872 (0.1015)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:40:49  iter: 3800  loss: 2.2092 (5.8747)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3010 (0.3026)  rel_ce_loss: 0.9084 (2.0406)  1_CE_loss: 0.0086 (0.0438)  2_CE_loss: 0.0296 (0.2703)  2_DKS_loss: 0.0002 (0.0091)  3_CE_loss: 0.0325 (0.3963)  3_DKS_loss: 0.0022 (0.0286)  4_CE_loss: 0.3384 (1.1048)  4_DKS_loss: 0.0065 (0.0568)  5_CE_loss: 0.5124 (1.3326)  5_DKS_loss: 0.0586 (0.2891)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0739 (1.0860)  data: 0.0919 (0.1028)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:40:02  iter: 3800  loss: 2.6808 (6.8659)  obj_loss: 0.9600 (1.3752)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7574 (1.9740)  1_CE_loss: 0.0104 (0.1620)  2_CE_loss: 0.0385 (0.2832)  2_DKS_loss: 0.0014 (0.0082)  3_CE_loss: 0.0395 (0.3390)  3_DKS_loss: 0.0031 (0.0251)  4_CE_loss: 0.3250 (1.1832)  4_DKS_loss: 0.0077 (0.0489)  5_CE_loss: 0.4126 (1.1888)  5_DKS_loss: 0.0313 (0.2749)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0881 (1.0822)  data: 0.0860 (0.1012)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:38:54  iter: 3900  loss: 2.0400 (5.7822)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2886 (0.3028)  rel_ce_loss: 0.7550 (2.0105)  1_CE_loss: 0.0062 (0.0430)  2_CE_loss: 0.0520 (0.2646)  2_DKS_loss: 0.0001 (0.0089)  3_CE_loss: 0.0435 (0.3873)  3_DKS_loss: 0.0016 (0.0280)  4_CE_loss: 0.3104 (1.0856)  4_DKS_loss: 0.0050 (0.0556)  5_CE_loss: 0.4911 (1.3130)  5_DKS_loss: 0.0249 (0.2830)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0785 (1.0855)  data: 0.0901 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:38:09  iter: 3900  loss: 2.4750 (6.7569)  obj_loss: 0.8262 (1.3623)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7383 (1.9436)  1_CE_loss: 0.0121 (0.1583)  2_CE_loss: 0.0380 (0.2771)  2_DKS_loss: 0.0007 (0.0080)  3_CE_loss: 0.0365 (0.3315)  3_DKS_loss: 0.0028 (0.0245)  4_CE_loss: 0.2934 (1.1615)  4_DKS_loss: 0.0067 (0.0479)  5_CE_loss: 0.3932 (1.1702)  5_DKS_loss: 0.0160 (0.2686)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0722 (1.0818)  data: 0.0874 (0.1009)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:---Total norm 9.94440 clip coef 0.50280-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.53452, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 4.17034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 3.72408, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.09778, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.94670, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.92513, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.79226, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.74286, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.72614, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 1.67757, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 1.13749, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 1.13209, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 1.13155, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.11387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.11387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.11387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.11387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 1.00399, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.97849, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.90847, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.81981, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.77095, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.75791, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.73599, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.72421, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.70798, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.68608, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.67893, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.67806, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.67374, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.60321, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.57426, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.54062, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.47665, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.46640, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.44150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.43741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.43228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.42353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.42344, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.42180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41723, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.40293, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.40245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.39945, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.34787, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.34211, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.30101, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.30003, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.28510, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26334, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.26221, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25903, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.25300, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24922, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23859, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.23730, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.23616, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21789, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21779, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21647, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.21561, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21246, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.20205, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.20205, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20152, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.19914, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19828, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19770, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.19524, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19501, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.18483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18392, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17784, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17484, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17253, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.17104, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16793, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16685, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16628, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16576, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16496, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16336, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16306, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16211, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16178, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16167, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16135, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16070, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15854, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15681, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15595, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15585, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.15474, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15417, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15254, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15193, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.15112, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14869, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14789, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14765, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14444, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14314, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14002, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13836, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13650, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.13426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13416, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13236, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13104, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12894, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12654, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12558, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12525, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12372, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12360, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12300, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12007, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.11972, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11800, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11342, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.11144, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10806, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.10768, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10246, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10061, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10041, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.09943, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09866, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.09667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09655, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09644, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.09491, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.09380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08661, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.08360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07742, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07534, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07475, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07347, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07313, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.07233, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07152, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07142, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.07132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07076, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06964, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06904, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06831, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.06813, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06810, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06682, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06349, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06122, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06078, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06061, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05803, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05786, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05539, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05514, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05445, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05445, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05313, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05313, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.05257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05250, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05234, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05003, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04999, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04730, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04547, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04487, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.04366, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04284, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04275, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04220, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04125, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04120, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03941, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03923, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03892, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.03866, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03808, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03806, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03727, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03711, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03703, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03682, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03676, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03553, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03473, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.03374, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.03261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03138, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03093, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03025, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.03004, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02896, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02800, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02767, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.02723, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.02723, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.02637, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.02637, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02530, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02524, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02484, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.02374, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02228, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02213, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02107, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.02090, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02008, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01948, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.01932, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01876, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01748, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01711, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01695, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01695, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01652, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01625, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01545, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01545, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01499, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01496, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01491, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.01482, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01476, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01475, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01464, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01449, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01443, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01438, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.01432, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.01376, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01357, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01270, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01257, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01235, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01235, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01214, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01205, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01199, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01197, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01187, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01186, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01182, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01177, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01152, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01115, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01113, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01083, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01055, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01055, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01040, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01037, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01037, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01036, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01020, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01019, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01014, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01010, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01007, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00993, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00992, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00983, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00980, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00976, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00965, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00963, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00958, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00955, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00928, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00927, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00925, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00912, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00911, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00908, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00851, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00838, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00778, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00759, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00758, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00682, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00660, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00596, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00590, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00533, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00526, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00522, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00518, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00503, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00483, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00481, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00471, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00470, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00449, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00447, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00445, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00435, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00434, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00431, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00430, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00425, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00421, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00414, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00409, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00407, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00407, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00402, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00394, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00389, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00384, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00383, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00368, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00364, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.00356, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00356, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00348, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00344, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00338, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00322, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00318, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00291, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00242, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00202, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00193, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00193, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00167, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00146, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00125, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00112, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00112, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00110, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00104, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00102, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00082, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00076, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00026, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:37:01  iter: 4000  loss: 2.1012 (5.6893)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2876 (0.3026)  rel_ce_loss: 0.7772 (1.9798)  1_CE_loss: 0.0051 (0.0423)  2_CE_loss: 0.0454 (0.2592)  2_DKS_loss: 0.0002 (0.0087)  3_CE_loss: 0.0372 (0.3789)  3_DKS_loss: 0.0036 (0.0273)  4_CE_loss: 0.3226 (1.0668)  4_DKS_loss: 0.0063 (0.0544)  5_CE_loss: 0.5100 (1.2924)  5_DKS_loss: 0.0440 (0.2769)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0779 (1.0851)  data: 0.0930 (0.1022)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:---Total norm 18.19124 clip coef 0.27486-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 8.31449, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 8.15466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 6.66729, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 6.05035, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 3.55319, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 3.40479, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 3.20970, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 3.06428, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 2.45247, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.88566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.88566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.88566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 1.88566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.68490, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.68323, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 1.61740, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.57993, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 1.53905, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.46525, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.43079, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 1.40051, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 1.34574, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.33244, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.28496, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 1.22736, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 1.11394, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 1.04009, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.93391, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.83860, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.81903, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.80918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.80747, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.78845, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.77126, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.76498, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.76482, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.76470, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.76256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.75760, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.71439, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.70982, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.70939, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.70727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.70084, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.69782, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.69426, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.64310, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.63512, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.59724, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.59688, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.54740, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.49610, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.45922, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.44492, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.42362, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.41967, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.41838, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.40597, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.40547, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.40238, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.39491, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.38582, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.38248, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.37763, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.37293, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.37293, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.36573, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.36366, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.36316, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.36177, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.35970, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.35907, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.35730, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.35687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.35629, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.35610, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.34867, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.34351, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.34126, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.34020, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.33638, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.32833, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.31496, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.31306, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.30896, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.30691, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29555, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.29172, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28854, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28719, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.28506, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.28038, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27480, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.27326, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.27274, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27233, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.26605, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.26554, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26453, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26242, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26206, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.26142, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.26117, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25901, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.25190, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.24927, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.24893, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.24456, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.24195, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23616, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23469, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.23413, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.22760, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.22586, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.22205, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.21910, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21420, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.21359, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20863, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20473, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.20338, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.20169, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.20145, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.20048, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.20033, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19997, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19551, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19386, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.18876, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.18624, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.18357, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.18313, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17872, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.17565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17523, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17509, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.17184, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.17088, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.17076, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16982, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.16969, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16670, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16455, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15806, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14921, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14822, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13760, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13727, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.13313, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.13169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13112, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12947, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12495, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12172, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12078, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11955, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11843, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.11696, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11684, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11520, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11229, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11227, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11072, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.10870, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.10722, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.10722, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.10564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.10477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.10380, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.10173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10150, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.10149, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.10149, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.10134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.09883, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09797, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09766, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09596, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09581, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09284, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08935, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08778, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.08517, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.08351, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.08001, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07990, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07899, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.07311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.07123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.07055, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07044, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06954, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06794, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.06742, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06586, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06557, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06142, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05984, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.05875, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.05875, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.05834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05664, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05519, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05479, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.05437, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.05373, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05346, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05238, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.05205, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.05177, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04971, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04845, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04657, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04444, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.04335, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04307, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.04258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.04227, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.04206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.04183, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.04141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03989, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03964, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03930, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03854, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.03747, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03583, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.03548, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.03548, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03441, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03355, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03304, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03236, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03187, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03110, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.03013, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02907, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02888, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02871, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02827, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02782, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02736, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02736, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02720, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02720, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02664, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02664, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02655, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02644, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02638, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02620, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02618, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.02587, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02549, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02546, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02522, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.02518, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02518, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02493, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02434, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02417, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02403, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02384, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02351, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02328, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02317, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02301, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02293, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02274, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02205, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.02164, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02112, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02102, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02090, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.02027, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02002, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01999, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01970, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01921, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01910, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01908, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01893, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01868, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01828, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01793, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01750, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01715, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01715, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.01674, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01658, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01614, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01602, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01581, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01555, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01537, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01530, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01518, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01456, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01410, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01309, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.01253, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01246, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01243, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01224, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01188, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01184, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.01142, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.01134, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01126, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01097, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01092, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01078, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01070, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01062, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01050, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01003, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01000, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00990, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00982, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00963, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00950, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00942, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00940, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00934, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00928, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00926, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00919, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00918, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00912, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00908, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00895, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00860, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00849, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00824, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00824, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00797, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00769, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00735, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00731, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00723, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00717, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00699, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00693, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00666, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00657, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00616, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00610, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00588, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00565, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00524, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00463, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00461, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00453, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00408, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00404, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00308, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00255, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00255, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00253, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00251, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00174, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00172, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00169, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00149, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00149, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 3:36:23  iter: 4000  loss: 2.5970 (6.6522)  obj_loss: 0.8516 (1.3496)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7783 (1.9142)  1_CE_loss: 0.0142 (0.1547)  2_CE_loss: 0.0502 (0.2714)  2_DKS_loss: 0.0007 (0.0078)  3_CE_loss: 0.0461 (0.3243)  3_DKS_loss: 0.0037 (0.0240)  4_CE_loss: 0.3462 (1.1409)  4_DKS_loss: 0.0095 (0.0469)  5_CE_loss: 0.4551 (1.1523)  5_DKS_loss: 0.0315 (0.2627)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0896 (1.0820)  data: 0.0903 (0.1011)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:35:16  iter: 4100  loss: 2.0093 (5.6029)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2854 (0.3026)  rel_ce_loss: 0.7566 (1.9510)  1_CE_loss: 0.0065 (0.0416)  2_CE_loss: 0.0455 (0.2542)  2_DKS_loss: 0.0003 (0.0085)  3_CE_loss: 0.0398 (0.3708)  3_DKS_loss: 0.0035 (0.0268)  4_CE_loss: 0.3519 (1.0494)  4_DKS_loss: 0.0066 (0.0533)  5_CE_loss: 0.4460 (1.2733)  5_DKS_loss: 0.0338 (0.2715)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0785 (1.0854)  data: 0.0898 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:34:34  iter: 4100  loss: 2.5684 (6.5537)  obj_loss: 0.8828 (1.3382)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7835 (1.8866)  1_CE_loss: 0.0071 (0.1513)  2_CE_loss: 0.0361 (0.2658)  2_DKS_loss: 0.0009 (0.0076)  3_CE_loss: 0.0450 (0.3174)  3_DKS_loss: 0.0030 (0.0236)  4_CE_loss: 0.3366 (1.1215)  4_DKS_loss: 0.0080 (0.0460)  5_CE_loss: 0.4706 (1.1352)  5_DKS_loss: 0.0196 (0.2570)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0682 (1.0819)  data: 0.0884 (0.1008)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:33:24  iter: 4200  loss: 2.1077 (5.5198)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2740 (0.3027)  rel_ce_loss: 0.8500 (1.9238)  1_CE_loss: 0.0100 (0.0409)  2_CE_loss: 0.0349 (0.2492)  2_DKS_loss: 0.0003 (0.0083)  3_CE_loss: 0.0330 (0.3629)  3_DKS_loss: 0.0033 (0.0263)  4_CE_loss: 0.2975 (1.0324)  4_DKS_loss: 0.0055 (0.0522)  5_CE_loss: 0.5101 (1.2551)  5_DKS_loss: 0.0290 (0.2660)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0858 (1.0851)  data: 0.0930 (0.1022)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:32:45  iter: 4200  loss: 2.5651 (6.4586)  obj_loss: 0.8618 (1.3272)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7245 (1.8599)  1_CE_loss: 0.0022 (0.1480)  2_CE_loss: 0.0335 (0.2605)  2_DKS_loss: 0.0006 (0.0075)  3_CE_loss: 0.0279 (0.3109)  3_DKS_loss: 0.0044 (0.0231)  4_CE_loss: 0.3224 (1.1029)  4_DKS_loss: 0.0078 (0.0451)  5_CE_loss: 0.4306 (1.1187)  5_DKS_loss: 0.0297 (0.2515)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0778 (1.0818)  data: 0.0873 (0.1006)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:31:39  iter: 4300  loss: 2.1931 (5.4515)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2915 (0.3029)  rel_ce_loss: 0.7694 (1.9015)  1_CE_loss: 0.0078 (0.0403)  2_CE_loss: 0.0428 (0.2447)  2_DKS_loss: 0.0002 (0.0081)  3_CE_loss: 0.0431 (0.3557)  3_DKS_loss: 0.0036 (0.0260)  4_CE_loss: 0.4004 (1.0176)  4_DKS_loss: 0.0061 (0.0513)  5_CE_loss: 0.4427 (1.2412)  5_DKS_loss: 0.0536 (0.2622)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0718 (1.0854)  data: 0.0917 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:31:02  iter: 4300  loss: 2.4866 (6.3687)  obj_loss: 0.8423 (1.3167)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6976 (1.8348)  1_CE_loss: 0.0097 (0.1449)  2_CE_loss: 0.0382 (0.2556)  2_DKS_loss: 0.0015 (0.0073)  3_CE_loss: 0.0304 (0.3047)  3_DKS_loss: 0.0029 (0.0227)  4_CE_loss: 0.3406 (1.0850)  4_DKS_loss: 0.0062 (0.0442)  5_CE_loss: 0.4043 (1.1030)  5_DKS_loss: 0.0247 (0.2463)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0690 (1.0823)  data: 0.0902 (0.1012)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:29:51  iter: 4400  loss: 1.9199 (5.3733)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2919 (0.3029)  rel_ce_loss: 0.7116 (1.8754)  1_CE_loss: 0.0023 (0.0397)  2_CE_loss: 0.0356 (0.2403)  2_DKS_loss: 0.0002 (0.0079)  3_CE_loss: 0.0257 (0.3485)  3_DKS_loss: 0.0012 (0.0255)  4_CE_loss: 0.3178 (1.0022)  4_DKS_loss: 0.0040 (0.0502)  5_CE_loss: 0.4418 (1.2235)  5_DKS_loss: 0.0222 (0.2571)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0773 (1.0855)  data: 0.0910 (0.1026)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:29:10  iter: 4400  loss: 2.4834 (6.2828)  obj_loss: 0.8677 (1.3072)  loss_cal: 0.0031 (0.0034)  rel_ce_loss: 0.7660 (1.8105)  1_CE_loss: 0.0092 (0.1419)  2_CE_loss: 0.0390 (0.2508)  2_DKS_loss: 0.0005 (0.0072)  3_CE_loss: 0.0362 (0.2987)  3_DKS_loss: 0.0024 (0.0222)  4_CE_loss: 0.2931 (1.0680)  4_DKS_loss: 0.0065 (0.0434)  5_CE_loss: 0.4243 (1.0880)  5_DKS_loss: 0.0211 (0.2414)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0581 (1.0819)  data: 0.0879 (0.1009)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:27:58  iter: 4500  loss: 2.0078 (5.3006)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2854 (0.3027)  rel_ce_loss: 0.7401 (1.8510)  1_CE_loss: 0.0020 (0.0391)  2_CE_loss: 0.0463 (0.2360)  2_DKS_loss: 0.0002 (0.0077)  3_CE_loss: 0.0408 (0.3418)  3_DKS_loss: 0.0036 (0.0250)  4_CE_loss: 0.3546 (0.9881)  4_DKS_loss: 0.0063 (0.0493)  5_CE_loss: 0.4752 (1.2074)  5_DKS_loss: 0.0324 (0.2524)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0652 (1.0851)  data: 0.0876 (0.1023)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:27:19  iter: 4500  loss: 2.7414 (6.2023)  obj_loss: 0.8750 (1.2975)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.8021 (1.7886)  1_CE_loss: 0.0082 (0.1390)  2_CE_loss: 0.0281 (0.2462)  2_DKS_loss: 0.0005 (0.0071)  3_CE_loss: 0.0356 (0.2930)  3_DKS_loss: 0.0021 (0.0218)  4_CE_loss: 0.3229 (1.0516)  4_DKS_loss: 0.0054 (0.0426)  5_CE_loss: 0.4656 (1.0748)  5_DKS_loss: 0.0296 (0.2368)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.0817)  data: 0.0855 (0.1006)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:26:12  iter: 4600  loss: 1.9279 (5.2452)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2866 (0.3031)  rel_ce_loss: 0.7388 (1.8341)  1_CE_loss: 0.0070 (0.0386)  2_CE_loss: 0.0343 (0.2320)  2_DKS_loss: 0.0002 (0.0076)  3_CE_loss: 0.0280 (0.3353)  3_DKS_loss: 0.0036 (0.0245)  4_CE_loss: 0.3016 (0.9748)  4_DKS_loss: 0.0074 (0.0485)  5_CE_loss: 0.4136 (1.1983)  5_DKS_loss: 0.0382 (0.2484)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0744 (1.0853)  data: 0.0892 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:25:34  iter: 4600  loss: 2.6923 (6.1260)  obj_loss: 0.8496 (1.2883)  loss_cal: 0.0032 (0.0034)  rel_ce_loss: 0.8362 (1.7677)  1_CE_loss: 0.0132 (0.1363)  2_CE_loss: 0.0340 (0.2418)  2_DKS_loss: 0.0009 (0.0069)  3_CE_loss: 0.0402 (0.2876)  3_DKS_loss: 0.0074 (0.0214)  4_CE_loss: 0.3658 (1.0361)  4_DKS_loss: 0.0130 (0.0419)  5_CE_loss: 0.4812 (1.0620)  5_DKS_loss: 0.0541 (0.2327)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0708 (1.0820)  data: 0.0904 (0.1011)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:24:30  iter: 4700  loss: 2.4601 (5.1937)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3328 (0.3035)  rel_ce_loss: 0.9764 (1.8183)  1_CE_loss: 0.0080 (0.0380)  2_CE_loss: 0.0424 (0.2282)  2_DKS_loss: 0.0004 (0.0074)  3_CE_loss: 0.0393 (0.3292)  3_DKS_loss: 0.0050 (0.0242)  4_CE_loss: 0.3820 (0.9621)  4_DKS_loss: 0.0102 (0.0476)  5_CE_loss: 0.5442 (1.1904)  5_DKS_loss: 0.0435 (0.2446)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0714 (1.0859)  data: 0.0886 (0.1029)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:23:45  iter: 4700  loss: 2.4172 (6.0493)  obj_loss: 0.7979 (1.2784)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7357 (1.7461)  1_CE_loss: 0.0028 (0.1336)  2_CE_loss: 0.0383 (0.2376)  2_DKS_loss: 0.0015 (0.0068)  3_CE_loss: 0.0384 (0.2824)  3_DKS_loss: 0.0044 (0.0211)  4_CE_loss: 0.3088 (1.0214)  4_DKS_loss: 0.0081 (0.0412)  5_CE_loss: 0.4167 (1.0489)  5_DKS_loss: 0.0273 (0.2284)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0413 (1.0819)  data: 0.0875 (0.1012)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:22:40  iter: 4800  loss: 2.2166 (5.1294)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3006 (0.3036)  rel_ce_loss: 0.8552 (1.7970)  1_CE_loss: 0.0173 (0.0375)  2_CE_loss: 0.0513 (0.2246)  2_DKS_loss: 0.0002 (0.0073)  3_CE_loss: 0.0454 (0.3234)  3_DKS_loss: 0.0049 (0.0238)  4_CE_loss: 0.2909 (0.9490)  4_DKS_loss: 0.0166 (0.0469)  5_CE_loss: 0.5322 (1.1758)  5_DKS_loss: 0.0429 (0.2405)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0792 (1.0858)  data: 0.0925 (0.1029)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:21:57  iter: 4800  loss: 2.4141 (5.9761)  obj_loss: 0.8403 (1.2697)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7240 (1.7255)  1_CE_loss: 0.0027 (0.1312)  2_CE_loss: 0.0290 (0.2336)  2_DKS_loss: 0.0007 (0.0067)  3_CE_loss: 0.0432 (0.2774)  3_DKS_loss: 0.0035 (0.0208)  4_CE_loss: 0.3440 (1.0070)  4_DKS_loss: 0.0065 (0.0405)  5_CE_loss: 0.4158 (1.0360)  5_DKS_loss: 0.0210 (0.2243)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0626 (1.0819)  data: 0.0875 (0.1011)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:20:49  iter: 4900  loss: 2.0173 (5.0665)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2833 (0.3036)  rel_ce_loss: 0.7731 (1.7760)  1_CE_loss: 0.0068 (0.0371)  2_CE_loss: 0.0407 (0.2209)  2_DKS_loss: 0.0002 (0.0071)  3_CE_loss: 0.0364 (0.3176)  3_DKS_loss: 0.0022 (0.0234)  4_CE_loss: 0.2883 (0.9367)  4_DKS_loss: 0.0063 (0.0462)  5_CE_loss: 0.4284 (1.1615)  5_DKS_loss: 0.0359 (0.2365)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0761 (1.0855)  data: 0.0900 (0.1027)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:20:06  iter: 4900  loss: 2.7131 (5.9066)  obj_loss: 0.7837 (1.2615)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.8077 (1.7061)  1_CE_loss: 0.0087 (0.1287)  2_CE_loss: 0.0317 (0.2297)  2_DKS_loss: 0.0013 (0.0066)  3_CE_loss: 0.0303 (0.2726)  3_DKS_loss: 0.0090 (0.0205)  4_CE_loss: 0.3389 (0.9931)  4_DKS_loss: 0.0112 (0.0399)  5_CE_loss: 0.4578 (1.0242)  5_DKS_loss: 0.0256 (0.2202)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0622 (1.0816)  data: 0.0866 (0.1009)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:18:59  iter: 5000  loss: 2.2450 (5.0073)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2917 (0.3037)  rel_ce_loss: 0.8640 (1.7566)  1_CE_loss: 0.0103 (0.0366)  2_CE_loss: 0.0362 (0.2173)  2_DKS_loss: 0.0001 (0.0070)  3_CE_loss: 0.0316 (0.3120)  3_DKS_loss: 0.0066 (0.0230)  4_CE_loss: 0.3170 (0.9248)  4_DKS_loss: 0.0081 (0.0454)  5_CE_loss: 0.4641 (1.1484)  5_DKS_loss: 0.0422 (0.2326)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0846 (1.0854)  data: 0.0924 (0.1025)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:18:19  iter: 5000  loss: 2.3908 (5.8383)  obj_loss: 0.8105 (1.2534)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7272 (1.6868)  1_CE_loss: 0.0125 (0.1264)  2_CE_loss: 0.0260 (0.2260)  2_DKS_loss: 0.0010 (0.0065)  3_CE_loss: 0.0349 (0.2680)  3_DKS_loss: 0.0030 (0.0202)  4_CE_loss: 0.3179 (0.9798)  4_DKS_loss: 0.0068 (0.0392)  5_CE_loss: 0.4361 (1.0124)  5_DKS_loss: 0.0191 (0.2162)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0866 (1.0817)  data: 0.0868 (0.1009)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:17:08  iter: 5100  loss: 1.9479 (4.9480)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2992 (0.3035)  rel_ce_loss: 0.7124 (1.7366)  1_CE_loss: 0.0113 (0.0362)  2_CE_loss: 0.0357 (0.2140)  2_DKS_loss: 0.0003 (0.0069)  3_CE_loss: 0.0325 (0.3068)  3_DKS_loss: 0.0027 (0.0227)  4_CE_loss: 0.3257 (0.9132)  4_DKS_loss: 0.0073 (0.0447)  5_CE_loss: 0.4345 (1.1348)  5_DKS_loss: 0.0267 (0.2287)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0729 (1.0852)  data: 0.0889 (0.1024)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:16:28  iter: 5100  loss: 2.5111 (5.7732)  obj_loss: 0.8384 (1.2454)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7541 (1.6686)  1_CE_loss: 0.0054 (0.1242)  2_CE_loss: 0.0368 (0.2225)  2_DKS_loss: 0.0012 (0.0064)  3_CE_loss: 0.0418 (0.2637)  3_DKS_loss: 0.0033 (0.0199)  4_CE_loss: 0.3231 (0.9669)  4_DKS_loss: 0.0063 (0.0386)  5_CE_loss: 0.4406 (1.0010)  5_DKS_loss: 0.0379 (0.2126)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0735 (1.0815)  data: 0.0882 (0.1007)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:15:16  iter: 5200  loss: 1.8635 (4.8910)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2828 (0.3036)  rel_ce_loss: 0.7151 (1.7177)  1_CE_loss: 0.0025 (0.0357)  2_CE_loss: 0.0254 (0.2106)  2_DKS_loss: 0.0002 (0.0067)  3_CE_loss: 0.0253 (0.3016)  3_DKS_loss: 0.0013 (0.0223)  4_CE_loss: 0.3304 (0.9016)  4_DKS_loss: 0.0067 (0.0440)  5_CE_loss: 0.4589 (1.1222)  5_DKS_loss: 0.0337 (0.2250)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0452 (1.0849)  data: 0.0895 (0.1022)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:14:38  iter: 5200  loss: 2.4805 (5.7106)  obj_loss: 0.8711 (1.2383)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7235 (1.6508)  1_CE_loss: 0.0035 (0.1220)  2_CE_loss: 0.0382 (0.2191)  2_DKS_loss: 0.0006 (0.0063)  3_CE_loss: 0.0376 (0.2593)  3_DKS_loss: 0.0036 (0.0195)  4_CE_loss: 0.3060 (0.9546)  4_DKS_loss: 0.0058 (0.0380)  5_CE_loss: 0.4508 (0.9900)  5_DKS_loss: 0.0236 (0.2092)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0712 (1.0814)  data: 0.0875 (0.1004)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:13:24  iter: 5300  loss: 1.9569 (4.8393)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3003 (0.3035)  rel_ce_loss: 0.7625 (1.7008)  1_CE_loss: 0.0031 (0.0353)  2_CE_loss: 0.0393 (0.2075)  2_DKS_loss: 0.0003 (0.0066)  3_CE_loss: 0.0344 (0.2967)  3_DKS_loss: 0.0013 (0.0219)  4_CE_loss: 0.3363 (0.8910)  4_DKS_loss: 0.0049 (0.0433)  5_CE_loss: 0.4265 (1.1111)  5_DKS_loss: 0.0388 (0.2216)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0672 (1.0845)  data: 0.0873 (0.1020)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:12:47  iter: 5300  loss: 2.5037 (5.6502)  obj_loss: 0.8936 (1.2310)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6981 (1.6339)  1_CE_loss: 0.0064 (0.1200)  2_CE_loss: 0.0311 (0.2157)  2_DKS_loss: 0.0004 (0.0062)  3_CE_loss: 0.0277 (0.2553)  3_DKS_loss: 0.0021 (0.0192)  4_CE_loss: 0.3063 (0.9428)  4_DKS_loss: 0.0046 (0.0374)  5_CE_loss: 0.4005 (0.9796)  5_DKS_loss: 0.0244 (0.2057)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0627 (1.0810)  data: 0.0873 (0.1002)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:11:36  iter: 5400  loss: 1.8855 (4.7853)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2946 (0.3035)  rel_ce_loss: 0.7038 (1.6828)  1_CE_loss: 0.0117 (0.0349)  2_CE_loss: 0.0421 (0.2044)  2_DKS_loss: 0.0003 (0.0065)  3_CE_loss: 0.0369 (0.2920)  3_DKS_loss: 0.0025 (0.0216)  4_CE_loss: 0.2966 (0.8803)  4_DKS_loss: 0.0062 (0.0426)  5_CE_loss: 0.4052 (1.0988)  5_DKS_loss: 0.0226 (0.2180)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0940 (1.0845)  data: 0.0975 (0.1019)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:10:59  iter: 5400  loss: 2.4564 (5.5928)  obj_loss: 0.8784 (1.2241)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7424 (1.6179)  1_CE_loss: 0.0062 (0.1180)  2_CE_loss: 0.0375 (0.2126)  2_DKS_loss: 0.0005 (0.0061)  3_CE_loss: 0.0381 (0.2514)  3_DKS_loss: 0.0030 (0.0190)  4_CE_loss: 0.2860 (0.9313)  4_DKS_loss: 0.0054 (0.0368)  5_CE_loss: 0.3891 (0.9699)  5_DKS_loss: 0.0189 (0.2024)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0962 (1.0811)  data: 0.0871 (0.1001)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:09:48  iter: 5500  loss: 1.8992 (4.7350)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3086 (0.3035)  rel_ce_loss: 0.7104 (1.6661)  1_CE_loss: 0.0094 (0.0344)  2_CE_loss: 0.0397 (0.2015)  2_DKS_loss: 0.0002 (0.0064)  3_CE_loss: 0.0370 (0.2875)  3_DKS_loss: 0.0026 (0.0213)  4_CE_loss: 0.3040 (0.8703)  4_DKS_loss: 0.0040 (0.0419)  5_CE_loss: 0.4183 (1.0874)  5_DKS_loss: 0.0180 (0.2146)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0980 (1.0846)  data: 0.0909 (0.1019)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:09:10  iter: 5500  loss: 2.5027 (5.5360)  obj_loss: 0.8423 (1.2172)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7010 (1.6018)  1_CE_loss: 0.0068 (0.1161)  2_CE_loss: 0.0283 (0.2095)  2_DKS_loss: 0.0005 (0.0060)  3_CE_loss: 0.0305 (0.2476)  3_DKS_loss: 0.0021 (0.0187)  4_CE_loss: 0.3205 (0.9204)  4_DKS_loss: 0.0057 (0.0363)  5_CE_loss: 0.4094 (0.9599)  5_DKS_loss: 0.0259 (0.1992)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0648 (1.0810)  data: 0.0875 (0.1002)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:08:03  iter: 5600  loss: 1.7999 (4.6866)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3120 (0.3036)  rel_ce_loss: 0.6665 (1.6500)  1_CE_loss: 0.0139 (0.0341)  2_CE_loss: 0.0339 (0.1988)  2_DKS_loss: 0.0003 (0.0063)  3_CE_loss: 0.0222 (0.2831)  3_DKS_loss: 0.0018 (0.0210)  4_CE_loss: 0.2711 (0.8606)  4_DKS_loss: 0.0041 (0.0413)  5_CE_loss: 0.4121 (1.0764)  5_DKS_loss: 0.0206 (0.2115)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0772 (1.0850)  data: 0.0910 (0.1022)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:07:26  iter: 5600  loss: 2.3898 (5.4821)  obj_loss: 0.8745 (1.2111)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6894 (1.5866)  1_CE_loss: 0.0070 (0.1142)  2_CE_loss: 0.0275 (0.2065)  2_DKS_loss: 0.0013 (0.0059)  3_CE_loss: 0.0341 (0.2439)  3_DKS_loss: 0.0024 (0.0184)  4_CE_loss: 0.3115 (0.9099)  4_DKS_loss: 0.0062 (0.0357)  5_CE_loss: 0.4049 (0.9504)  5_DKS_loss: 0.0223 (0.1961)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0550 (1.0814)  data: 0.0904 (0.1005)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:06:14  iter: 5700  loss: 2.0459 (4.6397)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3113 (0.3036)  rel_ce_loss: 0.7728 (1.6344)  1_CE_loss: 0.0098 (0.0337)  2_CE_loss: 0.0412 (0.1961)  2_DKS_loss: 0.0003 (0.0062)  3_CE_loss: 0.0338 (0.2789)  3_DKS_loss: 0.0017 (0.0207)  4_CE_loss: 0.3279 (0.8514)  4_DKS_loss: 0.0040 (0.0407)  5_CE_loss: 0.4907 (1.0658)  5_DKS_loss: 0.0282 (0.2083)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0694 (1.0849)  data: 0.0865 (0.1021)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:05:34  iter: 5700  loss: 2.5454 (5.4297)  obj_loss: 0.8052 (1.2049)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7331 (1.5718)  1_CE_loss: 0.0088 (0.1125)  2_CE_loss: 0.0417 (0.2037)  2_DKS_loss: 0.0006 (0.0058)  3_CE_loss: 0.0283 (0.2404)  3_DKS_loss: 0.0027 (0.0181)  4_CE_loss: 0.3346 (0.8997)  4_DKS_loss: 0.0045 (0.0352)  5_CE_loss: 0.4394 (0.9411)  5_DKS_loss: 0.0208 (0.1930)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0674 (1.0811)  data: 0.0875 (0.1003)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:04:24  iter: 5800  loss: 1.8473 (4.5936)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3085 (0.3036)  rel_ce_loss: 0.7667 (1.6190)  1_CE_loss: 0.0013 (0.0333)  2_CE_loss: 0.0289 (0.1934)  2_DKS_loss: 0.0002 (0.0061)  3_CE_loss: 0.0273 (0.2747)  3_DKS_loss: 0.0012 (0.0204)  4_CE_loss: 0.3012 (0.8422)  4_DKS_loss: 0.0049 (0.0401)  5_CE_loss: 0.4260 (1.0554)  5_DKS_loss: 0.0357 (0.2053)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0937 (1.0848)  data: 0.0925 (0.1020)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:03:45  iter: 5800  loss: 2.3763 (5.3788)  obj_loss: 0.8452 (1.1985)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7109 (1.5575)  1_CE_loss: 0.0077 (0.1107)  2_CE_loss: 0.0387 (0.2010)  2_DKS_loss: 0.0008 (0.0057)  3_CE_loss: 0.0339 (0.2369)  3_DKS_loss: 0.0030 (0.0179)  4_CE_loss: 0.3237 (0.8899)  4_DKS_loss: 0.0044 (0.0347)  5_CE_loss: 0.3985 (0.9324)  5_DKS_loss: 0.0188 (0.1901)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0507 (1.0809)  data: 0.0875 (0.1002)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:02:32  iter: 5900  loss: 1.8212 (4.5501)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2926 (0.3036)  rel_ce_loss: 0.6600 (1.6042)  1_CE_loss: 0.0080 (0.0329)  2_CE_loss: 0.0323 (0.1909)  2_DKS_loss: 0.0002 (0.0060)  3_CE_loss: 0.0261 (0.2708)  3_DKS_loss: 0.0015 (0.0201)  4_CE_loss: 0.3106 (0.8340)  4_DKS_loss: 0.0052 (0.0395)  5_CE_loss: 0.4244 (1.0456)  5_DKS_loss: 0.0210 (0.2025)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0631 (1.0844)  data: 0.0910 (0.1018)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:01:56  iter: 5900  loss: 2.6544 (5.3323)  obj_loss: 0.8452 (1.1928)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7965 (1.5446)  1_CE_loss: 0.0103 (0.1091)  2_CE_loss: 0.0355 (0.1983)  2_DKS_loss: 0.0029 (0.0057)  3_CE_loss: 0.0298 (0.2336)  3_DKS_loss: 0.0052 (0.0177)  4_CE_loss: 0.3603 (0.8808)  4_DKS_loss: 0.0081 (0.0342)  5_CE_loss: 0.4611 (0.9246)  5_DKS_loss: 0.0311 (0.1875)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0800 (1.0808)  data: 0.0888 (0.1000)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 3:00:42  iter: 6000  loss: 1.7945 (4.5059)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3091 (0.3036)  rel_ce_loss: 0.6761 (1.5894)  1_CE_loss: 0.0105 (0.0326)  2_CE_loss: 0.0247 (0.1884)  2_DKS_loss: 0.0003 (0.0059)  3_CE_loss: 0.0207 (0.2669)  3_DKS_loss: 0.0020 (0.0198)  4_CE_loss: 0.3167 (0.8252)  4_DKS_loss: 0.0077 (0.0389)  5_CE_loss: 0.4140 (1.0354)  5_DKS_loss: 0.0256 (0.1997)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0678 (1.0842)  data: 0.0924 (0.1016)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 3:00:06  iter: 6000  loss: 2.6133 (5.2864)  obj_loss: 0.8286 (1.1871)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7998 (1.5322)  1_CE_loss: 0.0015 (0.1075)  2_CE_loss: 0.0483 (0.1958)  2_DKS_loss: 0.0006 (0.0056)  3_CE_loss: 0.0420 (0.2304)  3_DKS_loss: 0.0014 (0.0174)  4_CE_loss: 0.3081 (0.8714)  4_DKS_loss: 0.0047 (0.0338)  5_CE_loss: 0.4363 (0.9172)  5_DKS_loss: 0.0180 (0.1848)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0686 (1.0806)  data: 0.0885 (0.0998)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:58:52  iter: 6100  loss: 2.0689 (4.4646)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2879 (0.3036)  rel_ce_loss: 0.7513 (1.5756)  1_CE_loss: 0.0031 (0.0323)  2_CE_loss: 0.0299 (0.1860)  2_DKS_loss: 0.0002 (0.0058)  3_CE_loss: 0.0285 (0.2633)  3_DKS_loss: 0.0015 (0.0195)  4_CE_loss: 0.3335 (0.8169)  4_DKS_loss: 0.0053 (0.0384)  5_CE_loss: 0.4707 (1.0262)  5_DKS_loss: 0.0269 (0.1968)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0549 (1.0840)  data: 0.0880 (0.1014)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:58:16  iter: 6100  loss: 2.5332 (5.2403)  obj_loss: 0.8242 (1.1816)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7516 (1.5192)  1_CE_loss: 0.0052 (0.1059)  2_CE_loss: 0.0564 (0.1933)  2_DKS_loss: 0.0005 (0.0055)  3_CE_loss: 0.0459 (0.2273)  3_DKS_loss: 0.0023 (0.0172)  4_CE_loss: 0.3313 (0.8624)  4_DKS_loss: 0.0053 (0.0333)  5_CE_loss: 0.4083 (0.9092)  5_DKS_loss: 0.0189 (0.1821)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0666 (1.0804)  data: 0.0869 (0.0996)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:57:02  iter: 6200  loss: 1.8180 (4.4244)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3060 (0.3036)  rel_ce_loss: 0.7119 (1.5621)  1_CE_loss: 0.0067 (0.0320)  2_CE_loss: 0.0434 (0.1838)  2_DKS_loss: 0.0002 (0.0057)  3_CE_loss: 0.0405 (0.2598)  3_DKS_loss: 0.0017 (0.0192)  4_CE_loss: 0.2805 (0.8090)  4_DKS_loss: 0.0048 (0.0379)  5_CE_loss: 0.4120 (1.0171)  5_DKS_loss: 0.0186 (0.1942)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0657 (1.0839)  data: 0.0898 (0.1014)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:56:28  iter: 6200  loss: 2.6016 (5.1962)  obj_loss: 0.8740 (1.1764)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7474 (1.5067)  1_CE_loss: 0.0022 (0.1044)  2_CE_loss: 0.0313 (0.1908)  2_DKS_loss: 0.0009 (0.0055)  3_CE_loss: 0.0334 (0.2243)  3_DKS_loss: 0.0048 (0.0170)  4_CE_loss: 0.3329 (0.8538)  4_DKS_loss: 0.0082 (0.0329)  5_CE_loss: 0.4568 (0.9015)  5_DKS_loss: 0.0394 (0.1796)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0733 (1.0804)  data: 0.0907 (0.0996)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:55:12  iter: 6300  loss: 1.9105 (4.3846)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3013 (0.3036)  rel_ce_loss: 0.7174 (1.5487)  1_CE_loss: 0.0148 (0.0317)  2_CE_loss: 0.0302 (0.1816)  2_DKS_loss: 0.0004 (0.0056)  3_CE_loss: 0.0268 (0.2563)  3_DKS_loss: 0.0024 (0.0190)  4_CE_loss: 0.2985 (0.8012)  4_DKS_loss: 0.0048 (0.0374)  5_CE_loss: 0.4115 (1.0082)  5_DKS_loss: 0.0231 (0.1915)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0689 (1.0837)  data: 0.0878 (0.1012)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:54:39  iter: 6300  loss: 2.3924 (5.1526)  obj_loss: 0.8345 (1.1708)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7010 (1.4943)  1_CE_loss: 0.0085 (0.1030)  2_CE_loss: 0.0347 (0.1886)  2_DKS_loss: 0.0007 (0.0054)  3_CE_loss: 0.0366 (0.2215)  3_DKS_loss: 0.0031 (0.0168)  4_CE_loss: 0.3292 (0.8454)  4_DKS_loss: 0.0057 (0.0325)  5_CE_loss: 0.4139 (0.8938)  5_DKS_loss: 0.0174 (0.1772)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.0803)  data: 0.0858 (0.0995)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:53:23  iter: 6400  loss: 1.8478 (4.3460)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2920 (0.3035)  rel_ce_loss: 0.6670 (1.5356)  1_CE_loss: 0.0079 (0.0313)  2_CE_loss: 0.0356 (0.1794)  2_DKS_loss: 0.0002 (0.0055)  3_CE_loss: 0.0359 (0.2529)  3_DKS_loss: 0.0014 (0.0187)  4_CE_loss: 0.3096 (0.7937)  4_DKS_loss: 0.0043 (0.0369)  5_CE_loss: 0.3962 (0.9995)  5_DKS_loss: 0.0150 (0.1888)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0725 (1.0837)  data: 0.0926 (0.1012)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:52:50  iter: 6400  loss: 2.4845 (5.1114)  obj_loss: 0.8433 (1.1660)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7300 (1.4826)  1_CE_loss: 0.0053 (0.1016)  2_CE_loss: 0.0475 (0.1864)  2_DKS_loss: 0.0007 (0.0053)  3_CE_loss: 0.0402 (0.2187)  3_DKS_loss: 0.0024 (0.0165)  4_CE_loss: 0.3504 (0.8374)  4_DKS_loss: 0.0063 (0.0320)  5_CE_loss: 0.3887 (0.8866)  5_DKS_loss: 0.0160 (0.1747)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0632 (1.0802)  data: 0.0860 (0.0994)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:51:34  iter: 6500  loss: 1.8537 (4.3085)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3019 (0.3035)  rel_ce_loss: 0.6741 (1.5230)  1_CE_loss: 0.0081 (0.0311)  2_CE_loss: 0.0303 (0.1773)  2_DKS_loss: 0.0001 (0.0054)  3_CE_loss: 0.0287 (0.2496)  3_DKS_loss: 0.0027 (0.0185)  4_CE_loss: 0.2878 (0.7863)  4_DKS_loss: 0.0044 (0.0364)  5_CE_loss: 0.4313 (0.9911)  5_DKS_loss: 0.0280 (0.1863)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0784 (1.0836)  data: 0.0883 (0.1012)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:51:00  iter: 6500  loss: 2.4911 (5.0703)  obj_loss: 0.8018 (1.1611)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7370 (1.4711)  1_CE_loss: 0.0116 (0.1003)  2_CE_loss: 0.0403 (0.1842)  2_DKS_loss: 0.0019 (0.0053)  3_CE_loss: 0.0408 (0.2160)  3_DKS_loss: 0.0036 (0.0164)  4_CE_loss: 0.2980 (0.8293)  4_DKS_loss: 0.0058 (0.0316)  5_CE_loss: 0.3978 (0.8793)  5_DKS_loss: 0.0224 (0.1724)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0581 (1.0801)  data: 0.0852 (0.0994)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:49:45  iter: 6600  loss: 1.9766 (4.2732)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2883 (0.3035)  rel_ce_loss: 0.7652 (1.5114)  1_CE_loss: 0.0084 (0.0308)  2_CE_loss: 0.0307 (0.1753)  2_DKS_loss: 0.0002 (0.0054)  3_CE_loss: 0.0443 (0.2464)  3_DKS_loss: 0.0014 (0.0182)  4_CE_loss: 0.3064 (0.7790)  4_DKS_loss: 0.0047 (0.0359)  5_CE_loss: 0.4161 (0.9832)  5_DKS_loss: 0.0235 (0.1839)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0864 (1.0835)  data: 0.0899 (0.1011)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:49:15  iter: 6600  loss: 2.5564 (5.0305)  obj_loss: 0.8037 (1.1563)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7661 (1.4597)  1_CE_loss: 0.0127 (0.0989)  2_CE_loss: 0.0429 (0.1821)  2_DKS_loss: 0.0006 (0.0052)  3_CE_loss: 0.0366 (0.2135)  3_DKS_loss: 0.0022 (0.0161)  4_CE_loss: 0.3219 (0.8216)  4_DKS_loss: 0.0052 (0.0313)  5_CE_loss: 0.4324 (0.8724)  5_DKS_loss: 0.0159 (0.1701)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0675 (1.0803)  data: 0.0875 (0.0996)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:47:58  iter: 6700  loss: 1.7710 (4.2403)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2886 (0.3037)  rel_ce_loss: 0.6474 (1.5005)  1_CE_loss: 0.0031 (0.0305)  2_CE_loss: 0.0292 (0.1734)  2_DKS_loss: 0.0002 (0.0053)  3_CE_loss: 0.0331 (0.2434)  3_DKS_loss: 0.0020 (0.0180)  4_CE_loss: 0.3022 (0.7724)  4_DKS_loss: 0.0051 (0.0355)  5_CE_loss: 0.4251 (0.9758)  5_DKS_loss: 0.0184 (0.1818)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0820 (1.0837)  data: 0.0916 (0.1012)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:47:26  iter: 6700  loss: 2.3826 (4.9913)  obj_loss: 0.8594 (1.1516)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.7389 (1.4486)  1_CE_loss: 0.0116 (0.0976)  2_CE_loss: 0.0518 (0.1801)  2_DKS_loss: 0.0008 (0.0051)  3_CE_loss: 0.0414 (0.2109)  3_DKS_loss: 0.0021 (0.0159)  4_CE_loss: 0.2945 (0.8138)  4_DKS_loss: 0.0057 (0.0309)  5_CE_loss: 0.3887 (0.8654)  5_DKS_loss: 0.0172 (0.1679)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0677 (1.0803)  data: 0.0882 (0.0994)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:46:08  iter: 6800  loss: 2.0783 (4.2066)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2981 (0.3037)  rel_ce_loss: 0.7829 (1.4893)  1_CE_loss: 0.0117 (0.0303)  2_CE_loss: 0.0469 (0.1715)  2_DKS_loss: 0.0002 (0.0052)  3_CE_loss: 0.0436 (0.2405)  3_DKS_loss: 0.0020 (0.0178)  4_CE_loss: 0.3041 (0.7657)  4_DKS_loss: 0.0041 (0.0351)  5_CE_loss: 0.4384 (0.9682)  5_DKS_loss: 0.0193 (0.1794)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0538 (1.0835)  data: 0.0896 (0.1011)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:45:38  iter: 6800  loss: 2.3515 (4.9541)  obj_loss: 0.7393 (1.1471)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7445 (1.4382)  1_CE_loss: 0.0067 (0.0964)  2_CE_loss: 0.0324 (0.1781)  2_DKS_loss: 0.0006 (0.0051)  3_CE_loss: 0.0313 (0.2084)  3_DKS_loss: 0.0034 (0.0157)  4_CE_loss: 0.3329 (0.8066)  4_DKS_loss: 0.0049 (0.0305)  5_CE_loss: 0.4201 (0.8588)  5_DKS_loss: 0.0194 (0.1657)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0859 (1.0803)  data: 0.0886 (0.0994)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:44:18  iter: 6900  loss: 1.9320 (4.1731)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3085 (0.3037)  rel_ce_loss: 0.7482 (1.4781)  1_CE_loss: 0.0083 (0.0300)  2_CE_loss: 0.0400 (0.1697)  2_DKS_loss: 0.0002 (0.0051)  3_CE_loss: 0.0340 (0.2376)  3_DKS_loss: 0.0023 (0.0176)  4_CE_loss: 0.2960 (0.7590)  4_DKS_loss: 0.0071 (0.0347)  5_CE_loss: 0.4294 (0.9606)  5_DKS_loss: 0.0247 (0.1771)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0643 (1.0834)  data: 0.0908 (0.1010)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:43:48  iter: 6900  loss: 2.4432 (4.9176)  obj_loss: 0.8071 (1.1425)  loss_cal: 0.0033 (0.0034)  rel_ce_loss: 0.7428 (1.4281)  1_CE_loss: 0.0143 (0.0952)  2_CE_loss: 0.0466 (0.1761)  2_DKS_loss: 0.0005 (0.0050)  3_CE_loss: 0.0382 (0.2060)  3_DKS_loss: 0.0027 (0.0156)  4_CE_loss: 0.3168 (0.7994)  4_DKS_loss: 0.0044 (0.0301)  5_CE_loss: 0.4113 (0.8524)  5_DKS_loss: 0.0180 (0.1636)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0525 (1.0800)  data: 0.0871 (0.0993)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:42:31  iter: 7000  loss: 1.8742 (4.1405)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3103 (0.3037)  rel_ce_loss: 0.7035 (1.4672)  1_CE_loss: 0.0107 (0.0297)  2_CE_loss: 0.0417 (0.1679)  2_DKS_loss: 0.0003 (0.0051)  3_CE_loss: 0.0418 (0.2347)  3_DKS_loss: 0.0030 (0.0173)  4_CE_loss: 0.3006 (0.7524)  4_DKS_loss: 0.0067 (0.0343)  5_CE_loss: 0.3890 (0.9532)  5_DKS_loss: 0.0191 (0.1749)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0930 (1.0835)  data: 0.0932 (0.1010)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:42:00  iter: 7000  loss: 2.3117 (4.8824)  obj_loss: 0.8379 (1.1383)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6535 (1.4181)  1_CE_loss: 0.0011 (0.0940)  2_CE_loss: 0.0325 (0.1742)  2_DKS_loss: 0.0005 (0.0050)  3_CE_loss: 0.0191 (0.2036)  3_DKS_loss: 0.0016 (0.0154)  4_CE_loss: 0.2902 (0.7926)  4_DKS_loss: 0.0037 (0.0298)  5_CE_loss: 0.3970 (0.8463)  5_DKS_loss: 0.0156 (0.1616)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0783 (1.0801)  data: 0.0877 (0.0993)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:40:41  iter: 7100  loss: 1.7973 (4.1092)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3048 (0.3037)  rel_ce_loss: 0.6763 (1.4566)  1_CE_loss: 0.0020 (0.0295)  2_CE_loss: 0.0360 (0.1661)  2_DKS_loss: 0.0001 (0.0050)  3_CE_loss: 0.0423 (0.2320)  3_DKS_loss: 0.0016 (0.0171)  4_CE_loss: 0.2925 (0.7463)  4_DKS_loss: 0.0044 (0.0339)  5_CE_loss: 0.4312 (0.9462)  5_DKS_loss: 0.0154 (0.1729)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0732 (1.0833)  data: 0.0915 (0.1009)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:40:10  iter: 7100  loss: 2.4482 (4.8481)  obj_loss: 0.7798 (1.1342)  loss_cal: 0.0034 (0.0034)  rel_ce_loss: 0.7236 (1.4084)  1_CE_loss: 0.0024 (0.0928)  2_CE_loss: 0.0340 (0.1724)  2_DKS_loss: 0.0010 (0.0049)  3_CE_loss: 0.0334 (0.2014)  3_DKS_loss: 0.0030 (0.0152)  4_CE_loss: 0.3230 (0.7860)  4_DKS_loss: 0.0054 (0.0294)  5_CE_loss: 0.4398 (0.8403)  5_DKS_loss: 0.0188 (0.1596)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0645 (1.0799)  data: 0.0893 (0.0992)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:38:53  iter: 7200  loss: 1.8509 (4.0787)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2941 (0.3037)  rel_ce_loss: 0.7033 (1.4464)  1_CE_loss: 0.0120 (0.0292)  2_CE_loss: 0.0309 (0.1644)  2_DKS_loss: 0.0002 (0.0049)  3_CE_loss: 0.0344 (0.2294)  3_DKS_loss: 0.0014 (0.0169)  4_CE_loss: 0.2962 (0.7403)  4_DKS_loss: 0.0034 (0.0335)  5_CE_loss: 0.4212 (0.9392)  5_DKS_loss: 0.0163 (0.1708)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0671 (1.0833)  data: 0.0924 (0.1010)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:38:22  iter: 7200  loss: 2.2101 (4.8135)  obj_loss: 0.7495 (1.1299)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.7027 (1.3989)  1_CE_loss: 0.0110 (0.0917)  2_CE_loss: 0.0291 (0.1706)  2_DKS_loss: 0.0005 (0.0049)  3_CE_loss: 0.0293 (0.1991)  3_DKS_loss: 0.0013 (0.0150)  4_CE_loss: 0.2888 (0.7793)  4_DKS_loss: 0.0034 (0.0291)  5_CE_loss: 0.3600 (0.8342)  5_DKS_loss: 0.0117 (0.1576)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0744 (1.0799)  data: 0.0904 (0.0992)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:37:03  iter: 7300  loss: 1.8026 (4.0480)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2965 (0.3037)  rel_ce_loss: 0.6600 (1.4360)  1_CE_loss: 0.0031 (0.0290)  2_CE_loss: 0.0352 (0.1627)  2_DKS_loss: 0.0001 (0.0049)  3_CE_loss: 0.0303 (0.2268)  3_DKS_loss: 0.0020 (0.0168)  4_CE_loss: 0.2846 (0.7342)  4_DKS_loss: 0.0044 (0.0331)  5_CE_loss: 0.4018 (0.9321)  5_DKS_loss: 0.0207 (0.1688)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0774 (1.0832)  data: 0.0915 (0.1009)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:36:34  iter: 7300  loss: 2.2112 (4.7792)  obj_loss: 0.7422 (1.1252)  loss_cal: 0.0037 (0.0034)  rel_ce_loss: 0.6752 (1.3893)  1_CE_loss: 0.0026 (0.0906)  2_CE_loss: 0.0361 (0.1689)  2_DKS_loss: 0.0008 (0.0048)  3_CE_loss: 0.0348 (0.1970)  3_DKS_loss: 0.0022 (0.0149)  4_CE_loss: 0.2605 (0.7726)  4_DKS_loss: 0.0040 (0.0288)  5_CE_loss: 0.3962 (0.8282)  5_DKS_loss: 0.0186 (0.1557)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0848 (1.0798)  data: 0.0902 (0.0991)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:35:16  iter: 7400  loss: 1.8340 (4.0177)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3012 (0.3038)  rel_ce_loss: 0.6675 (1.4256)  1_CE_loss: 0.0023 (0.0288)  2_CE_loss: 0.0354 (0.1611)  2_DKS_loss: 0.0002 (0.0048)  3_CE_loss: 0.0380 (0.2243)  3_DKS_loss: 0.0018 (0.0166)  4_CE_loss: 0.2883 (0.7283)  4_DKS_loss: 0.0052 (0.0327)  5_CE_loss: 0.3907 (0.9250)  5_DKS_loss: 0.0185 (0.1668)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0940 (1.0833)  data: 0.0899 (0.1009)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:34:45  iter: 7400  loss: 2.2925 (4.7457)  obj_loss: 0.7739 (1.1203)  loss_cal: 0.0036 (0.0034)  rel_ce_loss: 0.6514 (1.3798)  1_CE_loss: 0.0114 (0.0895)  2_CE_loss: 0.0514 (0.1672)  2_DKS_loss: 0.0004 (0.0047)  3_CE_loss: 0.0425 (0.1949)  3_DKS_loss: 0.0017 (0.0147)  4_CE_loss: 0.2844 (0.7664)  4_DKS_loss: 0.0055 (0.0285)  5_CE_loss: 0.3717 (0.8224)  5_DKS_loss: 0.0178 (0.1539)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0600 (1.0797)  data: 0.0853 (0.0991)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:33:26  iter: 7500  loss: 1.7328 (3.9880)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3098 (0.3037)  rel_ce_loss: 0.6798 (1.4155)  1_CE_loss: 0.0032 (0.0286)  2_CE_loss: 0.0380 (0.1595)  2_DKS_loss: 0.0001 (0.0047)  3_CE_loss: 0.0225 (0.2218)  3_DKS_loss: 0.0010 (0.0164)  4_CE_loss: 0.2690 (0.7225)  4_DKS_loss: 0.0029 (0.0324)  5_CE_loss: 0.3833 (0.9182)  5_DKS_loss: 0.0113 (0.1648)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0671 (1.0832)  data: 0.0927 (0.1008)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:32:57  iter: 7500  loss: 2.2418 (4.7131)  obj_loss: 0.6670 (1.1158)  loss_cal: 0.0035 (0.0034)  rel_ce_loss: 0.6644 (1.3707)  1_CE_loss: 0.0090 (0.0885)  2_CE_loss: 0.0362 (0.1655)  2_DKS_loss: 0.0007 (0.0047)  3_CE_loss: 0.0397 (0.1928)  3_DKS_loss: 0.0024 (0.0145)  4_CE_loss: 0.2764 (0.7602)  4_DKS_loss: 0.0047 (0.0282)  5_CE_loss: 0.3740 (0.8167)  5_DKS_loss: 0.0162 (0.1521)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0614 (1.0797)  data: 0.0875 (0.0991)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:31:38  iter: 7600  loss: 1.8806 (3.9604)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2873 (0.3037)  rel_ce_loss: 0.7551 (1.4064)  1_CE_loss: 0.0089 (0.0283)  2_CE_loss: 0.0391 (0.1579)  2_DKS_loss: 0.0002 (0.0047)  3_CE_loss: 0.0310 (0.2194)  3_DKS_loss: 0.0019 (0.0162)  4_CE_loss: 0.3108 (0.7170)  4_DKS_loss: 0.0039 (0.0320)  5_CE_loss: 0.4265 (0.9119)  5_DKS_loss: 0.0195 (0.1629)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0607 (1.0832)  data: 0.0896 (0.1008)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:31:08  iter: 7600  loss: 2.3357 (4.6817)  obj_loss: 0.8301 (1.1112)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6741 (1.3619)  1_CE_loss: 0.0079 (0.0875)  2_CE_loss: 0.0356 (0.1640)  2_DKS_loss: 0.0003 (0.0046)  3_CE_loss: 0.0382 (0.1909)  3_DKS_loss: 0.0017 (0.0144)  4_CE_loss: 0.2901 (0.7543)  4_DKS_loss: 0.0057 (0.0279)  5_CE_loss: 0.3599 (0.8112)  5_DKS_loss: 0.0145 (0.1503)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0813 (1.0796)  data: 0.0879 (0.0990)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:29:49  iter: 7700  loss: 1.8196 (3.9328)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2905 (0.3038)  rel_ce_loss: 0.6856 (1.3969)  1_CE_loss: 0.0092 (0.0281)  2_CE_loss: 0.0287 (0.1564)  2_DKS_loss: 0.0003 (0.0046)  3_CE_loss: 0.0223 (0.2170)  3_DKS_loss: 0.0040 (0.0160)  4_CE_loss: 0.2888 (0.7115)  4_DKS_loss: 0.0038 (0.0317)  5_CE_loss: 0.4144 (0.9056)  5_DKS_loss: 0.0182 (0.1611)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0695 (1.0830)  data: 0.0925 (0.1007)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:29:19  iter: 7700  loss: 2.2152 (4.6507)  obj_loss: 0.7310 (1.1070)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6860 (1.3532)  1_CE_loss: 0.0029 (0.0865)  2_CE_loss: 0.0295 (0.1624)  2_DKS_loss: 0.0004 (0.0046)  3_CE_loss: 0.0263 (0.1889)  3_DKS_loss: 0.0021 (0.0142)  4_CE_loss: 0.2764 (0.7484)  4_DKS_loss: 0.0045 (0.0276)  5_CE_loss: 0.3914 (0.8059)  5_DKS_loss: 0.0166 (0.1486)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0548 (1.0795)  data: 0.0872 (0.0989)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:27:59  iter: 7800  loss: 1.8301 (3.9057)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2983 (0.3037)  rel_ce_loss: 0.7130 (1.3877)  1_CE_loss: 0.0023 (0.0279)  2_CE_loss: 0.0339 (0.1550)  2_DKS_loss: 0.0002 (0.0046)  3_CE_loss: 0.0373 (0.2148)  3_DKS_loss: 0.0010 (0.0158)  4_CE_loss: 0.3047 (0.7062)  4_DKS_loss: 0.0033 (0.0313)  5_CE_loss: 0.4022 (0.8994)  5_DKS_loss: 0.0161 (0.1593)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0707 (1.0828)  data: 0.0888 (0.1006)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:27:30  iter: 7800  loss: 2.1893 (4.6202)  obj_loss: 0.7388 (1.1025)  loss_cal: 0.0034 (0.0035)  rel_ce_loss: 0.6068 (1.3446)  1_CE_loss: 0.0045 (0.0855)  2_CE_loss: 0.0346 (0.1609)  2_DKS_loss: 0.0009 (0.0046)  3_CE_loss: 0.0317 (0.1870)  3_DKS_loss: 0.0020 (0.0141)  4_CE_loss: 0.2966 (0.7427)  4_DKS_loss: 0.0046 (0.0273)  5_CE_loss: 0.3723 (0.8007)  5_DKS_loss: 0.0183 (0.1469)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0587 (1.0793)  data: 0.0878 (0.0989)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:26:09  iter: 7900  loss: 1.7985 (3.8791)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3256 (0.3038)  rel_ce_loss: 0.6325 (1.3787)  1_CE_loss: 0.0080 (0.0278)  2_CE_loss: 0.0323 (0.1535)  2_DKS_loss: 0.0001 (0.0045)  3_CE_loss: 0.0303 (0.2126)  3_DKS_loss: 0.0021 (0.0157)  4_CE_loss: 0.2863 (0.7009)  4_DKS_loss: 0.0033 (0.0310)  5_CE_loss: 0.3692 (0.8932)  5_DKS_loss: 0.0167 (0.1575)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0466 (1.0827)  data: 0.0890 (0.1006)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:25:41  iter: 7900  loss: 2.4281 (4.5909)  obj_loss: 0.8145 (1.0985)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.7362 (1.3364)  1_CE_loss: 0.0079 (0.0846)  2_CE_loss: 0.0323 (0.1593)  2_DKS_loss: 0.0011 (0.0045)  3_CE_loss: 0.0351 (0.1851)  3_DKS_loss: 0.0027 (0.0139)  4_CE_loss: 0.3104 (0.7371)  4_DKS_loss: 0.0054 (0.0270)  5_CE_loss: 0.4040 (0.7956)  5_DKS_loss: 0.0242 (0.1453)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0797 (1.0791)  data: 0.0890 (0.0988)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:---Total norm 3.55856 clip coef 1.40506-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.57166, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 1.53752, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.12818, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.98455, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 0.88815, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.62079, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.61112, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.55599, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.51609, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.46681, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.46598, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.45092, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.43180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.43180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.43180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.43180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.41552, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.40380, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.40105, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.39809, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.39637, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.39475, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.35926, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.28018, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.26089, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.23434, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20222, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.19035, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18027, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.18015, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.17285, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16898, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16704, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15952, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.15187, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.14086, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.13505, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.13447, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13172, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.12763, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.12679, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.12475, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.12118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12076, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.11784, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11702, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11491, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10945, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10752, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10159, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.09461, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.09461, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08957, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08900, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.08332, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.08270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08010, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07871, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07827, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07809, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07797, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.07734, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07592, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07588, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07504, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.07117, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.07105, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06768, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06705, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06689, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.06682, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06568, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06518, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06380, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06323, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05890, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.05843, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05819, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05759, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05712, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05626, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05551, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05539, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05326, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.05099, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04996, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04880, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04758, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04565, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04346, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04344, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04329, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04313, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04310, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04290, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04283, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04260, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04211, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04183, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04139, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.03951, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.03951, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03945, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03901, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03896, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03890, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03868, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03852, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03815, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03810, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03784, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03752, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03721, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03719, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03716, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03698, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03560, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03534, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03441, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03422, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03387, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03286, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03210, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03177, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03120, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03114, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03111, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03082, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03045, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02983, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02840, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02704, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02668, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02621, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02526, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02310, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02283, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02223, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02177, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02131, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02115, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02033, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02026, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02000, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01996, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01976, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01969, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01964, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01926, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01904, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.01877, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01860, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01807, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01791, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01770, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01760, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01727, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01715, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.01703, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.01703, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01701, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01647, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01639, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01632, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01575, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01565, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01540, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01528, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01528, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01527, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01505, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01505, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01483, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01453, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01443, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01431, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01411, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01409, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01396, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01290, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.01241, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01145, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01145, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01135, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01134, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01120, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01096, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01089, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01086, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01080, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01079, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01055, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01045, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01044, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01030, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00925, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00925, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.00900, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00873, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00857, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00826, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00778, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.00767, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00742, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00732, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00732, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00723, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00720, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00694, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00684, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00679, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00677, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00669, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00663, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00646, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00640, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00635, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00606, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00606, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00604, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00603, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00588, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00587, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00582, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00581, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.00572, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00571, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00554, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00549, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00539, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00537, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00503, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00492, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00487, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00486, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00476, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00462, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00462, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00424, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00401, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00395, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00355, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00336, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00330, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00327, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00325, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00324, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00319, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00319, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00318, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00318, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00316, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00315, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00303, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00303, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00302, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00298, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00298, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00296, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00295, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00292, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00292, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00292, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00290, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00286, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00285, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00282, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00281, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00272, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00271, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00271, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00268, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00267, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00266, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00264, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00263, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00263, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00256, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00255, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00254, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00254, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00248, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00243, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00237, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00236, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00227, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00208, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00208, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00206, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00203, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00202, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00202, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00200, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00185, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00171, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00143, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00136, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00131, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00127, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00126, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00123, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00121, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00117, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00115, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00114, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00113, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00112, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00111, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00111, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00110, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00110, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00103, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00103, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00103, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00102, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00102, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00098, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00093, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00092, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00089, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00088, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00081, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00073, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00071, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00069, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00066, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00064, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00058, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00058, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00054, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00049, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00045, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00045, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00041, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00033, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00031, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00025, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00025, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00024, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00015, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00014, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00004, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 2:24:22  iter: 8000  loss: 1.8172 (3.8532)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2996 (0.3038)  rel_ce_loss: 0.6752 (1.3698)  1_CE_loss: 0.0062 (0.0276)  2_CE_loss: 0.0320 (0.1521)  2_DKS_loss: 0.0003 (0.0045)  3_CE_loss: 0.0348 (0.2104)  3_DKS_loss: 0.0026 (0.0155)  4_CE_loss: 0.2750 (0.6958)  4_DKS_loss: 0.0073 (0.0307)  5_CE_loss: 0.4084 (0.8872)  5_DKS_loss: 0.0271 (0.1558)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0907 (1.0828)  data: 0.0913 (0.1006)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:---Total norm 5.58734 clip coef 0.89488-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 2.34224, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.60050, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.40688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.39726, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 1.36880, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.36794, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.29756, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.96667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 0.96159, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.89428, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.76855, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.74958, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.73853, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.73438, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.69040, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.68543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.68543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.68543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.68543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.66525, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.65169, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.63891, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.60905, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.59590, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.45067, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.43157, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.40970, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.38816, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.34096, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.33924, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.30729, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.30716, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.30001, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26467, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26180, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.26044, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.25247, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.24285, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23730, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.23687, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23043, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.22772, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22669, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.22577, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22556, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22547, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.22228, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22169, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.22057, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21958, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.21779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.21740, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21726, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21705, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21407, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.21290, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21241, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21223, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21144, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21136, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.20970, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20541, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.19632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.19457, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.19372, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19299, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.19148, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18829, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.18417, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18192, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.18120, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.17894, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17630, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17345, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17288, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17273, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17217, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16755, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16552, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.16481, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15863, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15858, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15689, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15665, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15579, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15487, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.15356, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.15339, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.15276, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15216, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14986, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14976, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14952, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14820, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14811, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14696, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14412, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14147, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14004, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14000, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13711, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13610, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.13384, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.13384, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13330, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13019, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12837, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12532, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12520, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.12432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12404, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12135, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12133, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.12059, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11935, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11739, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11681, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11571, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11507, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11506, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11404, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10997, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.10954, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10617, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10501, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10343, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10162, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10060, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09914, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09233, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.09079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09076, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08944, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08893, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08740, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08709, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08593, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08536, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08493, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08334, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08330, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08212, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08096, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08046, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08029, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08005, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07797, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07757, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07691, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07674, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07654, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07563, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07515, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07248, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07105, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07090, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06996, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06846, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.06643, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.06377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06260, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06236, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.06119, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06092, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06060, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05721, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05719, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05708, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05640, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.05380, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.05330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.05026, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.04952, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04921, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04742, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04723, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.04704, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04540, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.04393, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.04365, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.04292, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04185, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.04061, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.04061, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.03991, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03751, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.03737, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03667, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03656, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03580, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03505, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.03303, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03292, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.03290, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.03286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.03277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03249, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.03044, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02928, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02928, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02832, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02792, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02789, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02651, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02562, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.02550, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02273, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02272, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02234, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02125, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02103, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02075, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02074, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.02060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02017, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.02004, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01987, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01967, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01948, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01942, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01909, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01899, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01899, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01899, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01898, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01843, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01842, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01842, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01841, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01810, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01807, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01798, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01792, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01785, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01778, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01771, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01768, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.01757, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01737, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01736, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01734, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01727, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01714, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01680, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01671, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01644, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01641, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01600, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01570, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01555, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01549, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01545, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01510, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01470, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01445, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01436, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.01358, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.01358, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01308, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01297, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01225, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01223, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01200, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01146, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01140, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01093, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.01080, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01063, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.01029, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00998, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00984, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00982, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00966, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00960, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00950, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00946, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00944, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00944, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00905, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00891, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00887, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00881, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00878, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00877, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00861, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00842, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00819, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00813, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00810, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00809, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00803, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00789, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00789, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00773, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00763, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00756, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00744, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00711, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00711, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00696, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00691, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00689, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00680, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00660, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00657, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00635, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00629, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00611, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00609, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00606, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00604, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00603, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00602, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00581, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00580, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00579, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00563, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00556, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00549, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00544, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00537, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00535, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00533, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00520, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00519, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00490, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00456, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00453, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00453, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00444, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00432, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00428, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00428, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00392, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00387, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00348, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00341, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00331, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00323, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00263, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00247, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00245, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00232, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00232, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00191, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00190, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00176, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00175, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00174, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00152, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00144, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00119, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00096, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00076, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00048, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00041, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 2:23:53  iter: 8000  loss: 2.2300 (4.5627)  obj_loss: 0.7627 (1.0945)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6576 (1.3284)  1_CE_loss: 0.0038 (0.0837)  2_CE_loss: 0.0290 (0.1579)  2_DKS_loss: 0.0005 (0.0045)  3_CE_loss: 0.0375 (0.1833)  3_DKS_loss: 0.0020 (0.0138)  4_CE_loss: 0.2787 (0.7318)  4_DKS_loss: 0.0033 (0.0267)  5_CE_loss: 0.3843 (0.7909)  5_DKS_loss: 0.0120 (0.1438)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0717 (1.0791)  data: 0.0879 (0.0988)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:22:32  iter: 8100  loss: 1.7579 (3.8285)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3137 (0.3038)  rel_ce_loss: 0.6511 (1.3614)  1_CE_loss: 0.0072 (0.0274)  2_CE_loss: 0.0421 (0.1508)  2_DKS_loss: 0.0002 (0.0044)  3_CE_loss: 0.0365 (0.2084)  3_DKS_loss: 0.0019 (0.0154)  4_CE_loss: 0.2670 (0.6909)  4_DKS_loss: 0.0030 (0.0303)  5_CE_loss: 0.3813 (0.8815)  5_DKS_loss: 0.0126 (0.1541)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0767 (1.0826)  data: 0.0913 (0.1005)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:22:05  iter: 8100  loss: 2.2729 (4.5349)  obj_loss: 0.7231 (1.0905)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.7007 (1.3207)  1_CE_loss: 0.0039 (0.0828)  2_CE_loss: 0.0306 (0.1564)  2_DKS_loss: 0.0002 (0.0044)  3_CE_loss: 0.0394 (0.1816)  3_DKS_loss: 0.0018 (0.0137)  4_CE_loss: 0.2995 (0.7266)  4_DKS_loss: 0.0039 (0.0264)  5_CE_loss: 0.3711 (0.7861)  5_DKS_loss: 0.0139 (0.1422)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0957 (1.0792)  data: 0.0880 (0.0988)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:20:44  iter: 8200  loss: 1.8103 (3.8042)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3027 (0.3039)  rel_ce_loss: 0.7179 (1.3531)  1_CE_loss: 0.0035 (0.0272)  2_CE_loss: 0.0492 (0.1496)  2_DKS_loss: 0.0002 (0.0044)  3_CE_loss: 0.0360 (0.2064)  3_DKS_loss: 0.0011 (0.0152)  4_CE_loss: 0.2766 (0.6861)  4_DKS_loss: 0.0032 (0.0300)  5_CE_loss: 0.4158 (0.8759)  5_DKS_loss: 0.0129 (0.1525)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0700 (1.0827)  data: 0.0901 (0.1006)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:20:16  iter: 8200  loss: 2.0833 (4.5073)  obj_loss: 0.7393 (1.0864)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6556 (1.3131)  1_CE_loss: 0.0106 (0.0820)  2_CE_loss: 0.0316 (0.1550)  2_DKS_loss: 0.0003 (0.0044)  3_CE_loss: 0.0393 (0.1799)  3_DKS_loss: 0.0021 (0.0135)  4_CE_loss: 0.2778 (0.7214)  4_DKS_loss: 0.0042 (0.0262)  5_CE_loss: 0.3629 (0.7814)  5_DKS_loss: 0.0134 (0.1406)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0758 (1.0790)  data: 0.0900 (0.0987)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:18:55  iter: 8300  loss: 1.7440 (3.7801)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3023 (0.3039)  rel_ce_loss: 0.6574 (1.3449)  1_CE_loss: 0.0085 (0.0270)  2_CE_loss: 0.0375 (0.1482)  2_DKS_loss: 0.0002 (0.0043)  3_CE_loss: 0.0332 (0.2043)  3_DKS_loss: 0.0014 (0.0150)  4_CE_loss: 0.2830 (0.6814)  4_DKS_loss: 0.0035 (0.0297)  5_CE_loss: 0.3741 (0.8704)  5_DKS_loss: 0.0155 (0.1509)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0745 (1.0825)  data: 0.0911 (0.1005)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:18:27  iter: 8300  loss: 2.3606 (4.4811)  obj_loss: 0.8149 (1.0828)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.7301 (1.3059)  1_CE_loss: 0.0041 (0.0811)  2_CE_loss: 0.0354 (0.1536)  2_DKS_loss: 0.0013 (0.0043)  3_CE_loss: 0.0245 (0.1781)  3_DKS_loss: 0.0038 (0.0134)  4_CE_loss: 0.3121 (0.7164)  4_DKS_loss: 0.0043 (0.0259)  5_CE_loss: 0.4020 (0.7770)  5_DKS_loss: 0.0273 (0.1392)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.0789)  data: 0.0872 (0.0986)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:17:06  iter: 8400  loss: 1.6388 (3.7564)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3094 (0.3039)  rel_ce_loss: 0.6313 (1.3368)  1_CE_loss: 0.0079 (0.0268)  2_CE_loss: 0.0302 (0.1470)  2_DKS_loss: 0.0002 (0.0043)  3_CE_loss: 0.0301 (0.2024)  3_DKS_loss: 0.0012 (0.0149)  4_CE_loss: 0.2483 (0.6767)  4_DKS_loss: 0.0037 (0.0294)  5_CE_loss: 0.3838 (0.8649)  5_DKS_loss: 0.0137 (0.1492)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0897 (1.0824)  data: 0.0934 (0.1004)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:16:38  iter: 8400  loss: 2.2652 (4.4551)  obj_loss: 0.7197 (1.0790)  loss_cal: 0.0034 (0.0035)  rel_ce_loss: 0.6772 (1.2986)  1_CE_loss: 0.0068 (0.0803)  2_CE_loss: 0.0443 (0.1523)  2_DKS_loss: 0.0004 (0.0043)  3_CE_loss: 0.0416 (0.1765)  3_DKS_loss: 0.0023 (0.0133)  4_CE_loss: 0.3078 (0.7116)  4_DKS_loss: 0.0032 (0.0257)  5_CE_loss: 0.3710 (0.7724)  5_DKS_loss: 0.0144 (0.1377)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0656 (1.0788)  data: 0.0874 (0.0986)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:15:17  iter: 8500  loss: 1.6620 (3.7328)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3064 (0.3040)  rel_ce_loss: 0.6200 (1.3287)  1_CE_loss: 0.0101 (0.0267)  2_CE_loss: 0.0330 (0.1457)  2_DKS_loss: 0.0001 (0.0042)  3_CE_loss: 0.0312 (0.2004)  3_DKS_loss: 0.0009 (0.0147)  4_CE_loss: 0.2461 (0.6721)  4_DKS_loss: 0.0023 (0.0291)  5_CE_loss: 0.3737 (0.8595)  5_DKS_loss: 0.0094 (0.1476)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0864 (1.0823)  data: 0.0893 (0.1003)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:14:50  iter: 8500  loss: 2.2757 (4.4290)  obj_loss: 0.7612 (1.0755)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.7103 (1.2912)  1_CE_loss: 0.0036 (0.0794)  2_CE_loss: 0.0361 (0.1509)  2_DKS_loss: 0.0002 (0.0042)  3_CE_loss: 0.0339 (0.1749)  3_DKS_loss: 0.0017 (0.0131)  4_CE_loss: 0.2932 (0.7066)  4_DKS_loss: 0.0033 (0.0254)  5_CE_loss: 0.3745 (0.7678)  5_DKS_loss: 0.0141 (0.1363)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0560 (1.0787)  data: 0.0897 (0.0985)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:13:28  iter: 8600  loss: 1.7640 (3.7101)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2990 (0.3040)  rel_ce_loss: 0.6896 (1.3210)  1_CE_loss: 0.0074 (0.0265)  2_CE_loss: 0.0287 (0.1445)  2_DKS_loss: 0.0002 (0.0042)  3_CE_loss: 0.0290 (0.1986)  3_DKS_loss: 0.0010 (0.0146)  4_CE_loss: 0.2864 (0.6676)  4_DKS_loss: 0.0025 (0.0288)  5_CE_loss: 0.4165 (0.8543)  5_DKS_loss: 0.0127 (0.1461)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0702 (1.0823)  data: 0.0885 (0.1003)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:13:01  iter: 8600  loss: 2.3308 (4.4039)  obj_loss: 0.8345 (1.0722)  loss_cal: 0.0034 (0.0035)  rel_ce_loss: 0.7040 (1.2842)  1_CE_loss: 0.0111 (0.0787)  2_CE_loss: 0.0327 (0.1496)  2_DKS_loss: 0.0004 (0.0042)  3_CE_loss: 0.0250 (0.1733)  3_DKS_loss: 0.0016 (0.0130)  4_CE_loss: 0.2997 (0.7019)  4_DKS_loss: 0.0031 (0.0252)  5_CE_loss: 0.3840 (0.7633)  5_DKS_loss: 0.0144 (0.1349)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0758 (1.0786)  data: 0.0896 (0.0985)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:11:39  iter: 8700  loss: 1.7284 (3.6879)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2993 (0.3040)  rel_ce_loss: 0.6641 (1.3135)  1_CE_loss: 0.0024 (0.0263)  2_CE_loss: 0.0281 (0.1433)  2_DKS_loss: 0.0002 (0.0041)  3_CE_loss: 0.0250 (0.1967)  3_DKS_loss: 0.0011 (0.0144)  4_CE_loss: 0.2718 (0.6632)  4_DKS_loss: 0.0036 (0.0285)  5_CE_loss: 0.4221 (0.8492)  5_DKS_loss: 0.0132 (0.1445)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0771 (1.0821)  data: 0.0894 (0.1002)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:11:13  iter: 8700  loss: 2.2464 (4.3790)  obj_loss: 0.7207 (1.0687)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6569 (1.2771)  1_CE_loss: 0.0115 (0.0779)  2_CE_loss: 0.0480 (0.1485)  2_DKS_loss: 0.0005 (0.0042)  3_CE_loss: 0.0422 (0.1718)  3_DKS_loss: 0.0023 (0.0129)  4_CE_loss: 0.3012 (0.6972)  4_DKS_loss: 0.0047 (0.0249)  5_CE_loss: 0.3686 (0.7590)  5_DKS_loss: 0.0133 (0.1335)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0650 (1.0785)  data: 0.0868 (0.0983)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:09:51  iter: 8800  loss: 1.6751 (3.6665)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3138 (0.3040)  rel_ce_loss: 0.6177 (1.3062)  1_CE_loss: 0.0035 (0.0262)  2_CE_loss: 0.0298 (0.1421)  2_DKS_loss: 0.0001 (0.0041)  3_CE_loss: 0.0332 (0.1950)  3_DKS_loss: 0.0014 (0.0143)  4_CE_loss: 0.2627 (0.6590)  4_DKS_loss: 0.0027 (0.0282)  5_CE_loss: 0.4027 (0.8444)  5_DKS_loss: 0.0140 (0.1431)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0889 (1.0821)  data: 0.0943 (0.1002)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:09:24  iter: 8800  loss: 2.2197 (4.3553)  obj_loss: 0.7817 (1.0655)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6650 (1.2704)  1_CE_loss: 0.0023 (0.0772)  2_CE_loss: 0.0323 (0.1472)  2_DKS_loss: 0.0005 (0.0041)  3_CE_loss: 0.0258 (0.1703)  3_DKS_loss: 0.0025 (0.0127)  4_CE_loss: 0.2667 (0.6927)  4_DKS_loss: 0.0036 (0.0247)  5_CE_loss: 0.3666 (0.7548)  5_DKS_loss: 0.0132 (0.1321)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0575 (1.0784)  data: 0.0865 (0.0983)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:08:03  iter: 8900  loss: 1.7102 (3.6452)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3023 (0.3040)  rel_ce_loss: 0.6340 (1.2990)  1_CE_loss: 0.0075 (0.0260)  2_CE_loss: 0.0273 (0.1410)  2_DKS_loss: 0.0001 (0.0040)  3_CE_loss: 0.0211 (0.1932)  3_DKS_loss: 0.0013 (0.0141)  4_CE_loss: 0.2563 (0.6548)  4_DKS_loss: 0.0031 (0.0279)  5_CE_loss: 0.3652 (0.8394)  5_DKS_loss: 0.0139 (0.1417)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0778 (1.0821)  data: 0.0892 (0.1002)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:07:36  iter: 8900  loss: 2.2999 (4.3321)  obj_loss: 0.7964 (1.0626)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6575 (1.2638)  1_CE_loss: 0.0078 (0.0764)  2_CE_loss: 0.0360 (0.1460)  2_DKS_loss: 0.0003 (0.0041)  3_CE_loss: 0.0326 (0.1688)  3_DKS_loss: 0.0014 (0.0126)  4_CE_loss: 0.3083 (0.6882)  4_DKS_loss: 0.0029 (0.0244)  5_CE_loss: 0.3944 (0.7508)  5_DKS_loss: 0.0181 (0.1308)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0568 (1.0784)  data: 0.0889 (0.0983)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:06:14  iter: 9000  loss: 1.8325 (3.6243)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2992 (0.3040)  rel_ce_loss: 0.6716 (1.2918)  1_CE_loss: 0.0030 (0.0258)  2_CE_loss: 0.0573 (0.1399)  2_DKS_loss: 0.0001 (0.0040)  3_CE_loss: 0.0425 (0.1915)  3_DKS_loss: 0.0010 (0.0140)  4_CE_loss: 0.2873 (0.6507)  4_DKS_loss: 0.0021 (0.0276)  5_CE_loss: 0.4100 (0.8346)  5_DKS_loss: 0.0123 (0.1402)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0803 (1.0820)  data: 0.0902 (0.1001)  lr: 0.016000  max mem: 10732
INFO:maskrcnn_benchmark:eta: 2:05:47  iter: 9000  loss: 2.2178 (4.3091)  obj_loss: 0.7559 (1.0594)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6576 (1.2572)  1_CE_loss: 0.0054 (0.0757)  2_CE_loss: 0.0380 (0.1449)  2_DKS_loss: 0.0006 (0.0040)  3_CE_loss: 0.0285 (0.1674)  3_DKS_loss: 0.0013 (0.0125)  4_CE_loss: 0.2857 (0.6840)  4_DKS_loss: 0.0039 (0.0242)  5_CE_loss: 0.3964 (0.7468)  5_DKS_loss: 0.0119 (0.1296)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0777 (1.0783)  data: 0.0865 (0.0982)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:04:25  iter: 9100  loss: 1.6984 (3.6039)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3102 (0.3041)  rel_ce_loss: 0.6370 (1.2850)  1_CE_loss: 0.0037 (0.0257)  2_CE_loss: 0.0229 (0.1388)  2_DKS_loss: 0.0001 (0.0040)  3_CE_loss: 0.0302 (0.1897)  3_DKS_loss: 0.0017 (0.0139)  4_CE_loss: 0.2646 (0.6467)  4_DKS_loss: 0.0024 (0.0274)  5_CE_loss: 0.3737 (0.8299)  5_DKS_loss: 0.0076 (0.1388)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0873 (1.0820)  data: 0.0916 (0.1001)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 2:03:59  iter: 9100  loss: 2.1747 (4.2870)  obj_loss: 0.7314 (1.0565)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6771 (1.2509)  1_CE_loss: 0.0045 (0.0751)  2_CE_loss: 0.0406 (0.1438)  2_DKS_loss: 0.0004 (0.0040)  3_CE_loss: 0.0262 (0.1660)  3_DKS_loss: 0.0022 (0.0124)  4_CE_loss: 0.3085 (0.6797)  4_DKS_loss: 0.0045 (0.0240)  5_CE_loss: 0.3731 (0.7428)  5_DKS_loss: 0.0151 (0.1283)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0813 (1.0782)  data: 0.0872 (0.0981)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:02:36  iter: 9200  loss: 1.7319 (3.5838)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3056 (0.3041)  rel_ce_loss: 0.6441 (1.2782)  1_CE_loss: 0.0022 (0.0255)  2_CE_loss: 0.0291 (0.1377)  2_DKS_loss: 0.0001 (0.0039)  3_CE_loss: 0.0250 (0.1881)  3_DKS_loss: 0.0007 (0.0137)  4_CE_loss: 0.2804 (0.6428)  4_DKS_loss: 0.0021 (0.0271)  5_CE_loss: 0.3895 (0.8253)  5_DKS_loss: 0.0116 (0.1375)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0746 (1.0819)  data: 0.0902 (0.1000)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 2:02:11  iter: 9200  loss: 2.1267 (4.2650)  obj_loss: 0.6992 (1.0534)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6573 (1.2448)  1_CE_loss: 0.0136 (0.0744)  2_CE_loss: 0.0357 (0.1427)  2_DKS_loss: 0.0003 (0.0040)  3_CE_loss: 0.0319 (0.1646)  3_DKS_loss: 0.0023 (0.0123)  4_CE_loss: 0.2924 (0.6756)  4_DKS_loss: 0.0029 (0.0238)  5_CE_loss: 0.3788 (0.7389)  5_DKS_loss: 0.0145 (0.1271)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0639 (1.0781)  data: 0.0877 (0.0980)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 2:00:48  iter: 9300  loss: 1.7846 (3.5646)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2979 (0.3041)  rel_ce_loss: 0.6482 (1.2718)  1_CE_loss: 0.0092 (0.0254)  2_CE_loss: 0.0290 (0.1366)  2_DKS_loss: 0.0001 (0.0039)  3_CE_loss: 0.0329 (0.1865)  3_DKS_loss: 0.0007 (0.0136)  4_CE_loss: 0.2721 (0.6390)  4_DKS_loss: 0.0034 (0.0269)  5_CE_loss: 0.3815 (0.8208)  5_DKS_loss: 0.0140 (0.1361)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.0818)  data: 0.0911 (0.0999)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 2:00:23  iter: 9300  loss: 2.2303 (4.2435)  obj_loss: 0.8423 (1.0506)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6873 (1.2387)  1_CE_loss: 0.0058 (0.0737)  2_CE_loss: 0.0293 (0.1415)  2_DKS_loss: 0.0011 (0.0039)  3_CE_loss: 0.0313 (0.1633)  3_DKS_loss: 0.0015 (0.0122)  4_CE_loss: 0.3062 (0.6716)  4_DKS_loss: 0.0032 (0.0236)  5_CE_loss: 0.4076 (0.7352)  5_DKS_loss: 0.0126 (0.1259)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0669 (1.0781)  data: 0.0878 (0.0980)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:58:59  iter: 9400  loss: 1.7442 (3.5459)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3041 (0.3041)  rel_ce_loss: 0.6884 (1.2654)  1_CE_loss: 0.0054 (0.0252)  2_CE_loss: 0.0387 (0.1356)  2_DKS_loss: 0.0001 (0.0038)  3_CE_loss: 0.0265 (0.1849)  3_DKS_loss: 0.0011 (0.0134)  4_CE_loss: 0.2685 (0.6353)  4_DKS_loss: 0.0034 (0.0266)  5_CE_loss: 0.4077 (0.8165)  5_DKS_loss: 0.0119 (0.1349)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0753 (1.0817)  data: 0.0916 (0.0999)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:58:35  iter: 9400  loss: 2.3660 (4.2225)  obj_loss: 0.8115 (1.0476)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.7105 (1.2328)  1_CE_loss: 0.0027 (0.0730)  2_CE_loss: 0.0364 (0.1404)  2_DKS_loss: 0.0005 (0.0039)  3_CE_loss: 0.0388 (0.1619)  3_DKS_loss: 0.0016 (0.0121)  4_CE_loss: 0.2814 (0.6677)  4_DKS_loss: 0.0029 (0.0234)  5_CE_loss: 0.3997 (0.7315)  5_DKS_loss: 0.0126 (0.1247)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0696 (1.0780)  data: 0.0846 (0.0979)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:57:10  iter: 9500  loss: 2.0309 (3.5296)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2921 (0.3042)  rel_ce_loss: 0.7402 (1.2600)  1_CE_loss: 0.0106 (0.0251)  2_CE_loss: 0.0371 (0.1347)  2_DKS_loss: 0.0002 (0.0038)  3_CE_loss: 0.0319 (0.1834)  3_DKS_loss: 0.0011 (0.0133)  4_CE_loss: 0.3356 (0.6318)  4_DKS_loss: 0.0023 (0.0264)  5_CE_loss: 0.4485 (0.8131)  5_DKS_loss: 0.0285 (0.1337)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0691 (1.0816)  data: 0.0914 (0.0998)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:56:46  iter: 9500  loss: 2.1663 (4.2017)  obj_loss: 0.7559 (1.0447)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6165 (1.2269)  1_CE_loss: 0.0030 (0.0724)  2_CE_loss: 0.0142 (0.1394)  2_DKS_loss: 0.0004 (0.0039)  3_CE_loss: 0.0297 (0.1606)  3_DKS_loss: 0.0010 (0.0119)  4_CE_loss: 0.3071 (0.6638)  4_DKS_loss: 0.0023 (0.0231)  5_CE_loss: 0.3802 (0.7279)  5_DKS_loss: 0.0119 (0.1235)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0751 (1.0780)  data: 0.0870 (0.0978)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:55:21  iter: 9600  loss: 1.7774 (3.5124)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2896 (0.3043)  rel_ce_loss: 0.6460 (1.2543)  1_CE_loss: 0.0067 (0.0250)  2_CE_loss: 0.0437 (0.1338)  2_DKS_loss: 0.0001 (0.0038)  3_CE_loss: 0.0443 (0.1820)  3_DKS_loss: 0.0021 (0.0132)  4_CE_loss: 0.2877 (0.6283)  4_DKS_loss: 0.0039 (0.0261)  5_CE_loss: 0.4061 (0.8092)  5_DKS_loss: 0.0215 (0.1326)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0790 (1.0815)  data: 0.0912 (0.0997)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:54:58  iter: 9600  loss: 2.1291 (4.1816)  obj_loss: 0.7754 (1.0420)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6261 (1.2212)  1_CE_loss: 0.0091 (0.0718)  2_CE_loss: 0.0315 (0.1384)  2_DKS_loss: 0.0007 (0.0038)  3_CE_loss: 0.0251 (0.1594)  3_DKS_loss: 0.0015 (0.0118)  4_CE_loss: 0.2478 (0.6600)  4_DKS_loss: 0.0029 (0.0229)  5_CE_loss: 0.3590 (0.7244)  5_DKS_loss: 0.0133 (0.1224)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0885 (1.0779)  data: 0.0899 (0.0978)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:53:32  iter: 9700  loss: 1.7336 (3.4945)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3070 (0.3043)  rel_ce_loss: 0.6582 (1.2482)  1_CE_loss: 0.0025 (0.0248)  2_CE_loss: 0.0357 (0.1328)  2_DKS_loss: 0.0001 (0.0037)  3_CE_loss: 0.0342 (0.1804)  3_DKS_loss: 0.0009 (0.0131)  4_CE_loss: 0.2735 (0.6248)  4_DKS_loss: 0.0019 (0.0259)  5_CE_loss: 0.4108 (0.8052)  5_DKS_loss: 0.0092 (0.1313)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0529 (1.0814)  data: 0.0888 (0.0996)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:53:10  iter: 9700  loss: 2.2006 (4.1622)  obj_loss: 0.7749 (1.0391)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6957 (1.2158)  1_CE_loss: 0.0089 (0.0712)  2_CE_loss: 0.0365 (0.1374)  2_DKS_loss: 0.0004 (0.0038)  3_CE_loss: 0.0326 (0.1582)  3_DKS_loss: 0.0015 (0.0117)  4_CE_loss: 0.2827 (0.6564)  4_DKS_loss: 0.0038 (0.0227)  5_CE_loss: 0.3666 (0.7211)  5_DKS_loss: 0.0120 (0.1213)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0678 (1.0778)  data: 0.0867 (0.0977)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:51:43  iter: 9800  loss: 1.7620 (3.4768)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3234 (0.3043)  rel_ce_loss: 0.6327 (1.2421)  1_CE_loss: 0.0032 (0.0247)  2_CE_loss: 0.0442 (0.1318)  2_DKS_loss: 0.0002 (0.0037)  3_CE_loss: 0.0394 (0.1790)  3_DKS_loss: 0.0016 (0.0130)  4_CE_loss: 0.2872 (0.6214)  4_DKS_loss: 0.0031 (0.0257)  5_CE_loss: 0.3802 (0.8012)  5_DKS_loss: 0.0139 (0.1301)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0801 (1.0813)  data: 0.0899 (0.0996)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:51:21  iter: 9800  loss: 2.2758 (4.1426)  obj_loss: 0.7646 (1.0365)  loss_cal: 0.0035 (0.0035)  rel_ce_loss: 0.6826 (1.2103)  1_CE_loss: 0.0026 (0.0706)  2_CE_loss: 0.0437 (0.1364)  2_DKS_loss: 0.0003 (0.0038)  3_CE_loss: 0.0363 (0.1569)  3_DKS_loss: 0.0013 (0.0116)  4_CE_loss: 0.2649 (0.6527)  4_DKS_loss: 0.0031 (0.0226)  5_CE_loss: 0.3735 (0.7177)  5_DKS_loss: 0.0125 (0.1202)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0690 (1.0777)  data: 0.0852 (0.0976)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:49:57  iter: 9900  loss: 1.6753 (3.4593)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2985 (0.3043)  rel_ce_loss: 0.6379 (1.2361)  1_CE_loss: 0.0045 (0.0245)  2_CE_loss: 0.0328 (0.1308)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0257 (0.1775)  3_DKS_loss: 0.0014 (0.0128)  4_CE_loss: 0.2680 (0.6179)  4_DKS_loss: 0.0030 (0.0254)  5_CE_loss: 0.3792 (0.7971)  5_DKS_loss: 0.0110 (0.1289)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0757 (1.0815)  data: 0.0911 (0.0997)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:49:35  iter: 9900  loss: 2.2453 (4.1237)  obj_loss: 0.7627 (1.0338)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6696 (1.2050)  1_CE_loss: 0.0072 (0.0699)  2_CE_loss: 0.0388 (0.1354)  2_DKS_loss: 0.0004 (0.0037)  3_CE_loss: 0.0384 (0.1557)  3_DKS_loss: 0.0021 (0.0115)  4_CE_loss: 0.2752 (0.6492)  4_DKS_loss: 0.0033 (0.0224)  5_CE_loss: 0.3539 (0.7144)  5_DKS_loss: 0.0136 (0.1191)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0835 (1.0779)  data: 0.0863 (0.0978)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:48:08  iter: 10000  loss: 1.8602 (3.4429)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.2993 (0.3044)  rel_ce_loss: 0.6821 (1.2304)  1_CE_loss: 0.0104 (0.0244)  2_CE_loss: 0.0341 (0.1300)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0352 (0.1762)  3_DKS_loss: 0.0018 (0.0127)  4_CE_loss: 0.2919 (0.6147)  4_DKS_loss: 0.0030 (0.0252)  5_CE_loss: 0.4210 (0.7934)  5_DKS_loss: 0.0141 (0.1278)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0652 (1.0815)  data: 0.0861 (0.0997)  lr: 0.016000  max mem: 10898
INFO:maskrcnn_benchmark:Start validating
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
INFO:maskrcnn_benchmark:eta: 1:47:48  iter: 10000  loss: 2.2653 (4.1057)  obj_loss: 0.7896 (1.0313)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6893 (1.1999)  1_CE_loss: 0.0096 (0.0694)  2_CE_loss: 0.0351 (0.1345)  2_DKS_loss: 0.0008 (0.0037)  3_CE_loss: 0.0290 (0.1546)  3_DKS_loss: 0.0019 (0.0115)  4_CE_loss: 0.2875 (0.6458)  4_DKS_loss: 0.0048 (0.0222)  5_CE_loss: 0.4005 (0.7113)  5_DKS_loss: 0.0138 (0.1181)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1418 (1.0781)  data: 0.1540 (0.0980)  lr: 0.016000  max mem: 11395
INFO:maskrcnn_benchmark:Start validating
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
INFO:maskrcnn_benchmark:Total run time: 0:04:40.734924 (0.05614698481559753 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:04:01.043663 (0.048208732604980466 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Total run time: 0:04:32.334809 (0.054466961765289305 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:03:57.341338 (0.04746826753616333 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:     R @ 20: 0.3445;     R @ 50: 0.4418;     R @ 100: 0.4847;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.3745;  ng-R @ 50: 0.5215;  ng-R @ 100: 0.6360;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2778;    zR @ 50: 0.3173;    zR @ 100: 0.3857;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.3034; ng-zR @ 50: 0.4071; ng-zR @ 100: 0.5661;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0491;    mR @ 50: 0.0824;    mR @ 100: 0.1045;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0250) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.1035) (attached to:0.0023) (behind:0.0941) (belonging to:0.0000) (between:0.0385) (carrying:0.2851) (covered in:0.0000) (covering:0.0000) (eating:0.1429) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0368) (has:0.4011) (holding:0.2116) (in:0.1591) (in front of:0.0211) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1119) (of:0.3082) (on:0.6849) (on back of:0.0455) (over:0.0732) (painted on:0.0000) (parked on:0.0026) (part of:0.0000) (playing:0.0000) (riding:0.4911) (says:0.0000) (sitting on:0.0528) (standing on:0.1312) (to:0.0556) (under:0.0748) (using:0.0000) (walking in:0.0000) (walking on:0.6046) (watching:0.0196) (wearing:0.8661) (wears:0.0000) (with:0.1808) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0475;    mR @ 50: 0.0822;    mR @ 100: 0.1061;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0137) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.1163) (attached to:0.0038) (behind:0.0957) (belonging to:0.0000) (between:0.0526) (carrying:0.2857) (covered in:0.0000) (covering:0.0000) (eating:0.0435) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0366) (has:0.4681) (holding:0.2187) (in:0.1572) (in front of:0.0171) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1047) (of:0.2792) (on:0.7262) (on back of:0.0385) (over:0.0583) (painted on:0.0000) (parked on:0.0044) (part of:0.0000) (playing:0.0000) (riding:0.5152) (says:0.0000) (sitting on:0.0661) (standing on:0.1595) (to:0.0513) (under:0.0800) (using:0.0000) (walking in:0.0000) (walking on:0.6434) (watching:0.0357) (wearing:0.8687) (wears:0.0000) (with:0.1673) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0569; ng-mR @ 50: 0.1157; ng-mR @ 100: 0.1848;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1510) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2904) (attached to:0.0495) (behind:0.1896) (belonging to:0.0000) (between:0.0577) (carrying:0.4825) (covered in:0.1429) (covering:0.0914) (eating:0.1905) (flying in:0.0000) (for:0.0370) (from:0.1000) (growing on:0.0000) (hanging from:0.0662) (has:0.5478) (holding:0.3585) (in:0.2628) (in front of:0.1515) (laying on:0.0000) (looking at:0.1304) (lying on:0.3333) (made of:0.0000) (mounted on:0.0000) (near:0.2636) (of:0.4334) (on:0.8116) (on back of:0.0000) (over:0.1707) (painted on:0.1429) (parked on:0.0265) (part of:0.0451) (playing:0.0000) (riding:0.6741) (says:0.0000) (sitting on:0.1916) (standing on:0.3518) (to:0.0556) (under:0.1463) (using:0.1538) (walking in:0.0000) (walking on:0.6944) (watching:0.1667) (wearing:0.8971) (wears:0.0708) (with:0.3087) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0861; zs-mR @ 50: 0.0958; zs-mR @ 100: 0.1431;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.1111) (behind:0.4444) (belonging to:0.0000) (between:0.0000) (carrying:1.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.5000) (has:0.5667) (holding:0.2500) (in:0.1667) (in front of:0.0909) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4286) (of:0.4000) (on:0.8333) (on back of:0.2500) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.4000) (standing on:1.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.7143) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.3719;     A @ 50: 0.3727;     A @ 100: 0.3727;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Validation Result: 0.4847
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.5920
====================================================================================================
SGG eval:     R @ 20: 0.2581;     R @ 50: 0.3079;     R @ 100: 0.3250;  for mode=sgcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.2854;  ng-R @ 50: 0.3750;  ng-R @ 100: 0.4330;  for mode=sgcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.1442;    zR @ 50: 0.1667;    zR @ 100: 0.1731;  for mode=sgcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.1378; ng-zR @ 50: 0.1667; ng-zR @ 100: 0.2276;  for mode=sgcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0348;    mR @ 50: 0.0501;    mR @ 100: 0.0605;  for mode=sgcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0201) (across:0.0000) (against:0.0526) (along:0.0000) (and:0.0000) (at:0.0303) (attached to:0.0243) (behind:0.0381) (belonging to:0.0000) (between:0.0385) (carrying:0.0395) (covered in:0.0000) (covering:0.0000) (eating:0.1190) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0074) (has:0.3125) (holding:0.1300) (in:0.1033) (in front of:0.2749) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1184) (of:0.1912) (on:0.4767) (on back of:0.0455) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0139) (playing:0.0000) (riding:0.0357) (says:0.0000) (sitting on:0.1078) (standing on:0.0362) (to:0.0000) (under:0.0850) (using:0.1538) (walking in:0.0000) (walking on:0.0053) (watching:0.0588) (wearing:0.4829) (wears:0.0000) (with:0.0256) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0364;    mR @ 50: 0.0540;    mR @ 100: 0.0655;  for mode=sgcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0100) (across:0.0000) (against:0.0400) (along:0.0000) (and:0.0000) (at:0.0291) (attached to:0.0226) (behind:0.0307) (belonging to:0.0000) (between:0.0526) (carrying:0.0268) (covered in:0.0000) (covering:0.0000) (eating:0.1739) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0122) (has:0.3710) (holding:0.1440) (in:0.1067) (in front of:0.2762) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1322) (of:0.1872) (on:0.5263) (on back of:0.0385) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0426) (playing:0.0000) (riding:0.0404) (says:0.0000) (sitting on:0.1121) (standing on:0.0368) (to:0.0000) (under:0.1000) (using:0.1905) (walking in:0.0000) (walking on:0.0027) (watching:0.0357) (wearing:0.5094) (wears:0.0000) (with:0.0257) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0435; ng-mR @ 50: 0.0803; ng-mR @ 100: 0.1223;  for mode=sgcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1371) (across:0.0000) (against:0.0526) (along:0.0000) (and:0.0323) (at:0.1263) (attached to:0.0977) (behind:0.1204) (belonging to:0.0000) (between:0.0000) (carrying:0.2215) (covered in:0.1429) (covering:0.0571) (eating:0.2619) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0956) (has:0.3881) (holding:0.2504) (in:0.2275) (in front of:0.3191) (laying on:0.0476) (looking at:0.0870) (lying on:0.1111) (made of:0.0000) (mounted on:0.0652) (near:0.2694) (of:0.3461) (on:0.5542) (on back of:0.0909) (over:0.0305) (painted on:0.0000) (parked on:0.0460) (part of:0.0139) (playing:0.0000) (riding:0.2277) (says:0.0000) (sitting on:0.2416) (standing on:0.1322) (to:0.0556) (under:0.1437) (using:0.2308) (walking in:0.0000) (walking on:0.0765) (watching:0.1176) (wearing:0.4935) (wears:0.0857) (with:0.1181) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0357; zs-mR @ 50: 0.0450; zs-mR @ 100: 0.0500;  for mode=sgcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.1111) (behind:0.2222) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.2000) (holding:0.0000) (in:0.1111) (in front of:0.3636) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.2857) (of:0.0000) (on:0.2917) (on back of:0.2500) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.2000) (standing on:0.0000) (to:0.0000) (under:0.2500) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2143) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.2288;     A @ 50: 0.2291;     A @ 100: 0.2291;  for mode=sgcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Validation Result: 0.3250
INFO:maskrcnn_benchmark:eta: 1:50:51  iter: 10100  loss: 1.6244 (3.4256)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3031 (0.3043)  rel_ce_loss: 0.6111 (1.2245)  1_CE_loss: 0.0111 (0.0243)  2_CE_loss: 0.0327 (0.1291)  2_DKS_loss: 0.0001 (0.0036)  3_CE_loss: 0.0237 (0.1748)  3_DKS_loss: 0.0005 (0.0126)  4_CE_loss: 0.2745 (0.6114)  4_DKS_loss: 0.0010 (0.0250)  5_CE_loss: 0.3857 (0.7894)  5_DKS_loss: 0.0043 (0.1266)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0684 (1.1273)  data: 0.0903 (0.1456)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:50:21  iter: 10100  loss: 2.1770 (4.0862)  obj_loss: 0.7222 (1.0285)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6489 (1.1944)  1_CE_loss: 0.0018 (0.0688)  2_CE_loss: 0.0308 (0.1335)  2_DKS_loss: 0.0003 (0.0037)  3_CE_loss: 0.0308 (0.1534)  3_DKS_loss: 0.0006 (0.0113)  4_CE_loss: 0.2861 (0.6422)  4_DKS_loss: 0.0013 (0.0220)  5_CE_loss: 0.3617 (0.7079)  5_DKS_loss: 0.0044 (0.1170)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0683 (1.1222)  data: 0.0891 (0.1421)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:48:55  iter: 10200  loss: 1.6884 (3.4086)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3033 (0.3043)  rel_ce_loss: 0.6233 (1.2187)  1_CE_loss: 0.0071 (0.0242)  2_CE_loss: 0.0317 (0.1282)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0310 (0.1735)  3_DKS_loss: 0.0004 (0.0125)  4_CE_loss: 0.2765 (0.6082)  4_DKS_loss: 0.0010 (0.0248)  5_CE_loss: 0.3601 (0.7854)  5_DKS_loss: 0.0040 (0.1254)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0535 (1.1267)  data: 0.0914 (0.1451)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:48:25  iter: 10200  loss: 1.9746 (4.0667)  obj_loss: 0.7285 (1.0255)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6089 (1.1889)  1_CE_loss: 0.0062 (0.0682)  2_CE_loss: 0.0252 (0.1326)  2_DKS_loss: 0.0002 (0.0037)  3_CE_loss: 0.0205 (0.1522)  3_DKS_loss: 0.0005 (0.0112)  4_CE_loss: 0.2491 (0.6387)  4_DKS_loss: 0.0011 (0.0218)  5_CE_loss: 0.3196 (0.7044)  5_DKS_loss: 0.0033 (0.1159)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0561 (1.1217)  data: 0.0897 (0.1416)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:46:59  iter: 10300  loss: 1.6822 (3.3916)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3054 (0.3044)  rel_ce_loss: 0.6261 (1.2128)  1_CE_loss: 0.0072 (0.0241)  2_CE_loss: 0.0310 (0.1273)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0291 (0.1721)  3_DKS_loss: 0.0004 (0.0124)  4_CE_loss: 0.2708 (0.6048)  4_DKS_loss: 0.0010 (0.0245)  5_CE_loss: 0.3947 (0.7814)  5_DKS_loss: 0.0033 (0.1242)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0746 (1.1262)  data: 0.0924 (0.1446)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:46:30  iter: 10300  loss: 2.0324 (4.0476)  obj_loss: 0.6797 (1.0226)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6145 (1.1835)  1_CE_loss: 0.0097 (0.0677)  2_CE_loss: 0.0348 (0.1317)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0289 (0.1511)  3_DKS_loss: 0.0005 (0.0111)  4_CE_loss: 0.2793 (0.6352)  4_DKS_loss: 0.0011 (0.0216)  5_CE_loss: 0.3642 (0.7011)  5_DKS_loss: 0.0037 (0.1148)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0720 (1.1212)  data: 0.0911 (0.1412)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:45:03  iter: 10400  loss: 1.6632 (3.3747)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3080 (0.3044)  rel_ce_loss: 0.6155 (1.2069)  1_CE_loss: 0.0073 (0.0239)  2_CE_loss: 0.0280 (0.1264)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0246 (0.1708)  3_DKS_loss: 0.0004 (0.0123)  4_CE_loss: 0.2712 (0.6016)  4_DKS_loss: 0.0011 (0.0243)  5_CE_loss: 0.3870 (0.7775)  5_DKS_loss: 0.0031 (0.1230)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0763 (1.1257)  data: 0.0903 (0.1441)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:44:35  iter: 10400  loss: 2.0687 (4.0288)  obj_loss: 0.7217 (1.0198)  loss_cal: 0.0036 (0.0035)  rel_ce_loss: 0.6356 (1.1782)  1_CE_loss: 0.0116 (0.0672)  2_CE_loss: 0.0307 (0.1308)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0365 (0.1500)  3_DKS_loss: 0.0006 (0.0110)  4_CE_loss: 0.2727 (0.6317)  4_DKS_loss: 0.0015 (0.0214)  5_CE_loss: 0.3478 (0.6978)  5_DKS_loss: 0.0034 (0.1137)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0706 (1.1207)  data: 0.0914 (0.1407)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:43:08  iter: 10500  loss: 1.6816 (3.3582)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3025 (0.3044)  rel_ce_loss: 0.6360 (1.2012)  1_CE_loss: 0.0029 (0.0238)  2_CE_loss: 0.0284 (0.1256)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0371 (0.1695)  3_DKS_loss: 0.0003 (0.0121)  4_CE_loss: 0.2732 (0.5984)  4_DKS_loss: 0.0009 (0.0241)  5_CE_loss: 0.3825 (0.7737)  5_DKS_loss: 0.0031 (0.1219)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0511 (1.1251)  data: 0.0886 (0.1436)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:42:41  iter: 10500  loss: 2.0648 (4.0099)  obj_loss: 0.7036 (1.0169)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6433 (1.1729)  1_CE_loss: 0.0121 (0.0666)  2_CE_loss: 0.0318 (0.1299)  2_DKS_loss: 0.0002 (0.0036)  3_CE_loss: 0.0278 (0.1489)  3_DKS_loss: 0.0004 (0.0109)  4_CE_loss: 0.2833 (0.6284)  4_DKS_loss: 0.0010 (0.0212)  5_CE_loss: 0.3550 (0.6945)  5_DKS_loss: 0.0033 (0.1127)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0600 (1.1202)  data: 0.0895 (0.1402)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:41:12  iter: 10600  loss: 1.5211 (3.3421)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3088 (0.3044)  rel_ce_loss: 0.5740 (1.1956)  1_CE_loss: 0.0017 (0.0237)  2_CE_loss: 0.0240 (0.1247)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0316 (0.1683)  3_DKS_loss: 0.0003 (0.0120)  4_CE_loss: 0.2399 (0.5953)  4_DKS_loss: 0.0010 (0.0239)  5_CE_loss: 0.3530 (0.7700)  5_DKS_loss: 0.0027 (0.1208)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0588 (1.1246)  data: 0.0916 (0.1431)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:40:46  iter: 10600  loss: 2.0366 (3.9918)  obj_loss: 0.6812 (1.0139)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6361 (1.1679)  1_CE_loss: 0.0070 (0.0661)  2_CE_loss: 0.0338 (0.1290)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0328 (0.1479)  3_DKS_loss: 0.0004 (0.0108)  4_CE_loss: 0.2737 (0.6251)  4_DKS_loss: 0.0011 (0.0210)  5_CE_loss: 0.3442 (0.6914)  5_DKS_loss: 0.0028 (0.1116)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0726 (1.1197)  data: 0.0929 (0.1398)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:39:17  iter: 10700  loss: 1.7102 (3.3261)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3090 (0.3045)  rel_ce_loss: 0.6313 (1.1901)  1_CE_loss: 0.0072 (0.0236)  2_CE_loss: 0.0371 (0.1239)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0450 (0.1670)  3_DKS_loss: 0.0004 (0.0119)  4_CE_loss: 0.3157 (0.5923)  4_DKS_loss: 0.0010 (0.0236)  5_CE_loss: 0.3996 (0.7662)  5_DKS_loss: 0.0032 (0.1197)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0709 (1.1241)  data: 0.0905 (0.1426)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:38:52  iter: 10700  loss: 2.1526 (3.9742)  obj_loss: 0.7642 (1.0114)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6439 (1.1629)  1_CE_loss: 0.0025 (0.0656)  2_CE_loss: 0.0290 (0.1282)  2_DKS_loss: 0.0001 (0.0035)  3_CE_loss: 0.0305 (0.1469)  3_DKS_loss: 0.0004 (0.0107)  4_CE_loss: 0.3016 (0.6219)  4_DKS_loss: 0.0010 (0.0208)  5_CE_loss: 0.3617 (0.6883)  5_DKS_loss: 0.0032 (0.1106)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0686 (1.1193)  data: 0.0918 (0.1393)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:37:22  iter: 10800  loss: 1.5467 (3.3106)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3166 (0.3045)  rel_ce_loss: 0.5489 (1.1846)  1_CE_loss: 0.0060 (0.0235)  2_CE_loss: 0.0334 (0.1231)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0290 (0.1658)  3_DKS_loss: 0.0003 (0.0118)  4_CE_loss: 0.2591 (0.5893)  4_DKS_loss: 0.0010 (0.0234)  5_CE_loss: 0.3438 (0.7626)  5_DKS_loss: 0.0033 (0.1186)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0789 (1.1236)  data: 0.0899 (0.1422)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:36:57  iter: 10800  loss: 2.0028 (3.9565)  obj_loss: 0.6655 (1.0086)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.6130 (1.1579)  1_CE_loss: 0.0113 (0.0651)  2_CE_loss: 0.0270 (0.1273)  2_DKS_loss: 0.0002 (0.0035)  3_CE_loss: 0.0287 (0.1458)  3_DKS_loss: 0.0004 (0.0106)  4_CE_loss: 0.2399 (0.6187)  4_DKS_loss: 0.0011 (0.0206)  5_CE_loss: 0.3410 (0.6851)  5_DKS_loss: 0.0029 (0.1096)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0736 (1.1188)  data: 0.0899 (0.1389)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:35:27  iter: 10900  loss: 1.5174 (3.2950)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3154 (0.3046)  rel_ce_loss: 0.5448 (1.1791)  1_CE_loss: 0.0052 (0.0233)  2_CE_loss: 0.0206 (0.1224)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0257 (0.1646)  3_DKS_loss: 0.0004 (0.0117)  4_CE_loss: 0.2358 (0.5863)  4_DKS_loss: 0.0010 (0.0232)  5_CE_loss: 0.3416 (0.7589)  5_DKS_loss: 0.0030 (0.1175)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0517 (1.1231)  data: 0.0921 (0.1417)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:35:04  iter: 10900  loss: 1.8943 (3.9380)  obj_loss: 0.6074 (1.0052)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5997 (1.1527)  1_CE_loss: 0.0115 (0.0646)  2_CE_loss: 0.0259 (0.1265)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0249 (0.1448)  3_DKS_loss: 0.0004 (0.0105)  4_CE_loss: 0.2721 (0.6155)  4_DKS_loss: 0.0010 (0.0205)  5_CE_loss: 0.3475 (0.6820)  5_DKS_loss: 0.0027 (0.1086)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0671 (1.1184)  data: 0.0900 (0.1386)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:33:34  iter: 11000  loss: 1.5939 (3.2797)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3124 (0.3046)  rel_ce_loss: 0.5959 (1.1737)  1_CE_loss: 0.0064 (0.0232)  2_CE_loss: 0.0334 (0.1216)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0287 (0.1635)  3_DKS_loss: 0.0005 (0.0116)  4_CE_loss: 0.2564 (0.5833)  4_DKS_loss: 0.0011 (0.0230)  5_CE_loss: 0.3638 (0.7554)  5_DKS_loss: 0.0032 (0.1165)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.1228)  data: 0.0893 (0.1414)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:33:10  iter: 11000  loss: 1.9834 (3.9196)  obj_loss: 0.6616 (1.0019)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6052 (1.1477)  1_CE_loss: 0.0062 (0.0641)  2_CE_loss: 0.0277 (0.1257)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0294 (0.1438)  3_DKS_loss: 0.0003 (0.0105)  4_CE_loss: 0.2536 (0.6122)  4_DKS_loss: 0.0009 (0.0203)  5_CE_loss: 0.3301 (0.6788)  5_DKS_loss: 0.0025 (0.1077)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0610 (1.1180)  data: 0.0895 (0.1382)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:31:39  iter: 11100  loss: 1.5127 (3.2641)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3040 (0.3047)  rel_ce_loss: 0.5458 (1.1682)  1_CE_loss: 0.0031 (0.0231)  2_CE_loss: 0.0318 (0.1208)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0315 (0.1622)  3_DKS_loss: 0.0003 (0.0115)  4_CE_loss: 0.2382 (0.5802)  4_DKS_loss: 0.0009 (0.0228)  5_CE_loss: 0.3414 (0.7518)  5_DKS_loss: 0.0026 (0.1155)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0886 (1.1224)  data: 0.0906 (0.1410)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:31:16  iter: 11100  loss: 1.8800 (3.9011)  obj_loss: 0.5581 (0.9984)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5786 (1.1424)  1_CE_loss: 0.0029 (0.0636)  2_CE_loss: 0.0366 (0.1249)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0289 (0.1429)  3_DKS_loss: 0.0003 (0.0104)  4_CE_loss: 0.2491 (0.6090)  4_DKS_loss: 0.0010 (0.0201)  5_CE_loss: 0.3243 (0.6757)  5_DKS_loss: 0.0026 (0.1067)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0611 (1.1176)  data: 0.0883 (0.1377)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:29:45  iter: 11200  loss: 1.5911 (3.2493)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3104 (0.3047)  rel_ce_loss: 0.5669 (1.1630)  1_CE_loss: 0.0082 (0.0230)  2_CE_loss: 0.0363 (0.1200)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0336 (0.1611)  3_DKS_loss: 0.0003 (0.0114)  4_CE_loss: 0.2491 (0.5774)  4_DKS_loss: 0.0009 (0.0226)  5_CE_loss: 0.3697 (0.7483)  5_DKS_loss: 0.0032 (0.1145)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0734 (1.1220)  data: 0.0914 (0.1406)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:29:23  iter: 11200  loss: 1.8831 (3.8834)  obj_loss: 0.6099 (0.9951)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.5951 (1.1375)  1_CE_loss: 0.0024 (0.0631)  2_CE_loss: 0.0279 (0.1241)  2_DKS_loss: 0.0001 (0.0034)  3_CE_loss: 0.0305 (0.1419)  3_DKS_loss: 0.0003 (0.0103)  4_CE_loss: 0.2467 (0.6060)  4_DKS_loss: 0.0010 (0.0199)  5_CE_loss: 0.3367 (0.6727)  5_DKS_loss: 0.0025 (0.1058)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0737 (1.1173)  data: 0.0904 (0.1374)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:27:51  iter: 11300  loss: 1.5151 (3.2345)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3055 (0.3048)  rel_ce_loss: 0.5691 (1.1578)  1_CE_loss: 0.0068 (0.0229)  2_CE_loss: 0.0248 (0.1193)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0214 (0.1600)  3_DKS_loss: 0.0003 (0.0113)  4_CE_loss: 0.2389 (0.5745)  4_DKS_loss: 0.0009 (0.0224)  5_CE_loss: 0.3554 (0.7449)  5_DKS_loss: 0.0027 (0.1135)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0765 (1.1216)  data: 0.0917 (0.1402)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:27:29  iter: 11300  loss: 1.8866 (3.8660)  obj_loss: 0.6284 (0.9919)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.5876 (1.1327)  1_CE_loss: 0.0063 (0.0627)  2_CE_loss: 0.0358 (0.1234)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0322 (0.1410)  3_DKS_loss: 0.0003 (0.0102)  4_CE_loss: 0.2387 (0.6029)  4_DKS_loss: 0.0010 (0.0198)  5_CE_loss: 0.3101 (0.6696)  5_DKS_loss: 0.0026 (0.1049)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0892 (1.1169)  data: 0.0890 (0.1370)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:25:57  iter: 11400  loss: 1.5436 (3.2200)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3189 (0.3049)  rel_ce_loss: 0.5617 (1.1526)  1_CE_loss: 0.0053 (0.0228)  2_CE_loss: 0.0212 (0.1185)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0244 (0.1589)  3_DKS_loss: 0.0003 (0.0112)  4_CE_loss: 0.2368 (0.5716)  4_DKS_loss: 0.0009 (0.0222)  5_CE_loss: 0.3405 (0.7416)  5_DKS_loss: 0.0027 (0.1125)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0676 (1.1211)  data: 0.0897 (0.1398)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:25:35  iter: 11400  loss: 1.9415 (3.8491)  obj_loss: 0.6011 (0.9887)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.5841 (1.1281)  1_CE_loss: 0.0026 (0.0622)  2_CE_loss: 0.0316 (0.1226)  2_DKS_loss: 0.0002 (0.0033)  3_CE_loss: 0.0406 (0.1401)  3_DKS_loss: 0.0004 (0.0101)  4_CE_loss: 0.2831 (0.6000)  4_DKS_loss: 0.0010 (0.0196)  5_CE_loss: 0.3507 (0.6669)  5_DKS_loss: 0.0026 (0.1040)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0590 (1.1165)  data: 0.0922 (0.1366)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:24:03  iter: 11500  loss: 1.6227 (3.2058)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3141 (0.3050)  rel_ce_loss: 0.5815 (1.1476)  1_CE_loss: 0.0054 (0.0227)  2_CE_loss: 0.0318 (0.1178)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0318 (0.1578)  3_DKS_loss: 0.0003 (0.0111)  4_CE_loss: 0.2367 (0.5688)  4_DKS_loss: 0.0008 (0.0221)  5_CE_loss: 0.3434 (0.7382)  5_DKS_loss: 0.0028 (0.1116)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0986 (1.1207)  data: 0.0887 (0.1394)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:23:42  iter: 11500  loss: 1.8979 (3.8319)  obj_loss: 0.6377 (0.9858)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.5960 (1.1232)  1_CE_loss: 0.0062 (0.0618)  2_CE_loss: 0.0242 (0.1218)  2_DKS_loss: 0.0001 (0.0033)  3_CE_loss: 0.0279 (0.1391)  3_DKS_loss: 0.0003 (0.0100)  4_CE_loss: 0.2232 (0.5970)  4_DKS_loss: 0.0008 (0.0194)  5_CE_loss: 0.3152 (0.6639)  5_DKS_loss: 0.0022 (0.1031)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0832 (1.1162)  data: 0.0918 (0.1364)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:22:10  iter: 11600  loss: 1.5623 (3.1921)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3068 (0.3050)  rel_ce_loss: 0.5678 (1.1427)  1_CE_loss: 0.0123 (0.0226)  2_CE_loss: 0.0300 (0.1171)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0417 (0.1567)  3_DKS_loss: 0.0004 (0.0110)  4_CE_loss: 0.2439 (0.5661)  4_DKS_loss: 0.0009 (0.0219)  5_CE_loss: 0.3502 (0.7350)  5_DKS_loss: 0.0031 (0.1106)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0731 (1.1206)  data: 0.0906 (0.1392)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:21:49  iter: 11600  loss: 1.8997 (3.8152)  obj_loss: 0.5742 (0.9826)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5710 (1.1185)  1_CE_loss: 0.0081 (0.0613)  2_CE_loss: 0.0322 (0.1211)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0355 (0.1382)  3_DKS_loss: 0.0003 (0.0099)  4_CE_loss: 0.2753 (0.5942)  4_DKS_loss: 0.0009 (0.0193)  5_CE_loss: 0.3431 (0.6611)  5_DKS_loss: 0.0026 (0.1022)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0525 (1.1158)  data: 0.0883 (0.1361)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:20:16  iter: 11700  loss: 1.5560 (3.1784)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3084 (0.3051)  rel_ce_loss: 0.5850 (1.1379)  1_CE_loss: 0.0027 (0.0225)  2_CE_loss: 0.0232 (0.1164)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0283 (0.1557)  3_DKS_loss: 0.0003 (0.0109)  4_CE_loss: 0.2516 (0.5635)  4_DKS_loss: 0.0008 (0.0217)  5_CE_loss: 0.3679 (0.7319)  5_DKS_loss: 0.0028 (0.1097)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0794 (1.1202)  data: 0.0899 (0.1388)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:19:56  iter: 11700  loss: 2.0109 (3.7987)  obj_loss: 0.6641 (0.9795)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.6015 (1.1139)  1_CE_loss: 0.0067 (0.0609)  2_CE_loss: 0.0291 (0.1203)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0313 (0.1373)  3_DKS_loss: 0.0003 (0.0099)  4_CE_loss: 0.2614 (0.5913)  4_DKS_loss: 0.0009 (0.0191)  5_CE_loss: 0.3321 (0.6583)  5_DKS_loss: 0.0032 (0.1014)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0635 (1.1155)  data: 0.0887 (0.1357)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:18:23  iter: 11800  loss: 1.5908 (3.1651)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3054 (0.3051)  rel_ce_loss: 0.5561 (1.1332)  1_CE_loss: 0.0036 (0.0224)  2_CE_loss: 0.0211 (0.1157)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0269 (0.1546)  3_DKS_loss: 0.0003 (0.0108)  4_CE_loss: 0.2483 (0.5610)  4_DKS_loss: 0.0008 (0.0215)  5_CE_loss: 0.3425 (0.7288)  5_DKS_loss: 0.0028 (0.1088)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0879 (1.1198)  data: 0.0944 (0.1384)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:18:03  iter: 11800  loss: 1.8161 (3.7826)  obj_loss: 0.6235 (0.9765)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.4968 (1.1094)  1_CE_loss: 0.0107 (0.0605)  2_CE_loss: 0.0353 (0.1196)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0302 (0.1365)  3_DKS_loss: 0.0003 (0.0098)  4_CE_loss: 0.2368 (0.5885)  4_DKS_loss: 0.0008 (0.0190)  5_CE_loss: 0.3247 (0.6556)  5_DKS_loss: 0.0023 (0.1006)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0919 (1.1151)  data: 0.0892 (0.1353)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:16:30  iter: 11900  loss: 1.5380 (3.1519)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3142 (0.3052)  rel_ce_loss: 0.5871 (1.1286)  1_CE_loss: 0.0105 (0.0223)  2_CE_loss: 0.0287 (0.1150)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0303 (0.1537)  3_DKS_loss: 0.0003 (0.0108)  4_CE_loss: 0.2326 (0.5583)  4_DKS_loss: 0.0010 (0.0213)  5_CE_loss: 0.3367 (0.7257)  5_DKS_loss: 0.0027 (0.1079)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.1289 (1.1196)  data: 0.0946 (0.1382)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:16:11  iter: 11900  loss: 1.8019 (3.7662)  obj_loss: 0.5693 (0.9734)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.5599 (1.1049)  1_CE_loss: 0.0132 (0.0601)  2_CE_loss: 0.0260 (0.1189)  2_DKS_loss: 0.0001 (0.0032)  3_CE_loss: 0.0314 (0.1356)  3_DKS_loss: 0.0003 (0.0097)  4_CE_loss: 0.2180 (0.5857)  4_DKS_loss: 0.0008 (0.0188)  5_CE_loss: 0.2905 (0.6528)  5_DKS_loss: 0.0023 (0.0997)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0961 (1.1149)  data: 0.0991 (0.1351)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:---Total norm 2.74624 clip coef 1.82067-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.08784, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 1.04263, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.90781, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.71553, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.60134, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.59499, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.49432, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.48138, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.44960, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.38100, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.35103, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.33919, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.33835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.33835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.33835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.33835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.33777, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.32860, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.32176, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.31764, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.30906, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.30628, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.30088, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.29574, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.24859, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.23109, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.21532, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.21179, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.21154, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.20005, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.15616, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14736, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13904, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13789, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13430, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13011, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12470, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.11407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11358, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.11190, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.10881, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.10760, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10699, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.10694, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10668, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.10029, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.10012, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09873, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09744, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09586, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.09469, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09418, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09275, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09240, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09194, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08990, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.08721, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08522, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08450, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08116, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07688, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.07648, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07575, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07571, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07284, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07218, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07096, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07053, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06977, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.06881, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.06881, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06773, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.06575, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06557, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06545, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06537, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06369, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06313, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05939, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05848, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05790, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05699, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05699, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05675, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05497, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05390, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05297, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05274, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05244, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05224, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05223, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05221, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05213, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05198, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05040, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04933, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04930, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04902, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.04850, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04821, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04802, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04780, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04740, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.04616, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04603, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04586, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04582, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04578, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04577, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.04487, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04418, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04339, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04195, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04144, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04064, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04052, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04051, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04048, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03969, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03958, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03936, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03934, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03896, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03830, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03642, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03576, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03566, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03566, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03428, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03390, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03299, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.03197, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.03197, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03187, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03056, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.03042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03034, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03029, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03013, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03001, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02872, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02852, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02827, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02801, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02793, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02779, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02773, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02734, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02717, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02695, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02646, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02632, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02607, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02599, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02598, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02495, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02459, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.02427, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02415, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02410, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02371, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02371, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02330, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02270, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02252, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02228, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02223, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02196, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.02087, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02010, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01987, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01907, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01900, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01895, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.01893, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01856, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01853, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.01848, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.01848, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01828, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01750, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01743, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01737, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01726, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01688, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01687, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01660, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01607, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01594, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01579, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01571, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01560, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01516, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01493, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01481, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01459, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01453, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01420, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01331, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01288, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01231, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01190, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01146, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.01082, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.01082, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01016, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01000, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00982, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00881, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00823, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00781, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00781, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00773, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00748, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00739, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00736, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00728, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00727, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00720, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00716, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00714, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00710, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00710, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00693, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00689, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00681, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00661, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00646, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00644, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00633, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00622, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00608, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00597, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00577, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00565, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00547, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00543, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00540, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00534, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00530, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00530, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00526, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00525, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00521, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00514, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00511, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00508, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00499, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00460, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00449, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00448, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00445, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00445, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00440, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00440, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00438, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00431, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00426, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00418, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00416, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00413, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00410, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00408, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00406, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00398, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00398, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00397, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00395, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00376, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00373, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00371, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00368, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00359, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00359, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00359, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00332, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00329, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00325, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00322, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00311, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00308, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00307, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00307, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00289, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00279, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00264, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00220, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00206, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00205, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00193, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00185, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00181, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00172, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00160, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00155, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00149, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00148, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00142, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00138, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00138, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00137, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00136, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00136, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00136, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00135, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00134, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00133, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00133, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00131, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00128, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00127, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00127, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00125, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00118, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00118, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00118, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00116, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00113, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00112, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00111, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00109, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00106, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00102, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00097, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00087, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00086, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00072, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00067, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00067, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00061, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00061, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00060, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00059, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00051, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00049, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00047, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00045, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00044, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00042, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00035, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00034, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00017, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1:14:37  iter: 12000  loss: 1.5889 (3.1387)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3092 (0.3052)  rel_ce_loss: 0.5654 (1.1239)  1_CE_loss: 0.0101 (0.0222)  2_CE_loss: 0.0339 (0.1144)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0202 (0.1526)  3_DKS_loss: 0.0003 (0.0107)  4_CE_loss: 0.2557 (0.5558)  4_DKS_loss: 0.0008 (0.0212)  5_CE_loss: 0.3680 (0.7227)  5_DKS_loss: 0.0029 (0.1070)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0320 (1.1193)  data: 0.0877 (0.1379)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:---Total norm 3.35732 clip coef 1.48928-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 1.52515, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 1.28062, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.02278, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.86603, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 0.86217, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.81776, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.55575, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.55248, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.49481, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.43206, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.38584, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.35326, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.31742, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.28665, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.28066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27855, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26950, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.26639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.26639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.26639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.26639, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.25791, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24135, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24090, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21665, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.21465, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.20899, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19746, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19431, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.18815, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.18808, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18733, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18547, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18451, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18352, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18317, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.18314, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18123, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.18115, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18085, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.18083, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17736, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17643, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17422, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17111, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.16998, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.16922, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16739, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16251, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15964, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.15437, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.15270, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14618, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14533, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.14417, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14374, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.14372, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.14297, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13773, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13458, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.13371, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13324, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.12805, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12412, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11986, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.11856, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11624, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11555, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11516, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11404, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.11382, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11304, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11083, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10946, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10921, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10906, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10738, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10730, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10698, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10691, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10418, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10318, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.10243, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10232, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10140, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10060, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09969, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09964, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09925, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09892, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09872, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09871, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09819, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09549, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09475, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09265, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09199, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09186, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09144, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08871, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08743, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08680, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08678, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08669, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08642, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08558, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08484, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08433, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08295, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08254, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08152, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08032, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07994, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.07853, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07825, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.07822, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07725, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.07701, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07690, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07659, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07652, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07602, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07471, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07396, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07198, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07178, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06880, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06789, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.06747, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06660, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06650, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06637, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06559, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06530, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06496, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06151, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06129, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.06029, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05941, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05745, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05728, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05548, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05420, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05381, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05302, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05279, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05268, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05240, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.05039, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.05039, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04998, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04933, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.04875, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04762, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04698, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04587, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04510, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.04277, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.04014, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.04011, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03653, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03613, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03613, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.03605, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.03560, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03535, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03466, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03306, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03274, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03260, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03260, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03260, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03260, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.03119, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.03119, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03088, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03082, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03046, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.03006, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02969, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02958, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02807, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02718, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.02607, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02536, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02508, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.02469, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02451, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02444, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02441, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02383, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02327, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02248, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02218, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02166, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02031, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02031, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01967, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01964, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01960, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01918, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01904, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01873, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01859, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01804, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01799, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01789, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01783, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01739, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01737, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01718, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01707, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01700, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01695, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01680, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01663, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01646, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01641, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01632, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01630, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01627, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01624, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01622, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01594, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01576, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01576, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01576, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01568, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01556, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01548, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01541, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01533, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01522, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01522, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01511, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01490, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01485, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01466, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01465, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01461, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01452, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01428, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01413, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01381, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01369, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01361, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01289, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01280, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01246, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01232, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01218, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01218, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01173, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.01169, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01158, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01156, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01154, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01148, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01146, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01132, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01106, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01021, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00941, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00936, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00907, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00888, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00887, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00875, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00865, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00865, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00842, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00839, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00835, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00827, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00826, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00821, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00818, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00817, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00800, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00795, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00791, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00784, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00775, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00745, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00740, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00740, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00711, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00705, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00699, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00694, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00684, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00683, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00680, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00670, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00652, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00644, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00633, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00614, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00611, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00580, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00575, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00575, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00563, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00563, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00559, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00554, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00552, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00550, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00543, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00538, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00532, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00529, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00522, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00511, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00510, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00510, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00501, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00495, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00489, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00488, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00482, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00475, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00472, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00471, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00462, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00448, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00447, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00402, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00402, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00390, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00380, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00375, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00373, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00360, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00355, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00347, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00344, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00339, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00338, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00335, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00333, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00330, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00324, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00319, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00314, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00310, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00302, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00301, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00291, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00290, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00285, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00278, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00267, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00252, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00242, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00234, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00233, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00228, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00219, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00212, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00188, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00184, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00168, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00159, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00121, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00097, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00095, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00089, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00060, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00042, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00040, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00031, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00023, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 1:14:18  iter: 12000  loss: 1.8453 (3.7505)  obj_loss: 0.5698 (0.9704)  loss_cal: 0.0037 (0.0035)  rel_ce_loss: 0.5580 (1.1005)  1_CE_loss: 0.0024 (0.0597)  2_CE_loss: 0.0330 (0.1182)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0344 (0.1348)  3_DKS_loss: 0.0003 (0.0096)  4_CE_loss: 0.2364 (0.5830)  4_DKS_loss: 0.0009 (0.0187)  5_CE_loss: 0.3150 (0.6501)  5_DKS_loss: 0.0024 (0.0989)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0467 (1.1146)  data: 0.0902 (0.1349)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:12:43  iter: 12100  loss: 1.5301 (3.1256)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3154 (0.3053)  rel_ce_loss: 0.5408 (1.1193)  1_CE_loss: 0.0032 (0.0221)  2_CE_loss: 0.0341 (0.1137)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0277 (0.1516)  3_DKS_loss: 0.0003 (0.0106)  4_CE_loss: 0.2258 (0.5532)  4_DKS_loss: 0.0008 (0.0210)  5_CE_loss: 0.3463 (0.7196)  5_DKS_loss: 0.0026 (0.1062)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0635 (1.1189)  data: 0.0884 (0.1376)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:12:25  iter: 12100  loss: 1.8202 (3.7349)  obj_loss: 0.6650 (0.9675)  loss_cal: 0.0038 (0.0035)  rel_ce_loss: 0.5430 (1.0962)  1_CE_loss: 0.0040 (0.0593)  2_CE_loss: 0.0342 (0.1175)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0254 (0.1339)  3_DKS_loss: 0.0003 (0.0095)  4_CE_loss: 0.2336 (0.5802)  4_DKS_loss: 0.0009 (0.0185)  5_CE_loss: 0.2961 (0.6474)  5_DKS_loss: 0.0022 (0.0981)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0790 (1.1142)  data: 0.0916 (0.1345)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:10:50  iter: 12200  loss: 1.6619 (3.1127)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3099 (0.3054)  rel_ce_loss: 0.5889 (1.1147)  1_CE_loss: 0.0116 (0.0221)  2_CE_loss: 0.0356 (0.1131)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0329 (0.1506)  3_DKS_loss: 0.0003 (0.0105)  4_CE_loss: 0.2682 (0.5507)  4_DKS_loss: 0.0008 (0.0208)  5_CE_loss: 0.3688 (0.7166)  5_DKS_loss: 0.0031 (0.1053)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0557 (1.1185)  data: 0.0915 (0.1372)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:10:32  iter: 12200  loss: 1.8309 (3.7200)  obj_loss: 0.6143 (0.9649)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.5859 (1.0920)  1_CE_loss: 0.0037 (0.0589)  2_CE_loss: 0.0329 (0.1169)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0273 (0.1331)  3_DKS_loss: 0.0003 (0.0095)  4_CE_loss: 0.2514 (0.5776)  4_DKS_loss: 0.0009 (0.0184)  5_CE_loss: 0.3174 (0.6448)  5_DKS_loss: 0.0025 (0.0973)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0575 (1.1139)  data: 0.0893 (0.1342)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:08:57  iter: 12300  loss: 1.5716 (3.1002)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3063 (0.3054)  rel_ce_loss: 0.5694 (1.1102)  1_CE_loss: 0.0021 (0.0220)  2_CE_loss: 0.0347 (0.1124)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0345 (0.1497)  3_DKS_loss: 0.0003 (0.0104)  4_CE_loss: 0.2626 (0.5482)  4_DKS_loss: 0.0009 (0.0207)  5_CE_loss: 0.3520 (0.7136)  5_DKS_loss: 0.0027 (0.1045)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0800 (1.1184)  data: 0.0899 (0.1370)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:08:40  iter: 12300  loss: 1.7588 (3.7049)  obj_loss: 0.6143 (0.9621)  loss_cal: 0.0039 (0.0035)  rel_ce_loss: 0.5321 (1.0878)  1_CE_loss: 0.0044 (0.0585)  2_CE_loss: 0.0287 (0.1162)  2_DKS_loss: 0.0001 (0.0031)  3_CE_loss: 0.0204 (0.1323)  3_DKS_loss: 0.0003 (0.0094)  4_CE_loss: 0.2531 (0.5750)  4_DKS_loss: 0.0008 (0.0182)  5_CE_loss: 0.3225 (0.6422)  5_DKS_loss: 0.0023 (0.0966)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0491 (1.1136)  data: 0.0911 (0.1340)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:07:05  iter: 12400  loss: 1.5386 (3.0876)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3128 (0.3055)  rel_ce_loss: 0.5451 (1.1058)  1_CE_loss: 0.0031 (0.0219)  2_CE_loss: 0.0240 (0.1118)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0288 (0.1488)  3_DKS_loss: 0.0004 (0.0103)  4_CE_loss: 0.2269 (0.5457)  4_DKS_loss: 0.0009 (0.0205)  5_CE_loss: 0.3678 (0.7107)  5_DKS_loss: 0.0027 (0.1037)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0563 (1.1181)  data: 0.0906 (0.1367)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:06:48  iter: 12400  loss: 1.8470 (3.6902)  obj_loss: 0.6289 (0.9593)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5503 (1.0837)  1_CE_loss: 0.0038 (0.0581)  2_CE_loss: 0.0261 (0.1156)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0267 (0.1315)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2552 (0.5724)  4_DKS_loss: 0.0009 (0.0181)  5_CE_loss: 0.3062 (0.6398)  5_DKS_loss: 0.0025 (0.0958)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0535 (1.1134)  data: 0.0902 (0.1337)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:05:11  iter: 12500  loss: 1.5833 (3.0755)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3107 (0.3056)  rel_ce_loss: 0.5738 (1.1015)  1_CE_loss: 0.0024 (0.0218)  2_CE_loss: 0.0313 (0.1112)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0309 (0.1478)  3_DKS_loss: 0.0003 (0.0103)  4_CE_loss: 0.2426 (0.5434)  4_DKS_loss: 0.0009 (0.0204)  5_CE_loss: 0.3456 (0.7078)  5_DKS_loss: 0.0029 (0.1029)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0580 (1.1177)  data: 0.0926 (0.1363)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:04:55  iter: 12500  loss: 1.8147 (3.6758)  obj_loss: 0.5830 (0.9567)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5623 (1.0797)  1_CE_loss: 0.0096 (0.0577)  2_CE_loss: 0.0269 (0.1149)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0339 (0.1308)  3_DKS_loss: 0.0003 (0.0092)  4_CE_loss: 0.2380 (0.5699)  4_DKS_loss: 0.0009 (0.0180)  5_CE_loss: 0.3187 (0.6373)  5_DKS_loss: 0.0025 (0.0951)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0639 (1.1130)  data: 0.0893 (0.1334)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:03:18  iter: 12600  loss: 1.5558 (3.0636)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3082 (0.3056)  rel_ce_loss: 0.5854 (1.0972)  1_CE_loss: 0.0100 (0.0217)  2_CE_loss: 0.0242 (0.1106)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0289 (0.1469)  3_DKS_loss: 0.0003 (0.0102)  4_CE_loss: 0.2737 (0.5411)  4_DKS_loss: 0.0008 (0.0202)  5_CE_loss: 0.3483 (0.7051)  5_DKS_loss: 0.0029 (0.1021)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0774 (1.1173)  data: 0.0919 (0.1360)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:03:03  iter: 12600  loss: 1.8214 (3.6615)  obj_loss: 0.6050 (0.9539)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5385 (1.0757)  1_CE_loss: 0.0061 (0.0574)  2_CE_loss: 0.0263 (0.1143)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0233 (0.1300)  3_DKS_loss: 0.0003 (0.0092)  4_CE_loss: 0.2461 (0.5674)  4_DKS_loss: 0.0009 (0.0178)  5_CE_loss: 0.3188 (0.6350)  5_DKS_loss: 0.0024 (0.0943)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0703 (1.1127)  data: 0.0905 (0.1331)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 1:01:26  iter: 12700  loss: 1.5514 (3.0518)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3149 (0.3057)  rel_ce_loss: 0.5549 (1.0930)  1_CE_loss: 0.0072 (0.0216)  2_CE_loss: 0.0341 (0.1100)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0347 (0.1460)  3_DKS_loss: 0.0003 (0.0101)  4_CE_loss: 0.2631 (0.5388)  4_DKS_loss: 0.0009 (0.0201)  5_CE_loss: 0.3562 (0.7023)  5_DKS_loss: 0.0029 (0.1013)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0530 (1.1171)  data: 0.0907 (0.1358)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 1:01:11  iter: 12700  loss: 1.7811 (3.6474)  obj_loss: 0.5488 (0.9511)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5649 (1.0718)  1_CE_loss: 0.0074 (0.0570)  2_CE_loss: 0.0328 (0.1137)  2_DKS_loss: 0.0001 (0.0030)  3_CE_loss: 0.0250 (0.1292)  3_DKS_loss: 0.0003 (0.0091)  4_CE_loss: 0.2558 (0.5650)  4_DKS_loss: 0.0008 (0.0177)  5_CE_loss: 0.3120 (0.6327)  5_DKS_loss: 0.0024 (0.0936)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0748 (1.1125)  data: 0.0906 (0.1329)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:59:33  iter: 12800  loss: 1.4926 (3.0403)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3129 (0.3058)  rel_ce_loss: 0.5260 (1.0889)  1_CE_loss: 0.0146 (0.0216)  2_CE_loss: 0.0255 (0.1094)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0255 (0.1451)  3_DKS_loss: 0.0003 (0.0100)  4_CE_loss: 0.2308 (0.5366)  4_DKS_loss: 0.0008 (0.0199)  5_CE_loss: 0.3404 (0.6996)  5_DKS_loss: 0.0027 (0.1005)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0820 (1.1168)  data: 0.0946 (0.1356)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:59:19  iter: 12800  loss: 1.7513 (3.6333)  obj_loss: 0.5806 (0.9483)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5609 (1.0679)  1_CE_loss: 0.0024 (0.0566)  2_CE_loss: 0.0293 (0.1131)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0256 (0.1285)  3_DKS_loss: 0.0003 (0.0090)  4_CE_loss: 0.2295 (0.5626)  4_DKS_loss: 0.0008 (0.0176)  5_CE_loss: 0.3000 (0.6303)  5_DKS_loss: 0.0023 (0.0929)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0806 (1.1122)  data: 0.0920 (0.1327)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:57:41  iter: 12900  loss: 1.5827 (3.0290)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3066 (0.3058)  rel_ce_loss: 0.5469 (1.0848)  1_CE_loss: 0.0038 (0.0215)  2_CE_loss: 0.0410 (0.1089)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0322 (0.1443)  3_DKS_loss: 0.0003 (0.0100)  4_CE_loss: 0.2680 (0.5343)  4_DKS_loss: 0.0008 (0.0198)  5_CE_loss: 0.3621 (0.6970)  5_DKS_loss: 0.0028 (0.0998)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0568 (1.1166)  data: 0.0900 (0.1353)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:57:26  iter: 12900  loss: 1.8559 (3.6196)  obj_loss: 0.5796 (0.9457)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5606 (1.0640)  1_CE_loss: 0.0081 (0.0563)  2_CE_loss: 0.0300 (0.1125)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0339 (0.1278)  3_DKS_loss: 0.0003 (0.0090)  4_CE_loss: 0.2563 (0.5602)  4_DKS_loss: 0.0008 (0.0174)  5_CE_loss: 0.3362 (0.6280)  5_DKS_loss: 0.0024 (0.0922)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0667 (1.1119)  data: 0.0899 (0.1324)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:55:48  iter: 13000  loss: 1.5401 (3.0178)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3131 (0.3059)  rel_ce_loss: 0.5421 (1.0808)  1_CE_loss: 0.0046 (0.0214)  2_CE_loss: 0.0287 (0.1083)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0225 (0.1434)  3_DKS_loss: 0.0003 (0.0099)  4_CE_loss: 0.2509 (0.5322)  4_DKS_loss: 0.0008 (0.0196)  5_CE_loss: 0.3548 (0.6944)  5_DKS_loss: 0.0027 (0.0990)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0693 (1.1162)  data: 0.0915 (0.1349)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:55:34  iter: 13000  loss: 1.8763 (3.6062)  obj_loss: 0.5747 (0.9432)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5930 (1.0603)  1_CE_loss: 0.0030 (0.0559)  2_CE_loss: 0.0240 (0.1119)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0244 (0.1271)  3_DKS_loss: 0.0003 (0.0089)  4_CE_loss: 0.2531 (0.5579)  4_DKS_loss: 0.0009 (0.0173)  5_CE_loss: 0.3213 (0.6258)  5_DKS_loss: 0.0023 (0.0915)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0642 (1.1115)  data: 0.0889 (0.1321)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:53:56  iter: 13100  loss: 1.5154 (3.0065)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3131 (0.3060)  rel_ce_loss: 0.5438 (1.0768)  1_CE_loss: 0.0023 (0.0213)  2_CE_loss: 0.0255 (0.1077)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0244 (0.1425)  3_DKS_loss: 0.0003 (0.0098)  4_CE_loss: 0.2231 (0.5300)  4_DKS_loss: 0.0009 (0.0195)  5_CE_loss: 0.3336 (0.6918)  5_DKS_loss: 0.0024 (0.0983)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0832 (1.1159)  data: 0.0899 (0.1346)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:53:42  iter: 13100  loss: 1.8452 (3.5927)  obj_loss: 0.5913 (0.9404)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5666 (1.0567)  1_CE_loss: 0.0081 (0.0556)  2_CE_loss: 0.0296 (0.1113)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0261 (0.1264)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2598 (0.5556)  4_DKS_loss: 0.0009 (0.0172)  5_CE_loss: 0.3183 (0.6235)  5_DKS_loss: 0.0024 (0.0908)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0848 (1.1113)  data: 0.0920 (0.1319)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:52:03  iter: 13200  loss: 1.5684 (2.9959)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3053 (0.3060)  rel_ce_loss: 0.5623 (1.0731)  1_CE_loss: 0.0069 (0.0212)  2_CE_loss: 0.0273 (0.1072)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0247 (0.1417)  3_DKS_loss: 0.0003 (0.0097)  4_CE_loss: 0.2468 (0.5280)  4_DKS_loss: 0.0009 (0.0193)  5_CE_loss: 0.3431 (0.6893)  5_DKS_loss: 0.0027 (0.0976)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0731 (1.1156)  data: 0.0909 (0.1343)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:51:50  iter: 13200  loss: 1.8627 (3.5798)  obj_loss: 0.5591 (0.9380)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5625 (1.0531)  1_CE_loss: 0.0056 (0.0553)  2_CE_loss: 0.0298 (0.1108)  2_DKS_loss: 0.0001 (0.0029)  3_CE_loss: 0.0286 (0.1257)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2607 (0.5534)  4_DKS_loss: 0.0009 (0.0171)  5_CE_loss: 0.3453 (0.6213)  5_DKS_loss: 0.0026 (0.0902)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0774 (1.1110)  data: 0.0887 (0.1316)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:50:11  iter: 13300  loss: 1.4920 (2.9852)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3307 (0.3061)  rel_ce_loss: 0.5448 (1.0693)  1_CE_loss: 0.0042 (0.0211)  2_CE_loss: 0.0249 (0.1066)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0197 (0.1409)  3_DKS_loss: 0.0003 (0.0097)  4_CE_loss: 0.2182 (0.5259)  4_DKS_loss: 0.0009 (0.0192)  5_CE_loss: 0.3150 (0.6868)  5_DKS_loss: 0.0026 (0.0969)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0739 (1.1153)  data: 0.0941 (0.1341)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:49:59  iter: 13300  loss: 1.9184 (3.5670)  obj_loss: 0.5591 (0.9354)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5983 (1.0495)  1_CE_loss: 0.0083 (0.0549)  2_CE_loss: 0.0291 (0.1102)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0383 (0.1250)  3_DKS_loss: 0.0003 (0.0087)  4_CE_loss: 0.2583 (0.5512)  4_DKS_loss: 0.0009 (0.0169)  5_CE_loss: 0.3318 (0.6192)  5_DKS_loss: 0.0022 (0.0895)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0841 (1.1108)  data: 0.0899 (0.1313)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:48:19  iter: 13400  loss: 1.5748 (2.9746)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3043 (0.3061)  rel_ce_loss: 0.5792 (1.0656)  1_CE_loss: 0.0078 (0.0210)  2_CE_loss: 0.0303 (0.1061)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0285 (0.1401)  3_DKS_loss: 0.0003 (0.0096)  4_CE_loss: 0.2302 (0.5238)  4_DKS_loss: 0.0008 (0.0191)  5_CE_loss: 0.3595 (0.6843)  5_DKS_loss: 0.0026 (0.0962)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0748 (1.1150)  data: 0.0896 (0.1338)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:48:07  iter: 13400  loss: 1.7143 (3.5541)  obj_loss: 0.5981 (0.9329)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5327 (1.0460)  1_CE_loss: 0.0061 (0.0546)  2_CE_loss: 0.0290 (0.1097)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0222 (0.1243)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2274 (0.5490)  4_DKS_loss: 0.0009 (0.0168)  5_CE_loss: 0.3120 (0.6170)  5_DKS_loss: 0.0024 (0.0888)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0926 (1.1105)  data: 0.0912 (0.1310)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:46:27  iter: 13500  loss: 1.5575 (2.9640)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3094 (0.3062)  rel_ce_loss: 0.5731 (1.0618)  1_CE_loss: 0.0062 (0.0210)  2_CE_loss: 0.0364 (0.1056)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0273 (0.1393)  3_DKS_loss: 0.0003 (0.0095)  4_CE_loss: 0.2459 (0.5217)  4_DKS_loss: 0.0008 (0.0189)  5_CE_loss: 0.3471 (0.6819)  5_DKS_loss: 0.0026 (0.0955)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0672 (1.1148)  data: 0.0891 (0.1335)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:46:15  iter: 13500  loss: 1.7773 (3.5416)  obj_loss: 0.6177 (0.9305)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5778 (1.0425)  1_CE_loss: 0.0090 (0.0543)  2_CE_loss: 0.0326 (0.1091)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0350 (0.1237)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2295 (0.5468)  4_DKS_loss: 0.0009 (0.0167)  5_CE_loss: 0.2969 (0.6149)  5_DKS_loss: 0.0024 (0.0882)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0812 (1.1103)  data: 0.0886 (0.1308)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:44:35  iter: 13600  loss: 1.5374 (2.9537)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3117 (0.3062)  rel_ce_loss: 0.5658 (1.0582)  1_CE_loss: 0.0122 (0.0209)  2_CE_loss: 0.0310 (0.1050)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0276 (0.1385)  3_DKS_loss: 0.0003 (0.0095)  4_CE_loss: 0.2367 (0.5197)  4_DKS_loss: 0.0009 (0.0188)  5_CE_loss: 0.3406 (0.6794)  5_DKS_loss: 0.0027 (0.0948)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0894 (1.1146)  data: 0.0927 (0.1333)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:44:24  iter: 13600  loss: 1.8778 (3.5293)  obj_loss: 0.5879 (0.9281)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5650 (1.0390)  1_CE_loss: 0.0037 (0.0540)  2_CE_loss: 0.0295 (0.1086)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0262 (0.1230)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2749 (0.5447)  4_DKS_loss: 0.0008 (0.0166)  5_CE_loss: 0.3402 (0.6128)  5_DKS_loss: 0.0023 (0.0876)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0833 (1.1101)  data: 0.0882 (0.1306)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:42:43  iter: 13700  loss: 1.6117 (2.9435)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3135 (0.3063)  rel_ce_loss: 0.5749 (1.0546)  1_CE_loss: 0.0069 (0.0208)  2_CE_loss: 0.0362 (0.1045)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0321 (0.1378)  3_DKS_loss: 0.0003 (0.0094)  4_CE_loss: 0.2557 (0.5177)  4_DKS_loss: 0.0008 (0.0187)  5_CE_loss: 0.3663 (0.6771)  5_DKS_loss: 0.0024 (0.0941)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0721 (1.1145)  data: 0.0891 (0.1330)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:42:32  iter: 13700  loss: 1.7978 (3.5168)  obj_loss: 0.5703 (0.9257)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5591 (1.0356)  1_CE_loss: 0.0038 (0.0536)  2_CE_loss: 0.0311 (0.1080)  2_DKS_loss: 0.0001 (0.0028)  3_CE_loss: 0.0267 (0.1223)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2649 (0.5425)  4_DKS_loss: 0.0007 (0.0165)  5_CE_loss: 0.3207 (0.6107)  5_DKS_loss: 0.0024 (0.0870)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0785 (1.1099)  data: 0.0919 (0.1304)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:40:51  iter: 13800  loss: 1.5643 (2.9337)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3163 (0.3063)  rel_ce_loss: 0.5813 (1.0510)  1_CE_loss: 0.0107 (0.0207)  2_CE_loss: 0.0278 (0.1040)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0269 (0.1371)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2394 (0.5158)  4_DKS_loss: 0.0009 (0.0185)  5_CE_loss: 0.3325 (0.6748)  5_DKS_loss: 0.0028 (0.0934)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0856 (1.1142)  data: 0.0920 (0.1328)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:40:41  iter: 13800  loss: 1.8488 (3.5048)  obj_loss: 0.6089 (0.9235)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5979 (1.0323)  1_CE_loss: 0.0054 (0.0533)  2_CE_loss: 0.0309 (0.1075)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0289 (0.1217)  3_DKS_loss: 0.0003 (0.0084)  4_CE_loss: 0.2562 (0.5404)  4_DKS_loss: 0.0008 (0.0164)  5_CE_loss: 0.3327 (0.6087)  5_DKS_loss: 0.0022 (0.0863)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0609 (1.1097)  data: 0.0903 (0.1301)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:38:59  iter: 13900  loss: 1.5933 (2.9239)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3131 (0.3064)  rel_ce_loss: 0.5774 (1.0476)  1_CE_loss: 0.0151 (0.0207)  2_CE_loss: 0.0290 (0.1036)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0301 (0.1363)  3_DKS_loss: 0.0003 (0.0093)  4_CE_loss: 0.2358 (0.5139)  4_DKS_loss: 0.0008 (0.0184)  5_CE_loss: 0.3649 (0.6725)  5_DKS_loss: 0.0025 (0.0928)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0734 (1.1140)  data: 0.0908 (0.1326)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:38:49  iter: 13900  loss: 1.8339 (3.4932)  obj_loss: 0.6084 (0.9213)  loss_cal: 0.0037 (0.0036)  rel_ce_loss: 0.5721 (1.0291)  1_CE_loss: 0.0031 (0.0530)  2_CE_loss: 0.0272 (0.1070)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0282 (0.1211)  3_DKS_loss: 0.0003 (0.0083)  4_CE_loss: 0.2596 (0.5384)  4_DKS_loss: 0.0008 (0.0162)  5_CE_loss: 0.3231 (0.6067)  5_DKS_loss: 0.0023 (0.0857)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0720 (1.1095)  data: 0.0899 (0.1299)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:37:07  iter: 14000  loss: 1.4809 (2.9143)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3156 (0.3064)  rel_ce_loss: 0.5501 (1.0441)  1_CE_loss: 0.0016 (0.0206)  2_CE_loss: 0.0342 (0.1031)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0226 (0.1356)  3_DKS_loss: 0.0003 (0.0092)  4_CE_loss: 0.2255 (0.5120)  4_DKS_loss: 0.0008 (0.0183)  5_CE_loss: 0.3601 (0.6703)  5_DKS_loss: 0.0024 (0.0922)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0860 (1.1137)  data: 0.0926 (0.1323)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:36:58  iter: 14000  loss: 1.8419 (3.4817)  obj_loss: 0.6079 (0.9192)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5640 (1.0259)  1_CE_loss: 0.0037 (0.0527)  2_CE_loss: 0.0404 (0.1065)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0316 (0.1205)  3_DKS_loss: 0.0003 (0.0083)  4_CE_loss: 0.2469 (0.5364)  4_DKS_loss: 0.0008 (0.0161)  5_CE_loss: 0.3347 (0.6047)  5_DKS_loss: 0.0023 (0.0851)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0664 (1.1092)  data: 0.0899 (0.1296)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:35:15  iter: 14100  loss: 1.4612 (2.9045)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3103 (0.3065)  rel_ce_loss: 0.5603 (1.0407)  1_CE_loss: 0.0038 (0.0205)  2_CE_loss: 0.0286 (0.1026)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0244 (0.1349)  3_DKS_loss: 0.0003 (0.0091)  4_CE_loss: 0.2226 (0.5100)  4_DKS_loss: 0.0008 (0.0182)  5_CE_loss: 0.3418 (0.6680)  5_DKS_loss: 0.0025 (0.0915)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0753 (1.1135)  data: 0.0914 (0.1320)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:35:07  iter: 14100  loss: 1.8673 (3.4703)  obj_loss: 0.5776 (0.9168)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5776 (1.0227)  1_CE_loss: 0.0076 (0.0524)  2_CE_loss: 0.0291 (0.1060)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0294 (0.1199)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2505 (0.5344)  4_DKS_loss: 0.0008 (0.0160)  5_CE_loss: 0.3132 (0.6028)  5_DKS_loss: 0.0021 (0.0846)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0797 (1.1090)  data: 0.0905 (0.1294)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:33:23  iter: 14200  loss: 1.5128 (2.8949)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3183 (0.3065)  rel_ce_loss: 0.5516 (1.0373)  1_CE_loss: 0.0086 (0.0205)  2_CE_loss: 0.0306 (0.1021)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0251 (0.1341)  3_DKS_loss: 0.0003 (0.0091)  4_CE_loss: 0.2182 (0.5082)  4_DKS_loss: 0.0009 (0.0180)  5_CE_loss: 0.3492 (0.6657)  5_DKS_loss: 0.0027 (0.0909)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0786 (1.1132)  data: 0.0911 (0.1317)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:33:15  iter: 14200  loss: 1.7164 (3.4587)  obj_loss: 0.5229 (0.9146)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5604 (1.0195)  1_CE_loss: 0.0016 (0.0521)  2_CE_loss: 0.0308 (0.1055)  2_DKS_loss: 0.0001 (0.0027)  3_CE_loss: 0.0277 (0.1192)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2199 (0.5325)  4_DKS_loss: 0.0008 (0.0159)  5_CE_loss: 0.2968 (0.6009)  5_DKS_loss: 0.0020 (0.0840)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0701 (1.1088)  data: 0.0900 (0.1291)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:31:32  iter: 14300  loss: 1.4834 (2.8855)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3178 (0.3066)  rel_ce_loss: 0.5567 (1.0339)  1_CE_loss: 0.0060 (0.0204)  2_CE_loss: 0.0232 (0.1016)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0218 (0.1334)  3_DKS_loss: 0.0003 (0.0090)  4_CE_loss: 0.2317 (0.5063)  4_DKS_loss: 0.0008 (0.0179)  5_CE_loss: 0.3239 (0.6635)  5_DKS_loss: 0.0024 (0.0903)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0841 (1.1132)  data: 0.0884 (0.1316)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:31:24  iter: 14300  loss: 1.7671 (3.4472)  obj_loss: 0.5527 (0.9125)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5552 (1.0163)  1_CE_loss: 0.0019 (0.0518)  2_CE_loss: 0.0270 (0.1050)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0260 (0.1186)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2317 (0.5305)  4_DKS_loss: 0.0008 (0.0158)  5_CE_loss: 0.2950 (0.5989)  5_DKS_loss: 0.0021 (0.0834)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0710 (1.1087)  data: 0.0914 (0.1291)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:29:40  iter: 14400  loss: 1.6043 (2.8764)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3041 (0.3066)  rel_ce_loss: 0.6142 (1.0307)  1_CE_loss: 0.0028 (0.0203)  2_CE_loss: 0.0381 (0.1012)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0307 (0.1327)  3_DKS_loss: 0.0003 (0.0089)  4_CE_loss: 0.2511 (0.5046)  4_DKS_loss: 0.0008 (0.0178)  5_CE_loss: 0.3593 (0.6613)  5_DKS_loss: 0.0027 (0.0897)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0627 (1.1129)  data: 0.0915 (0.1314)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:29:33  iter: 14400  loss: 1.7240 (3.4358)  obj_loss: 0.5762 (0.9102)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5361 (1.0131)  1_CE_loss: 0.0021 (0.0515)  2_CE_loss: 0.0353 (0.1045)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0273 (0.1180)  3_DKS_loss: 0.0002 (0.0081)  4_CE_loss: 0.2238 (0.5286)  4_DKS_loss: 0.0007 (0.0157)  5_CE_loss: 0.3139 (0.5970)  5_DKS_loss: 0.0023 (0.0828)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0694 (1.1085)  data: 0.0907 (0.1288)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:27:48  iter: 14500  loss: 1.5145 (2.8670)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3220 (0.3067)  rel_ce_loss: 0.5418 (1.0273)  1_CE_loss: 0.0107 (0.0202)  2_CE_loss: 0.0256 (0.1007)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0242 (0.1320)  3_DKS_loss: 0.0003 (0.0089)  4_CE_loss: 0.2232 (0.5027)  4_DKS_loss: 0.0008 (0.0177)  5_CE_loss: 0.3301 (0.6592)  5_DKS_loss: 0.0026 (0.0891)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.1126)  data: 0.0923 (0.1311)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:27:42  iter: 14500  loss: 1.7529 (3.4245)  obj_loss: 0.5591 (0.9079)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5411 (1.0099)  1_CE_loss: 0.0039 (0.0513)  2_CE_loss: 0.0312 (0.1041)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0289 (0.1174)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2463 (0.5266)  4_DKS_loss: 0.0008 (0.0156)  5_CE_loss: 0.2973 (0.5951)  5_DKS_loss: 0.0023 (0.0823)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0769 (1.1082)  data: 0.0918 (0.1286)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:25:57  iter: 14600  loss: 1.5906 (2.8580)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3111 (0.3067)  rel_ce_loss: 0.5557 (1.0241)  1_CE_loss: 0.0025 (0.0202)  2_CE_loss: 0.0271 (0.1002)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0269 (0.1314)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2521 (0.5010)  4_DKS_loss: 0.0008 (0.0176)  5_CE_loss: 0.3725 (0.6571)  5_DKS_loss: 0.0027 (0.0885)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0865 (1.1124)  data: 0.0912 (0.1308)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:25:51  iter: 14600  loss: 1.7372 (3.4132)  obj_loss: 0.5908 (0.9055)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5343 (1.0068)  1_CE_loss: 0.0046 (0.0510)  2_CE_loss: 0.0245 (0.1036)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0214 (0.1168)  3_DKS_loss: 0.0003 (0.0080)  4_CE_loss: 0.2144 (0.5248)  4_DKS_loss: 0.0008 (0.0155)  5_CE_loss: 0.2882 (0.5933)  5_DKS_loss: 0.0021 (0.0817)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0840 (1.1080)  data: 0.0901 (0.1283)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:24:00  iter: 14700  loss: 1.7867 (3.4020)  obj_loss: 0.5488 (0.9032)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5387 (1.0037)  1_CE_loss: 0.0072 (0.0507)  2_CE_loss: 0.0333 (0.1031)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0277 (0.1163)  3_DKS_loss: 0.0003 (0.0079)  4_CE_loss: 0.2294 (0.5229)  4_DKS_loss: 0.0008 (0.0154)  5_CE_loss: 0.3052 (0.5914)  5_DKS_loss: 0.0022 (0.0812)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0900 (1.1079)  data: 0.0883 (0.1282)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:24:05  iter: 14700  loss: 1.4845 (2.8487)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3215 (0.3068)  rel_ce_loss: 0.5583 (1.0208)  1_CE_loss: 0.0079 (0.0201)  2_CE_loss: 0.0282 (0.0998)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0201 (0.1307)  3_DKS_loss: 0.0003 (0.0088)  4_CE_loss: 0.2208 (0.4991)  4_DKS_loss: 0.0008 (0.0174)  5_CE_loss: 0.3076 (0.6549)  5_DKS_loss: 0.0023 (0.0879)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0847 (1.1123)  data: 0.0897 (0.1307)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:22:09  iter: 14800  loss: 1.7753 (3.3911)  obj_loss: 0.5781 (0.9009)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5515 (1.0007)  1_CE_loss: 0.0038 (0.0504)  2_CE_loss: 0.0389 (0.1026)  2_DKS_loss: 0.0001 (0.0026)  3_CE_loss: 0.0381 (0.1157)  3_DKS_loss: 0.0003 (0.0079)  4_CE_loss: 0.2402 (0.5211)  4_DKS_loss: 0.0008 (0.0153)  5_CE_loss: 0.3258 (0.5897)  5_DKS_loss: 0.0021 (0.0807)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0692 (1.1077)  data: 0.0903 (0.1280)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:22:14  iter: 14800  loss: 1.5546 (2.8397)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3097 (0.3069)  rel_ce_loss: 0.5558 (1.0176)  1_CE_loss: 0.0058 (0.0200)  2_CE_loss: 0.0259 (0.0993)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0280 (0.1300)  3_DKS_loss: 0.0003 (0.0087)  4_CE_loss: 0.2436 (0.4973)  4_DKS_loss: 0.0008 (0.0173)  5_CE_loss: 0.3511 (0.6528)  5_DKS_loss: 0.0027 (0.0873)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0753 (1.1121)  data: 0.0903 (0.1305)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:20:18  iter: 14900  loss: 1.8102 (3.3803)  obj_loss: 0.6104 (0.8987)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5441 (0.9977)  1_CE_loss: 0.0080 (0.0501)  2_CE_loss: 0.0229 (0.1022)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0240 (0.1151)  3_DKS_loss: 0.0003 (0.0078)  4_CE_loss: 0.2491 (0.5193)  4_DKS_loss: 0.0008 (0.0152)  5_CE_loss: 0.3326 (0.5879)  5_DKS_loss: 0.0021 (0.0801)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0603 (1.1075)  data: 0.0903 (0.1278)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:20:23  iter: 14900  loss: 1.4609 (2.8308)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3193 (0.3070)  rel_ce_loss: 0.5325 (1.0144)  1_CE_loss: 0.0031 (0.0200)  2_CE_loss: 0.0254 (0.0989)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0251 (0.1293)  3_DKS_loss: 0.0004 (0.0087)  4_CE_loss: 0.2295 (0.4956)  4_DKS_loss: 0.0009 (0.0172)  5_CE_loss: 0.3291 (0.6507)  5_DKS_loss: 0.0028 (0.0868)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0777 (1.1120)  data: 0.0895 (0.1303)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:18:27  iter: 15000  loss: 1.7512 (3.3697)  obj_loss: 0.5054 (0.8965)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5558 (0.9948)  1_CE_loss: 0.0054 (0.0499)  2_CE_loss: 0.0394 (0.1017)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0311 (0.1146)  3_DKS_loss: 0.0003 (0.0077)  4_CE_loss: 0.2762 (0.5175)  4_DKS_loss: 0.0009 (0.0151)  5_CE_loss: 0.3413 (0.5861)  5_DKS_loss: 0.0024 (0.0796)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0712 (1.1073)  data: 0.0887 (0.1276)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:18:31  iter: 15000  loss: 1.5208 (2.8223)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3097 (0.3070)  rel_ce_loss: 0.5332 (1.0113)  1_CE_loss: 0.0028 (0.0199)  2_CE_loss: 0.0289 (0.0984)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0358 (0.1287)  3_DKS_loss: 0.0003 (0.0086)  4_CE_loss: 0.2492 (0.4939)  4_DKS_loss: 0.0009 (0.0171)  5_CE_loss: 0.3490 (0.6487)  5_DKS_loss: 0.0029 (0.0862)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0736 (1.1117)  data: 0.0902 (0.1300)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:16:36  iter: 15100  loss: 1.7068 (3.3591)  obj_loss: 0.5278 (0.8943)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5206 (0.9919)  1_CE_loss: 0.0082 (0.0496)  2_CE_loss: 0.0297 (0.1013)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0222 (0.1140)  3_DKS_loss: 0.0003 (0.0077)  4_CE_loss: 0.2289 (0.5157)  4_DKS_loss: 0.0009 (0.0150)  5_CE_loss: 0.3080 (0.5844)  5_DKS_loss: 0.0023 (0.0791)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0655 (1.1071)  data: 0.0894 (0.1275)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:16:40  iter: 15100  loss: 1.5106 (2.8137)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3177 (0.3071)  rel_ce_loss: 0.5331 (1.0082)  1_CE_loss: 0.0046 (0.0198)  2_CE_loss: 0.0324 (0.0980)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0256 (0.1280)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2389 (0.4922)  4_DKS_loss: 0.0009 (0.0170)  5_CE_loss: 0.3404 (0.6467)  5_DKS_loss: 0.0029 (0.0856)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0656 (1.1115)  data: 0.0877 (0.1299)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:14:45  iter: 15200  loss: 1.8410 (3.3490)  obj_loss: 0.5854 (0.8921)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5729 (0.9891)  1_CE_loss: 0.0052 (0.0494)  2_CE_loss: 0.0280 (0.1008)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0270 (0.1135)  3_DKS_loss: 0.0002 (0.0077)  4_CE_loss: 0.2708 (0.5140)  4_DKS_loss: 0.0008 (0.0149)  5_CE_loss: 0.3335 (0.5827)  5_DKS_loss: 0.0023 (0.0786)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0740 (1.1069)  data: 0.0913 (0.1273)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:14:49  iter: 15200  loss: 1.4878 (2.8052)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3137 (0.3071)  rel_ce_loss: 0.5530 (1.0051)  1_CE_loss: 0.0097 (0.0198)  2_CE_loss: 0.0271 (0.0976)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0254 (0.1274)  3_DKS_loss: 0.0003 (0.0085)  4_CE_loss: 0.2357 (0.4906)  4_DKS_loss: 0.0009 (0.0169)  5_CE_loss: 0.3465 (0.6447)  5_DKS_loss: 0.0027 (0.0851)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0764 (1.1113)  data: 0.0915 (0.1296)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:12:54  iter: 15300  loss: 1.7418 (3.3387)  obj_loss: 0.5767 (0.8900)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5279 (0.9863)  1_CE_loss: 0.0097 (0.0491)  2_CE_loss: 0.0289 (0.1004)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0285 (0.1130)  3_DKS_loss: 0.0003 (0.0076)  4_CE_loss: 0.2425 (0.5123)  4_DKS_loss: 0.0008 (0.0148)  5_CE_loss: 0.2937 (0.5810)  5_DKS_loss: 0.0021 (0.0781)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0893 (1.1066)  data: 0.0873 (0.1271)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:12:57  iter: 15300  loss: 1.5895 (2.7970)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3031 (0.3072)  rel_ce_loss: 0.5709 (1.0022)  1_CE_loss: 0.0096 (0.0197)  2_CE_loss: 0.0325 (0.0972)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0339 (0.1268)  3_DKS_loss: 0.0003 (0.0084)  4_CE_loss: 0.2300 (0.4889)  4_DKS_loss: 0.0009 (0.0168)  5_CE_loss: 0.3538 (0.6428)  5_DKS_loss: 0.0029 (0.0846)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0653 (1.1111)  data: 0.0887 (0.1294)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:11:03  iter: 15400  loss: 1.6955 (3.3285)  obj_loss: 0.5039 (0.8879)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5368 (0.9835)  1_CE_loss: 0.0047 (0.0489)  2_CE_loss: 0.0317 (0.1000)  2_DKS_loss: 0.0001 (0.0025)  3_CE_loss: 0.0348 (0.1125)  3_DKS_loss: 0.0003 (0.0076)  4_CE_loss: 0.2556 (0.5105)  4_DKS_loss: 0.0008 (0.0147)  5_CE_loss: 0.2975 (0.5792)  5_DKS_loss: 0.0021 (0.0776)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.1064)  data: 0.0910 (0.1268)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:11:06  iter: 15400  loss: 1.6385 (2.7888)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3067 (0.3072)  rel_ce_loss: 0.5968 (0.9993)  1_CE_loss: 0.0029 (0.0196)  2_CE_loss: 0.0264 (0.0968)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0418 (0.1262)  3_DKS_loss: 0.0003 (0.0084)  4_CE_loss: 0.2417 (0.4873)  4_DKS_loss: 0.0008 (0.0167)  5_CE_loss: 0.3570 (0.6409)  5_DKS_loss: 0.0023 (0.0840)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0746 (1.1108)  data: 0.0893 (0.1292)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:09:13  iter: 15500  loss: 1.7899 (3.3186)  obj_loss: 0.5518 (0.8859)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5614 (0.9808)  1_CE_loss: 0.0023 (0.0486)  2_CE_loss: 0.0283 (0.0996)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0285 (0.1120)  3_DKS_loss: 0.0003 (0.0075)  4_CE_loss: 0.2462 (0.5088)  4_DKS_loss: 0.0008 (0.0146)  5_CE_loss: 0.3317 (0.5776)  5_DKS_loss: 0.0023 (0.0771)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0697 (1.1062)  data: 0.0904 (0.1266)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:09:15  iter: 15500  loss: 1.5550 (2.7807)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3095 (0.3073)  rel_ce_loss: 0.5560 (0.9964)  1_CE_loss: 0.0103 (0.0196)  2_CE_loss: 0.0304 (0.0964)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0294 (0.1256)  3_DKS_loss: 0.0003 (0.0083)  4_CE_loss: 0.2337 (0.4858)  4_DKS_loss: 0.0008 (0.0166)  5_CE_loss: 0.3375 (0.6390)  5_DKS_loss: 0.0028 (0.0835)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0886 (1.1106)  data: 0.0877 (0.1289)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:07:22  iter: 15600  loss: 1.7865 (3.3087)  obj_loss: 0.5610 (0.8839)  loss_cal: 0.0040 (0.0036)  rel_ce_loss: 0.5480 (0.9780)  1_CE_loss: 0.0048 (0.0484)  2_CE_loss: 0.0269 (0.0992)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0296 (0.1115)  3_DKS_loss: 0.0003 (0.0075)  4_CE_loss: 0.2330 (0.5072)  4_DKS_loss: 0.0008 (0.0146)  5_CE_loss: 0.3159 (0.5759)  5_DKS_loss: 0.0024 (0.0766)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0722 (1.1061)  data: 0.0900 (0.1266)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:07:24  iter: 15600  loss: 1.4165 (2.7726)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3148 (0.3073)  rel_ce_loss: 0.5246 (0.9935)  1_CE_loss: 0.0034 (0.0195)  2_CE_loss: 0.0293 (0.0960)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0289 (0.1250)  3_DKS_loss: 0.0004 (0.0083)  4_CE_loss: 0.1862 (0.4841)  4_DKS_loss: 0.0009 (0.0165)  5_CE_loss: 0.2941 (0.6371)  5_DKS_loss: 0.0027 (0.0830)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0798 (1.1106)  data: 0.0901 (0.1289)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:05:31  iter: 15700  loss: 1.7391 (3.2987)  obj_loss: 0.5488 (0.8818)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5419 (0.9752)  1_CE_loss: 0.0064 (0.0481)  2_CE_loss: 0.0299 (0.0987)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0257 (0.1110)  3_DKS_loss: 0.0003 (0.0074)  4_CE_loss: 0.2419 (0.5055)  4_DKS_loss: 0.0008 (0.0145)  5_CE_loss: 0.3046 (0.5742)  5_DKS_loss: 0.0021 (0.0762)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0730 (1.1060)  data: 0.0902 (0.1264)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:05:33  iter: 15700  loss: 1.5981 (2.7647)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3132 (0.3074)  rel_ce_loss: 0.5838 (0.9906)  1_CE_loss: 0.0116 (0.0194)  2_CE_loss: 0.0357 (0.0956)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0291 (0.1244)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2415 (0.4826)  4_DKS_loss: 0.0008 (0.0164)  5_CE_loss: 0.3388 (0.6352)  5_DKS_loss: 0.0031 (0.0825)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0638 (1.1104)  data: 0.0885 (0.1286)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:03:41  iter: 15800  loss: 1.7519 (3.2892)  obj_loss: 0.5347 (0.8798)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5370 (0.9726)  1_CE_loss: 0.0084 (0.0479)  2_CE_loss: 0.0279 (0.0983)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0222 (0.1105)  3_DKS_loss: 0.0003 (0.0074)  4_CE_loss: 0.2435 (0.5040)  4_DKS_loss: 0.0008 (0.0144)  5_CE_loss: 0.3118 (0.5727)  5_DKS_loss: 0.0021 (0.0757)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0746 (1.1058)  data: 0.0902 (0.1262)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:03:42  iter: 15800  loss: 1.5295 (2.7570)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3105 (0.3074)  rel_ce_loss: 0.5351 (0.9879)  1_CE_loss: 0.0028 (0.0194)  2_CE_loss: 0.0354 (0.0952)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0358 (0.1238)  3_DKS_loss: 0.0003 (0.0082)  4_CE_loss: 0.2244 (0.4811)  4_DKS_loss: 0.0008 (0.0163)  5_CE_loss: 0.3438 (0.6334)  5_DKS_loss: 0.0024 (0.0820)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0996 (1.1102)  data: 0.0901 (0.1284)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:eta: 0:01:50  iter: 15900  loss: 1.7962 (3.2798)  obj_loss: 0.5674 (0.8779)  loss_cal: 0.0038 (0.0036)  rel_ce_loss: 0.5457 (0.9700)  1_CE_loss: 0.0086 (0.0476)  2_CE_loss: 0.0255 (0.0979)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0239 (0.1100)  3_DKS_loss: 0.0003 (0.0073)  4_CE_loss: 0.2503 (0.5024)  4_DKS_loss: 0.0009 (0.0143)  5_CE_loss: 0.3390 (0.5711)  5_DKS_loss: 0.0024 (0.0752)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0655 (1.1056)  data: 0.0886 (0.1260)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark:eta: 0:01:50  iter: 15900  loss: 1.4649 (2.7493)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3157 (0.3075)  rel_ce_loss: 0.5576 (0.9852)  1_CE_loss: 0.0054 (0.0193)  2_CE_loss: 0.0267 (0.0949)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0229 (0.1232)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2332 (0.4795)  4_DKS_loss: 0.0008 (0.0162)  5_CE_loss: 0.3155 (0.6316)  5_DKS_loss: 0.0026 (0.0815)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0990 (1.1100)  data: 0.0889 (0.1282)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark:---Total norm 4.61890 clip coef 1.08251-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 2.43620, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 2.07143, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.52475, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 1.18094, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: 0.97514, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.91958, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.75900, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.74355, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.67299, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.49483, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.41864, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.35582, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.34798, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.31005, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29580, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.29578, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.29179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.29179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.29179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.29179, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.27984, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.27534, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.26859, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.25919, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.25255, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.24620, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.24419, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23958, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23858, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23609, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23423, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.23093, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.22596, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.22368, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.21851, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.21849, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21072, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.20787, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.20202, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.20170, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.20054, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.19715, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19483, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.19269, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.19188, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.18689, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.18613, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.18567, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.18533, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.18028, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17924, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17854, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17389, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17232, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17177, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.17135, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.17079, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.17051, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.16044, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.15284, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.15220, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14953, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.14953, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.14396, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.14323, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.14314, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13983, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.13849, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.13550, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13454, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13435, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.13402, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.13113, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12845, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12825, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.12621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.12470, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.12000, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11999, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.11609, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.11480, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11416, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.11342, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10990, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10980, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10967, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10942, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10725, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.10676, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10330, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.10235, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.10092, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.10058, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.10034, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09882, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09870, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09863, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09803, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.09786, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09727, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09591, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.09399, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.09268, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.09258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.09250, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.09214, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.09004, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08897, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08874, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08811, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08755, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08696, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.08591, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08504, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08402, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.08353, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.08352, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.08296, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.07873, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07676, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07572, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.07448, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.07354, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07302, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07189, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07113, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.07081, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06886, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06876, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06845, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06737, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06656, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06573, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06496, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.06467, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06197, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06182, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06158, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.06127, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.06047, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06040, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05755, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05741, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.05666, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05576, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05519, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05440, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05384, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05315, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05274, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.05251, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05238, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.05173, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.04858, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04674, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.04610, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: 0.04593, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.04576, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04540, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04496, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.04403, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.04403, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04325, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03811, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03755, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03710, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03703, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03536, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.03478, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.03390, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03290, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03287, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03283, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03252, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03186, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.03153, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03149, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.03119, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03043, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.03036, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02993, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02909, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02822, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02799, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02794, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02739, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02718, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.02674, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.02490, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.02442, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02438, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.02318, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.02277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02242, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02231, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02230, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02222, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02216, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02212, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.02204, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02185, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02163, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02156, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.02150, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.02078, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02065, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02056, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02030, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.02025, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.02013, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01984, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01984, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01970, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.01965, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01955, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01915, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01883, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01880, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01874, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01864, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01857, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01853, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01850, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01840, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01828, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01818, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01802, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01789, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01769, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01762, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01762, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01733, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01619, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01598, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01580, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01561, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01506, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01502, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01478, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01457, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01440, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.01423, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.01423, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01399, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01352, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01342, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01340, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.01307, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01286, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01284, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01282, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01263, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01244, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01243, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01209, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01205, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01205, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01135, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01135, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.01119, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01117, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.01112, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.01095, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.01092, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01081, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01079, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.01078, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01068, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.01057, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01039, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01035, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00995, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00993, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00992, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00961, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00945, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00915, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00902, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00898, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00894, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00893, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00892, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00886, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.00886, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00882, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00882, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00879, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00848, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00844, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00844, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00838, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00838, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00838, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00837, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00825, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00822, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00820, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00809, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00795, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.00794, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00790, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00779, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00770, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00766, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00764, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00757, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00737, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00731, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00722, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00710, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00704, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00704, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00697, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00694, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00692, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00688, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00686, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00680, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00679, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00678, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00673, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00665, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00651, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00627, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00605, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00593, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00593, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00590, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00566, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00564, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00552, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00534, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00522, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00507, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00505, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00504, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00496, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00493, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00492, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00477, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00474, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00454, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00424, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00419, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00409, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00405, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00399, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00394, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00392, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00386, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00362, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00354, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00348, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00348, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00345, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00323, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00323, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00321, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00321, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00320, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00319, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00300, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00297, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00294, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00293, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00292, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00292, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00284, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00262, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00258, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00240, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00240, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00199, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00197, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00196, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00194, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00194, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.00191, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.00191, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00188, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00180, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00175, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00120, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00087, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00080, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00073, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00073, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00064, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00051, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00049, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00038, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 0:00:00  iter: 16000  loss: 1.8519 (3.2705)  obj_loss: 0.5869 (0.8760)  loss_cal: 0.0039 (0.0036)  rel_ce_loss: 0.5584 (0.9674)  1_CE_loss: 0.0071 (0.0474)  2_CE_loss: 0.0370 (0.0975)  2_DKS_loss: 0.0001 (0.0024)  3_CE_loss: 0.0308 (0.1095)  3_DKS_loss: 0.0003 (0.0073)  4_CE_loss: 0.2493 (0.5008)  4_DKS_loss: 0.0009 (0.0142)  5_CE_loss: 0.3085 (0.5696)  5_DKS_loss: 0.0023 (0.0748)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0777 (1.1054)  data: 0.0894 (0.1257)  lr: 0.001600  max mem: 11395
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-sgcls0.001/model_0016000.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-sgcls0.001/model_final.pth
INFO:maskrcnn_benchmark:Total training time: 4:54:49.932851 (1.1056 s / it)
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:---Total norm 2.68517 clip coef 1.86208-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.28765, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.weight: 0.95794, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: 0.94305, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: 0.81171, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: 0.69111, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 0.64056, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.56354, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: 0.44970, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.weight: 0.38871, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.weight: 0.37685, (torch.Size([39, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : 0.30406, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.28191, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: 0.26421, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.23806, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22356, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.22148, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.21834, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.21547, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.20349, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.19408, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: 0.18250, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.16202, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.15977, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.15325, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : 0.15210, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.weight: 0.14994, (torch.Size([39, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.weight: 0.13632, (torch.Size([5, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.13406, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.12800, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.11621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.weight: 0.11058, (torch.Size([11, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10711, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10484, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.10331, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.09579, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.09403, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08862, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08576, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08517, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.08228, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.08207, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.08056, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.08011, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07930, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07800, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07628, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07590, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07518, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.07464, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07437, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07346, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07220, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.07205, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07205, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.07177, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.07174, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.07130, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06994, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06993, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06953, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06884, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.weight: 0.06828, (torch.Size([20, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06726, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06713, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.06653, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.06585, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.06556, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06449, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.06439, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06408, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06260, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06202, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06097, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_5.bias : 0.06038, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_5.bias  : 0.06038, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.06021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.06001, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05999, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05997, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.05958, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : 0.05893, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05879, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05811, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.weight: 0.05804, (torch.Size([5, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05771, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05770, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05735, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05682, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05623, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05517, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.05509, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.05436, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05386, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05382, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.05380, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05359, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05338, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05337, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05312, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: 0.05267, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.05199, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.05021, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04969, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04867, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.04817, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04814, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04758, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04621, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04575, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04438, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04359, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.weight: 0.04334, (torch.Size([11, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04297, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04273, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04263, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04230, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.04189, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.04066, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04038, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.04011, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.04004, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03983, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03948, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03889, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03888, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03888, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.03886, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.03786, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.03720, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03616, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03610, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03582, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03500, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.03366, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03364, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: 0.03277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.03212, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.03204, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03202, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03166, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03154, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.03150, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03091, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03083, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03068, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03023, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03011, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.03003, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.02925, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02925, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02903, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.02875, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_4.bias : 0.02866, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_4.bias  : 0.02866, (torch.Size([39]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02865, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02853, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02796, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02775, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.weight: 0.02766, (torch.Size([20, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02631, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02625, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: 0.02580, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02561, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02546, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02518, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.02474, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02428, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02280, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.02279, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02247, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.02141, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.02123, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.01996, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: 0.01935, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: 0.01864, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: 0.01849, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01833, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: 0.01804, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : 0.01770, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.01756, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01712, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01695, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01630, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01595, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01513, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01445, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01438, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01357, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.01272, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.01266, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01261, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01206, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: 0.01183, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01178, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.01143, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : 0.01097, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.01053, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.01044, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.01038, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.01028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_1.bias : 0.01022, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_1.bias  : 0.01022, (torch.Size([5]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00965, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00965, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00956, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00932, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00930, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00916, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00852, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00761, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00756, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00741, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00712, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00701, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00688, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00667, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00650, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00638, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00630, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00611, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00610, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00587, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00584, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00577, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00572, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00563, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00527, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00523, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00516, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00516, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00515, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00512, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : 0.00511, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00511, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00510, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00508, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00505, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00504, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00498, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00495, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00494, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00490, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00488, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00487, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00487, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00486, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00484, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00482, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00480, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00472, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00472, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00465, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00464, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00463, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00462, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00461, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00458, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00457, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00453, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_2.bias : 0.00453, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_2.bias  : 0.00453, (torch.Size([11]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00449, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00449, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00448, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00447, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00447, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00446, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00444, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00443, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00442, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00441, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00431, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00427, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: 0.00425, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00423, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00415, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00412, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00412, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00409, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00408, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00407, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00404, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00404, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00404, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00398, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00398, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00394, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: 0.00393, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00392, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00389, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00388, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00387, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00385, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00384, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00382, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00379, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00379, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00379, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00378, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00377, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00372, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00370, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00367, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00366, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00365, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: 0.00364, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00363, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00358, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00356, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00353, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00350, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00344, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: 0.00343, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_classifer_3.bias : 0.00338, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress_3.bias  : 0.00338, (torch.Size([20]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: 0.00331, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00313, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00312, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00277, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00263, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00229, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00217, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00211, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00200, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00196, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00195, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00189, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00175, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00169, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00165, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00162, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00159, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00158, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00157, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00152, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00151, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00151, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00150, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00149, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00148, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00147, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00147, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00146, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00143, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00143, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00143, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00142, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00141, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00139, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00139, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00138, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00137, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00135, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00133, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00132, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00132, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00131, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: 0.00129, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00129, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00124, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00122, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00116, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00116, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00115, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00114, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00114, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00108, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: 0.00108, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00108, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: 0.00107, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: 0.00106, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00105, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00102, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00085, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00084, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00081, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00078, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00078, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00076, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00070, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00066, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00065, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00061, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00044, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00043, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: 0.00036, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00028, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00026, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: 0.00013, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 0:00:00  iter: 16000  loss: 1.4681 (2.7417)  obj_loss: 0.0000 (0.0000)  loss_cal: 0.3165 (0.3075)  rel_ce_loss: 0.5274 (0.9824)  1_CE_loss: 0.0036 (0.0193)  2_CE_loss: 0.0262 (0.0945)  2_DKS_loss: 0.0001 (0.0023)  3_CE_loss: 0.0201 (0.1227)  3_DKS_loss: 0.0003 (0.0081)  4_CE_loss: 0.2148 (0.4781)  4_DKS_loss: 0.0008 (0.0161)  5_CE_loss: 0.3287 (0.6299)  5_DKS_loss: 0.0025 (0.0810)  loss_fg: 0.0000 (0.0000)  loss_bg: 0.0000 (0.0000)  time: 1.0790 (1.1097)  data: 0.0908 (0.1280)  lr: 0.001600  max mem: 10898
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-predcls0.1/model_0016000.pth
INFO:maskrcnn_benchmark.utils.checkpoint:Saving checkpoint to ./checkpoints/LAMBDA/Lxy1-predcls0.1/model_final.pth
INFO:maskrcnn_benchmark:Total training time: 4:56:00.078101 (1.1100 s / it)
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_test dataset(26446 images).
INFO:maskrcnn_benchmark:Start evaluation on VG_stanford_filtered_with_attribute_test dataset(26446 images).
INFO:maskrcnn_benchmark:Total run time: 0:25:32.962299 (0.05796575282116408 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:21:59.017838 (0.04987589193978938 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Total run time: 0:25:38.024578 (0.058157172261122485 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:Model inference time: 0:22:08.301881 (0.05022694853947309 s / img per device, on 1 devices)
INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.5922
====================================================================================================
SGG eval:     R @ 20: 0.2818;     R @ 50: 0.3379;     R @ 100: 0.3578;  for mode=sgcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.3078;  ng-R @ 50: 0.4027;  ng-R @ 100: 0.4588;  for mode=sgcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.0763;    zR @ 50: 0.1028;    zR @ 100: 0.1166;  for mode=sgcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.0843; ng-zR @ 50: 0.1281; ng-zR @ 100: 0.1685;  for mode=sgcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0565;    mR @ 50: 0.0811;    mR @ 100: 0.0955;  for mode=sgcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0691) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0932) (attached to:0.0208) (behind:0.2114) (belonging to:0.0101) (between:0.0035) (carrying:0.1115) (covered in:0.0470) (covering:0.0194) (eating:0.0583) (flying in:0.0000) (for:0.0307) (from:0.0000) (growing on:0.0000) (hanging from:0.0189) (has:0.4634) (holding:0.3118) (in:0.1602) (in front of:0.1536) (laying on:0.0405) (looking at:0.0930) (lying on:0.0102) (made of:0.0000) (mounted on:0.0000) (near:0.1740) (of:0.4073) (on:0.4029) (on back of:0.0000) (over:0.0521) (painted on:0.0172) (parked on:0.1429) (part of:0.0247) (playing:0.0000) (riding:0.3047) (says:0.0000) (sitting on:0.1207) (standing on:0.0446) (to:0.0000) (under:0.1144) (using:0.1594) (walking in:0.0000) (walking on:0.1342) (watching:0.0671) (wearing:0.4937) (wears:0.0710) (with:0.1174) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0586;    mR @ 50: 0.0865;    mR @ 100: 0.1029;  for mode=sgcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0570) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.1095) (attached to:0.0251) (behind:0.2172) (belonging to:0.0108) (between:0.0046) (carrying:0.1223) (covered in:0.0663) (covering:0.0165) (eating:0.0976) (flying in:0.0000) (for:0.0282) (from:0.0000) (growing on:0.0000) (hanging from:0.0147) (has:0.5166) (holding:0.3444) (in:0.1921) (in front of:0.1473) (laying on:0.0329) (looking at:0.0946) (lying on:0.0294) (made of:0.0000) (mounted on:0.0000) (near:0.1778) (of:0.4475) (on:0.4367) (on back of:0.0000) (over:0.0489) (painted on:0.0098) (parked on:0.1712) (part of:0.0141) (playing:0.0000) (riding:0.3349) (says:0.0000) (sitting on:0.1313) (standing on:0.0490) (to:0.0000) (under:0.1120) (using:0.1719) (walking in:0.0000) (walking on:0.1420) (watching:0.0867) (wearing:0.5069) (wears:0.0620) (with:0.1127) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0673; ng-mR @ 50: 0.1169; ng-mR @ 100: 0.1688;  for mode=sgcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1870) (across:0.0159) (against:0.0000) (along:0.0367) (and:0.0296) (at:0.1781) (attached to:0.0698) (behind:0.3058) (belonging to:0.0556) (between:0.0382) (carrying:0.2439) (covered in:0.1446) (covering:0.0497) (eating:0.1179) (flying in:0.0000) (for:0.1148) (from:0.0000) (growing on:0.0000) (hanging from:0.0851) (has:0.5234) (holding:0.3706) (in:0.2394) (in front of:0.2545) (laying on:0.2050) (looking at:0.1904) (lying on:0.1303) (made of:0.0000) (mounted on:0.0438) (near:0.2992) (of:0.5039) (on:0.5050) (on back of:0.0567) (over:0.1046) (painted on:0.0554) (parked on:0.4181) (part of:0.1029) (playing:0.0000) (riding:0.4251) (says:0.0000) (sitting on:0.2369) (standing on:0.1671) (to:0.1189) (under:0.2214) (using:0.2133) (walking in:0.0376) (walking on:0.2764) (watching:0.0892) (wearing:0.4993) (wears:0.2255) (with:0.2528) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.0331; zs-mR @ 50: 0.0456; zs-mR @ 100: 0.0558;  for mode=sgcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.0849) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0189) (attached to:0.0328) (behind:0.1374) (belonging to:0.0864) (between:0.0072) (carrying:0.0167) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0292) (from:0.0000) (growing on:0.0000) (hanging from:0.0312) (has:0.1407) (holding:0.0751) (in:0.1311) (in front of:0.1670) (laying on:0.1042) (looking at:0.1500) (lying on:0.0270) (made of:0.0000) (mounted on:0.0000) (near:0.1904) (of:0.1115) (on:0.1477) (on back of:0.0000) (over:0.0687) (painted on:0.0357) (parked on:0.0000) (part of:0.0606) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.1459) (standing on:0.1333) (to:0.0000) (under:0.1434) (using:0.0000) (walking in:0.0000) (walking on:0.1765) (watching:0.0976) (wearing:0.0610) (wears:0.0233) (with:0.1534) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.2248;     A @ 50: 0.2249;     A @ 100: 0.2249;  for mode=sgcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:
====================================================================================================
Detection evaluation mAp=0.9995
====================================================================================================
SGG eval:     R @ 20: 0.2969;     R @ 50: 0.4131;     R @ 100: 0.4704;  for mode=predcls, type=Recall(Main).
SGG eval:  ng-R @ 20: 0.3218;  ng-R @ 50: 0.4898;  ng-R @ 100: 0.6198;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:    zR @ 20: 0.2573;    zR @ 50: 0.3359;    zR @ 100: 0.3832;  for mode=predcls, type=Zero Shot Recall.
SGG eval: ng-zR @ 20: 0.2741; ng-zR @ 50: 0.3924; ng-zR @ 100: 0.4984;  for mode=predcls, type=No Graph Constraint Zero Shot Recall.
SGG eval:    mR @ 20: 0.0540;    mR @ 50: 0.0896;    mR @ 100: 0.1149;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0520) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0346) (attached to:0.0031) (behind:0.2253) (belonging to:0.0169) (between:0.0139) (carrying:0.1183) (covered in:0.1292) (covering:0.0000) (eating:0.1035) (flying in:0.0000) (for:0.0041) (from:0.0000) (growing on:0.0000) (hanging from:0.0352) (has:0.6031) (holding:0.4086) (in:0.2197) (in front of:0.1170) (laying on:0.1079) (looking at:0.0937) (lying on:0.0459) (made of:0.0000) (mounted on:0.0000) (near:0.1298) (of:0.4512) (on:0.4936) (on back of:0.0355) (over:0.0561) (painted on:0.0172) (parked on:0.0337) (part of:0.0123) (playing:0.0000) (riding:0.3464) (says:0.0000) (sitting on:0.1450) (standing on:0.0696) (to:0.0082) (under:0.1762) (using:0.0125) (walking in:0.0282) (walking on:0.1842) (watching:0.1753) (wearing:0.8881) (wears:0.0211) (with:0.1287) 
--------------------------------------------------------
SGG eval:    mR @ 20: 0.0549;    mR @ 50: 0.0935;    mR @ 100: 0.1213;  for mode=predcls, type=Mean Micro Recall.
----------------------- Details ------------------------
(above:0.0385) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0320) (attached to:0.0024) (behind:0.2181) (belonging to:0.0188) (between:0.0278) (carrying:0.1223) (covered in:0.1429) (covering:0.0000) (eating:0.1509) (flying in:0.0000) (for:0.0028) (from:0.0000) (growing on:0.0000) (hanging from:0.0442) (has:0.6614) (holding:0.4370) (in:0.2531) (in front of:0.1223) (laying on:0.1018) (looking at:0.0917) (lying on:0.0529) (made of:0.0000) (mounted on:0.0000) (near:0.1315) (of:0.4942) (on:0.5178) (on back of:0.0337) (over:0.0550) (painted on:0.0098) (parked on:0.0205) (part of:0.0070) (playing:0.0000) (riding:0.3944) (says:0.0000) (sitting on:0.1591) (standing on:0.0930) (to:0.0065) (under:0.1683) (using:0.0090) (walking in:0.0177) (walking on:0.1944) (watching:0.2143) (wearing:0.8857) (wears:0.0185) (with:0.1150) 
--------------------------------------------------------
SGG eval: ng-mR @ 20: 0.0609; ng-mR @ 50: 0.1179; ng-mR @ 100: 0.1830;  for mode=predcls, type=No Graph Constraint Mean Recall.
----------------------- Details ------------------------
(above:0.1545) (across:0.0000) (against:0.0000) (along:0.0183) (and:0.0473) (at:0.0814) (attached to:0.0564) (behind:0.3058) (belonging to:0.0427) (between:0.0278) (carrying:0.2591) (covered in:0.1673) (covering:0.0581) (eating:0.1796) (flying in:0.0000) (for:0.0820) (from:0.0000) (growing on:0.0000) (hanging from:0.0870) (has:0.6888) (holding:0.5033) (in:0.3283) (in front of:0.2104) (laying on:0.2922) (looking at:0.1807) (lying on:0.1684) (made of:0.0000) (mounted on:0.0104) (near:0.2436) (of:0.5785) (on:0.6399) (on back of:0.0567) (over:0.1231) (painted on:0.0431) (parked on:0.1470) (part of:0.0556) (playing:0.0379) (riding:0.5285) (says:0.0000) (sitting on:0.2786) (standing on:0.1962) (to:0.0656) (under:0.3129) (using:0.0375) (walking in:0.0423) (walking on:0.2611) (watching:0.2389) (wearing:0.9115) (wears:0.1237) (with:0.2765) 
--------------------------------------------------------
SGG eval: zs-mR @ 20: 0.1022; zs-mR @ 50: 0.1454; zs-mR @ 100: 0.1730;  for mode=predcls, type=Zero-Shot Mean Recall.
----------------------- Details ------------------------
(above:0.2103) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0755) (attached to:0.0109) (behind:0.4075) (belonging to:0.2099) (between:0.0290) (carrying:0.1833) (covered in:0.0833) (covering:0.0000) (eating:0.2647) (flying in:0.0000) (for:0.0125) (from:0.0000) (growing on:0.0000) (hanging from:0.1157) (has:0.5523) (holding:0.3869) (in:0.4866) (in front of:0.3088) (laying on:0.2708) (looking at:0.3083) (lying on:0.1351) (made of:0.0000) (mounted on:0.0000) (near:0.4654) (of:0.4048) (on:0.7033) (on back of:0.1250) (over:0.1038) (painted on:0.0357) (parked on:0.0000) (part of:0.0303) (playing:0.0000) (riding:0.1458) (says:0.0000) (sitting on:0.3202) (standing on:0.3104) (to:0.0263) (under:0.4174) (using:0.0000) (walking in:0.1250) (walking on:0.2059) (watching:0.3463) (wearing:0.1707) (wears:0.1647) (with:0.4973) 
--------------------------------------------------------
SGG eval:     A @ 20: 0.3206;     A @ 50: 0.3210;     A @ 100: 0.3210;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

INFO:maskrcnn_benchmark:Using 1 GPUs
INFO:maskrcnn_benchmark:Namespace(config_file='configs/e2e_relation_X_101_32_8_FPN_1x.yaml', distributed=False, local_rank=0, opts=['SOLVER.PRE_VAL', 'False', 'MODEL.ROI_RELATION_HEAD.LAMBDA_', '0.01', 'MODEL.ROI_RELATION_HEAD.PRUNE_RATE', '0.85', 'MODEL.ROI_RELATION_HEAD.PREDICT_USE_BIAS', 'False', 'MODEL.ROI_RELATION_HEAD.USE_GT_BOX', 'True', 'MODEL.ROI_RELATION_HEAD.USE_GT_OBJECT_LABEL', 'False', 'MODEL.ROI_RELATION_HEAD.PREDICTOR', 'NPredictor101', 'SOLVER.IMS_PER_BATCH', '16', 'TEST.IMS_PER_BATCH', '2', 'DTYPE', 'float16', 'SOLVER.MAX_ITER', '16000', 'SOLVER.BASE_LR', '0.001', 'SOLVER.SCHEDULE.TYPE', 'WarmupMultiStepLR', 'SOLVER.STEPS', '(10000, 16000)', 'SOLVER.VAL_PERIOD', '10000', 'SOLVER.CHECKPOINT_PERIOD', '16000', 'GLOVE_DIR', '/media/n702/data1/Lxy/datasets/glove', 'MODEL.PRETRAINED_DETECTOR_CKPT', '/media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth', 'OUTPUT_DIR', './checkpoints/NPredictor101-sgcls-loss'], skip_test=False)
INFO:maskrcnn_benchmark:Collecting env info (might take some time)
INFO:maskrcnn_benchmark:
PyTorch version: 1.9.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.6 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
Clang version: Could not collect
CMake version: Could not collect
Libc version: glibc-2.17

Python version: 3.7 (64-bit runtime)
Python platform: Linux-5.15.0-78-generic-x86_64-with-debian-bullseye-sid
Is CUDA available: True
CUDA runtime version: 11.1.74
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 535.54.03
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.5
[pip3] torch==1.9.1+cu111
[pip3] torchaudio==0.9.1
[pip3] torchvision==0.10.1+cu111
[conda] blas                      1.0                         mkl  
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py37h7f8727e_0  
[conda] mkl_fft                   1.3.1            py37hd3c417c_0  
[conda] mkl_random                1.2.2            py37h51133e4_0  
[conda] numpy                     1.21.5           py37h6c91a56_3  
[conda] numpy-base                1.21.5           py37ha15fc14_3  
[conda] torch                     1.9.1+cu111              pypi_0    pypi
[conda] torchaudio                0.9.1                    pypi_0    pypi
[conda] torchvision               0.10.1+cu111             pypi_0    pypi
        Pillow (9.5.0)
INFO:maskrcnn_benchmark:Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
INFO:maskrcnn_benchmark:
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    #PREDICTOR: "TransformerPredictor"
    PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048 
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.01
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: (10000, 16000)
  MAX_ITER: 40000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
OUTPUT_DIR: './output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5
    PRE_NMS_PREDICTION_THRES: 0.3
  CUSTUM_EVAL: False       # eval SGDet model on custum images, output a json
  CUSTUM_PATH: '.'         # the folder that contains the custum images, only jpg files are allowed  

INFO:maskrcnn_benchmark:Running with config:
ALPHA: 1.0
AMP_VERBOSE: False
BETA: 1.0
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 0
  SIZE_DIVISIBILITY: 32
DATASETS:
  POST_NMS: True
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TO_TEST: None
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DETECTED_SGG_DIR: .
DTYPE: float16
GAMMA: 1.0
GLOBAL_SETTING:
  BASIC_ENCODER: Hybrid-Attention
  CHOOSE_BEST_MODEL_BY_METRIC: _mean_recall
  DATASET_CHOICE: VG
  GCL_SETTING:
    GROUP_SPLIT_MODE: divide4
    KNOWLEDGE_LOSS_COEFFICIENT: 1.0
    KNOWLEDGE_TRANSFER_MODE: KL_logit_TopDown
    NO_RELATION_PENALTY: 0.1
    NO_RELATION_RESTRAIN: True
    ZERO_LABEL_PADDING_MODE: rand_insert
  PRINT_INTERVAL: 100
  RELATION_PREDICTOR: TransLike_GCL
  USE_BIAS: True
GLOVE_DIR: /media/n702/data1/Lxy/datasets/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
LOSS: dnorm
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    META_ARCH: Default
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    DECOUPLE_INPUT: False
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    GPR_TYPE: default
    L21_LOSS: None
    LABEL_SMOOTHING_LOSS: False
    LAMBDA_: 0.01
    LOSS: Default
    META_ARCH: Default
    MP_LAYER_NUM: 2
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PPR_ALPHA: -0.5
    PREDICTOR: NPredictor101
    PREDICT_USE_BIAS: False
    PREDICT_USE_VISION: True
    PRUNE_RATE: 0.85
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    SOFTTRIPLE: False
    SOFTTRIPLE_GAMMA: 0.0
    SOFTTRIPLE_K: 1
    SOFTTRIPLE_LAMBDA: 0.0
    SOFTTRIPLE_MARGIN: 0.0
    SOFTTRIPLE_MARGIN_INFER: False
    SOFTTRIPLE_TAU: 0.0
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: False
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: ./checkpoints/NPredictor101-sgcls-loss
PATHS_CATALOG: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /media/n702/data1/Lxy/T-CAR/maskrcnn_benchmark/config/../data/datasets
SOLVER:
  BASE_LR: 0.001
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 16000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 16
  MAX_ITER: 16000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.1
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (10000, 16000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 10000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: True
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  CUSTUM_EVAL: False
  CUSTUM_PATH: .
  DETECTIONS_PER_IMG: 100
  ESTIMATE_EVAL: False
  ESTIMATE_K: 2
  ESTIMATE_TAU: 0.1
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 2
  LOAD_ESTIMATE: False
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    PRE_NMS_PREDICTION_THRES: 0.3
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
INFO:maskrcnn_benchmark:Saving config into: ./checkpoints/NPredictor101-sgcls-loss/config.yml
INFO:maskrcnn_benchmark:#################### prepare training ####################
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark.data.build:get dataset statistics...
INFO:maskrcnn_benchmark.data.build:Unable to load data statistics from: ./checkpoints/NPredictor101-sgcls-loss/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:finish
INFO:maskrcnn_benchmark.data.build:Save data statistics to: ./checkpoints/NPredictor101-sgcls-loss/VG_stanford_filtered_with_attribute_train_statistics.cache
INFO:maskrcnn_benchmark.data.build:----------------------------------------------------------------------------------------------------
INFO:maskrcnn_benchmark:#################### end model construction ####################
INFO:maskrcnn_benchmark:#################### end optimizer and shcedule ####################
INFO:maskrcnn_benchmark:#################### end distributed ####################
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from /media/n702/data1/Lxy/datasets/vg/pretrained_faster_rcnn/model_final.pth
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                                                                      loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                                                                                    loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight of shape (2048, 512, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight of shape (512, 2048, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight of shape (512, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge_visual.weight of shape (512, 4608)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_textual.weight of shape (512, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.bias of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj_visual.weight of shape (512, 4224)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                                                                                loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                                                                                  loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
INFO:maskrcnn_benchmark.utils.model_serialization:REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                                                                                loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
INFO:maskrcnn_benchmark:#################### end load checkpointer ####################
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark.utils.miscellaneous:Saving labels mapping into ./checkpoints/NPredictor101-sgcls-loss/labels.json
WARNING:maskrcnn_benchmark.data.build:When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14
INFO:maskrcnn_benchmark:#################### end dataloader ####################
INFO:maskrcnn_benchmark:Start training
INFO:maskrcnn_benchmark:---Total norm nan clip coef nan-----------------
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: nan, (torch.Size([256, 1024, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.weight: nan, (torch.Size([128, 2, 7, 7]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.0.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.weight: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.2.bias: inf, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.weight: nan, (torch.Size([256, 128, 3, 3]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.4.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.weight: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.union_feature_extractor.rect_conv.6.bias: nan, (torch.Size([256]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.weight: nan, (torch.Size([4096, 12544]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc6.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.weight: nan, (torch.Size([4096, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.box_feature_extractor.fc7.bias : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed1.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.obj_embed2.weight: nan, (torch.Size([151, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: nan, (torch.Size([32, 9]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: nan, (torch.Size([32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: nan, (torch.Size([128, 32]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: nan, (torch.Size([128]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.weight: nan, (torch.Size([512, 4224]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_obj_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.weight: nan, (torch.Size([512, 4608]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_visual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.weight: nan, (torch.Size([512, 200]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.lin_edge_textual.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.weight: nan, (torch.Size([151, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.out_obj.bias: nan, (torch.Size([151]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.2.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_obj.cross_module.3.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.0.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_vis.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.SA_Cell_txt.SA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_vis.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_qs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_ks.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.w_vs.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.weight: nan, (torch.Size([512, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.slf_attn.fc.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.weight: nan, (torch.Size([2048, 512, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_1.bias: inf, (torch.Size([2048]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.weight: nan, (torch.Size([512, 2048, 1]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.w_2.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.weight: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.context_layer.context_edge.cross_module.1.CA_Cell_txt.CA_transformer_encoder.transformer_layer.pos_ffn.layer_norm.bias: nan, (torch.Size([512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.weight      : nan, (torch.Size([1024, 512]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_emb.bias        : nan, (torch.Size([1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.weight      : nan, (torch.Size([4096, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.post_cat.bias        : nan, (torch.Size([4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.weight  : nan, (torch.Size([51, 4096]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.rel_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.weight  : nan, (torch.Size([51, 1024]))
INFO:maskrcnn_benchmark:roi_heads.relation.predictor.ctx_compress.bias    : nan, (torch.Size([51]))
INFO:maskrcnn_benchmark:-------------------------------
INFO:maskrcnn_benchmark:eta: 4:58:55  iter: 1  loss: 1173.7767 (1173.7767)  obj_loss: 5.5312 (5.5312)  loss_fg: 85.7219 (85.7219)  loss_bg: 1082.5236 (1082.5236)  time: 1.1210 (1.1210)  data: 0.1375 (0.1375)  lr: 0.001600  max mem: 7918
INFO:maskrcnn_benchmark:eta: 4:31:03  iter: 100  loss: 48.6556 (240.0065)  obj_loss: 5.4648 (5.6573)  loss_fg: 30.7586 (59.5566)  loss_bg: 4.6463 (174.7926)  time: 1.0349 (1.0229)  data: 0.1031 (0.0999)  lr: 0.004451  max mem: 9651
INFO:maskrcnn_benchmark:eta: 4:30:34  iter: 200  loss: 43.2440 (151.6823)  obj_loss: 4.9141 (5.3447)  loss_fg: 29.7127 (48.1309)  loss_bg: 2.3638 (98.2066)  time: 1.0410 (1.0275)  data: 0.0892 (0.0987)  lr: 0.007331  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:28:48  iter: 300  loss: 27.0761 (116.2327)  obj_loss: 4.3789 (5.0892)  loss_fg: 18.6770 (39.8803)  loss_bg: 3.0688 (71.2632)  time: 1.0268 (1.0273)  data: 0.0938 (0.0981)  lr: 0.010211  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:27:07  iter: 400  loss: 21.9648 (94.6042)  obj_loss: 3.4297 (4.7793)  loss_fg: 13.0664 (33.7921)  loss_bg: 2.7895 (56.0328)  time: 1.0275 (1.0274)  data: 0.0908 (0.0976)  lr: 0.013091  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:24:55  iter: 500  loss: 23.9005 (82.3653)  obj_loss: 2.7949 (4.4401)  loss_fg: 14.5864 (29.9795)  loss_bg: 1.5743 (47.9457)  time: 1.0186 (1.0255)  data: 0.0902 (0.0972)  lr: 0.015971  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:23:10  iter: 600  loss: 17.2653 (74.7220)  obj_loss: 2.2598 (4.1110)  loss_fg: 10.4866 (27.1616)  loss_bg: 1.6525 (43.4495)  time: 1.0295 (1.0253)  data: 0.0875 (0.0971)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:21:14  iter: 700  loss: 14.9427 (66.7067)  obj_loss: 1.8877 (3.8108)  loss_fg: 8.1058 (24.8370)  loss_bg: 1.9665 (38.0589)  time: 1.0345 (1.0245)  data: 0.0894 (0.0969)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:19:31  iter: 800  loss: 15.8647 (61.0093)  obj_loss: 1.7305 (3.5542)  loss_fg: 10.9054 (23.1028)  loss_bg: 1.8426 (34.3523)  time: 1.0352 (1.0244)  data: 0.0932 (0.0969)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:17:44  iter: 900  loss: 14.4212 (56.4415)  obj_loss: 1.5283 (3.3345)  loss_fg: 8.5554 (21.7057)  loss_bg: 1.9774 (31.4013)  time: 1.0165 (1.0241)  data: 0.0873 (0.0967)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:15:53  iter: 1000  loss: 13.4762 (52.8771)  obj_loss: 1.3955 (3.1482)  loss_fg: 8.7225 (20.5079)  loss_bg: 1.4095 (29.2211)  time: 1.0408 (1.0236)  data: 0.0941 (0.0969)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:14:15  iter: 1100  loss: 17.2372 (49.6495)  obj_loss: 1.2246 (2.9841)  loss_fg: 9.7762 (19.5226)  loss_bg: 2.6030 (27.1428)  time: 1.0336 (1.0239)  data: 0.0903 (0.0970)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:12:37  iter: 1200  loss: 12.1232 (46.8976)  obj_loss: 1.2109 (2.8436)  loss_fg: 8.3931 (18.6398)  loss_bg: 1.4122 (25.4142)  time: 1.0339 (1.0242)  data: 0.0917 (0.0971)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:10:49  iter: 1300  loss: 12.4537 (44.5470)  obj_loss: 1.1797 (2.7215)  loss_fg: 7.7145 (17.8957)  loss_bg: 1.4838 (23.9297)  time: 1.0178 (1.0238)  data: 0.0897 (0.0970)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:09:02  iter: 1400  loss: 12.4058 (42.3664)  obj_loss: 1.2334 (2.6128)  loss_fg: 7.2980 (17.2238)  loss_bg: 1.0675 (22.5299)  time: 1.0220 (1.0235)  data: 0.0890 (0.0969)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:07:18  iter: 1500  loss: 11.8700 (40.4250)  obj_loss: 1.1221 (2.5155)  loss_fg: 8.6686 (16.6182)  loss_bg: 1.6281 (21.2913)  time: 1.0416 (1.0233)  data: 0.0921 (0.0967)  lr: 0.016000  max mem: 10013
INFO:maskrcnn_benchmark:eta: 4:05:41  iter: 1600  loss: 12.0684 (38.8581)  obj_loss: 1.1680 (2.4304)  loss_fg: 9.1622 (16.1399)  loss_bg: 1.7710 (20.2877)  time: 1.0387 (1.0237)  data: 0.0925 (0.0967)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 4:04:00  iter: 1700  loss: 11.7859 (37.3458)  obj_loss: 1.0771 (2.3534)  loss_fg: 8.5714 (15.7077)  loss_bg: 1.1278 (19.2848)  time: 1.0203 (1.0238)  data: 0.0900 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 4:02:17  iter: 1800  loss: 11.7867 (35.9519)  obj_loss: 1.1240 (2.2829)  loss_fg: 6.9603 (15.2687)  loss_bg: 0.9860 (18.4002)  time: 1.0117 (1.0237)  data: 0.0883 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 4:00:31  iter: 1900  loss: 11.9320 (34.7554)  obj_loss: 1.0078 (2.2183)  loss_fg: 7.2231 (14.8851)  loss_bg: 1.3388 (17.6520)  time: 1.0350 (1.0235)  data: 0.1019 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:58:48  iter: 2000  loss: 11.6062 (33.6595)  obj_loss: 1.1006 (2.1611)  loss_fg: 8.1766 (14.5457)  loss_bg: 1.4488 (16.9526)  time: 1.0240 (1.0235)  data: 0.0877 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:57:05  iter: 2100  loss: 11.7458 (32.6579)  obj_loss: 1.0352 (2.1074)  loss_fg: 7.0925 (14.2224)  loss_bg: 0.9317 (16.3282)  time: 1.0223 (1.0234)  data: 0.0898 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:55:24  iter: 2200  loss: 12.3620 (31.8258)  obj_loss: 0.9526 (2.0581)  loss_fg: 7.4615 (13.9201)  loss_bg: 1.2311 (15.8477)  time: 1.0344 (1.0235)  data: 0.0887 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:53:39  iter: 2300  loss: 11.4006 (30.9695)  obj_loss: 0.9858 (2.0123)  loss_fg: 6.7768 (13.6451)  loss_bg: 1.8642 (15.3121)  time: 1.0307 (1.0233)  data: 0.0961 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:51:55  iter: 2400  loss: 11.5266 (30.2729)  obj_loss: 0.9834 (1.9710)  loss_fg: 7.0842 (13.3966)  loss_bg: 1.5005 (14.9053)  time: 1.0183 (1.0232)  data: 0.0900 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:50:13  iter: 2500  loss: 10.2038 (29.5466)  obj_loss: 0.9971 (1.9325)  loss_fg: 6.2658 (13.1586)  loss_bg: 1.3567 (14.4556)  time: 1.0209 (1.0232)  data: 0.0892 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:48:30  iter: 2600  loss: 12.2562 (28.8493)  obj_loss: 1.0049 (1.8970)  loss_fg: 8.2573 (12.9343)  loss_bg: 1.9037 (14.0180)  time: 1.0118 (1.0232)  data: 0.0913 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:46:49  iter: 2700  loss: 9.9767 (28.2256)  obj_loss: 1.0107 (1.8634)  loss_fg: 7.1647 (12.7258)  loss_bg: 1.1750 (13.6364)  time: 1.0308 (1.0233)  data: 0.0878 (0.0968)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:45:07  iter: 2800  loss: 9.6912 (27.5837)  obj_loss: 0.9824 (1.8325)  loss_fg: 6.8137 (12.5113)  loss_bg: 1.0245 (13.2399)  time: 1.0030 (1.0233)  data: 0.0898 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:43:25  iter: 2900  loss: 8.6908 (26.9708)  obj_loss: 0.9189 (1.8032)  loss_fg: 6.0499 (12.3032)  loss_bg: 0.9377 (12.8644)  time: 1.0263 (1.0233)  data: 0.0972 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:41:43  iter: 3000  loss: 9.7304 (26.4039)  obj_loss: 0.9600 (1.7753)  loss_fg: 6.5654 (12.1017)  loss_bg: 1.2654 (12.5269)  time: 1.0315 (1.0233)  data: 0.0940 (0.0969)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:40:01  iter: 3100  loss: 9.5711 (25.8504)  obj_loss: 1.0166 (1.7495)  loss_fg: 5.7626 (11.9093)  loss_bg: 0.7531 (12.1915)  time: 1.0256 (1.0234)  data: 0.0916 (0.0970)  lr: 0.016000  max mem: 10207
INFO:maskrcnn_benchmark:eta: 3:38:21  iter: 3200  loss: 8.3860 (25.3792)  obj_loss: 0.9824 (1.7243)  loss_fg: 4.9099 (11.7282)  loss_bg: 1.2674 (11.9267)  time: 1.0331 (1.0235)  data: 0.0943 (0.0971)  lr: 0.016000  max mem: 10207
